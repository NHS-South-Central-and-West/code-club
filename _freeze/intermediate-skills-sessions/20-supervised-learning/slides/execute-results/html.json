{
  "hash": "deef61990d2e428c4fcd9680633adee9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Supervised Learning\nsubtitle: Understanding How Models Learn\nfooter: '**SAT //** Supervised Learning **//** February 2026'\nformat: revealjs\n---\n\n# Supervised Learning Fundamentals {data-background-color=\"#425563\"}\n\nCore Concepts and Terminology\n\n## What is Supervised Learning? {.center}\n\n- Learning from labelled examples to predict outcomes on new data.\n- You provide **features** (X), the input variables describing each observation, and the **target** (y), the outcome you want to predict.\n- The model learns the relationship between X and y, and predicts the value of y given X.\n\n## Two Types of Supervised Learning {.center}\n\n- **Classification** - Predicting categories\n  - Will this patient be readmitted? (Yes/No)\n  - Which diagnosis code applies? (A, B, C, or D)\n  - Binary or multi-class outcomes\n\n- **Regression** - Predicting continuous numbers\n  - How long will this patient stay in hospital? (days)\n  - What will next month's A&E attendances be? (count)\n  - Numerical outcomes\n\n## Classification vs Regression {.center}\n\n| Aspect         | Classification                      | Regression                        |\n|----------------|-------------------------------------|-----------------------------------|\n| **Output**     | Categories                          | Numbers                           |\n| **Example**    | Survived: Yes/No                    | Length of stay: 4.2 days          |\n| **Algorithms** | Logistic regression, decision trees | Linear regression, decision trees |\n| **Evaluation** | Accuracy, precision                 | RMSE, MAE                         |\n\n**Note** - Some algorithms work for both (e.g., decision trees)\n\n## The Generalisation Challenge {.center}\n\n- The goal of supervised learning is to learn patterns that work on new data, not just training data.\n- But this is difficult. Models can underfit and overfit, and will not generalise well.\n  - **Underfitting** - Model too simple, misses patterns, performs poorly on training and test data.\n  - **Overfitting** - Model too complex, memorises training data, performs well on training but poorly on test data.\n- The sweet spot is a model that captures real patterns/signal but ignores noise.\n\n# How Do Models Learn? {data-background-color=\"#425563\"}\n\nSorting the Signal from the Noise\n\n## The Learning Process {.center}\n\n- Models learn by optimising a mathematical objective during training. \n- This involves finding patterns that minimise the prediction error in training.\n- What happens during `.fit()`:\n  1. Model makes predictions on training data\n  2. Calculates how wrong those predictions are\n  3. Adjusts internal parameters to reduce errors\n  4. Repeats until errors stop decreasing\n\n## No Free Lunch Theorem {.center}\n\n- Different algorithms use different strategies to find patterns and optimise the mathematical objective.\n- Some algorithms deal well with simplicity, others work well with sparse data, some can handle incredibly complexity.\n- But there is no single \"best\" algorithm.\n  - This is known as the **No Free Lunch Theorem**.\n\n## Algorithms vs Models {.center}\n\n- The **algorithm** is the learning approach (the recipe).\n  - Logistic regression, decision tree, random forest\n- The **model** is the trained result (applying the recipe to your data).\n  - A random forest *algorithm* trained on Titanic data becomes a *model*\n- You can use many different algorithms, but the workflow remains the same.\n\n## Many Algorithms to Choose From {.center}\n\n- Linear methods\n  - Linear Regression, Logistic Regression, Ridge, Lasso\n- Tree-based methods\n  - Decision Trees, Random Forests, Gradient Boosting (XGBoost, LightGBM, CatBoost)\n- And many other approaches\n  - K-Nearest Neighbours, Support Vector Machines, Neural Networks\n\n## The Workflow is Plug & Play {.center}\n\n::: {#workflow .cell output-location='default' execution_count=1}\n``` {.python .cell-code}\n# swap algorithms, keep the workflow\nclf = LogisticRegression()      # or\nclf = DecisionTreeClassifier()  # or\nclf = RandomForestClassifier()  # or\nclf = GradientBoostingClassifier()\n\n# same workflow regardless\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\n```\n:::\n\n\n## Choosing the Right Model {.center}\n\n- Consider interpretability:\n  - Model/predictions needs to be easy to explain - Simpler models\n  - Predictive performance matters most - Complex models\n- Consider your data:\n  - Small datasets (< 1000 rows) - Simpler models, less overfitting risk\n  - Large datasets - Complex models can find subtle patterns\n- Consider constraints:\n  - Limited compute - Faster algorithms\n  - Deploying to production - Consider training/prediction time\n- Best practice - Try multiple models, compare on hold-out (testing) data\n\n# Let's Write Some Code... {data-background-color=\"#425563\"}\n\n## Thank You! {.center}\n\nContact:\n\n{{< fa solid envelope >}} [scwcsu.analytics.specialist@nhs.net](mailto:scwcsu.analytics.specialist@nhs.net)\n\nCode & Slides:\n\n{{< fa brands github >}} [/NHS-South-Central-and-West/code-club](https://github.com/nhs-south-central-and-west/code-club)\n\n... And don't forget to give us your [feedback](https://forms.office.com/e/g9fnSVPRwE).\n\n",
    "supporting": [
      "slides_files\\figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}