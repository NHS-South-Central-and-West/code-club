{
  "hash": "ef83e38327732a121b1c187ee3a4f549",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Regression Fundamentals: Introduction to Linear Regression\"\nformat:\n  html: default\n  ipynb: default\n---\n\nThis is the third session in our regression foundations series. Previous sessions discussed how to evaluate the differences between samples and the relationships between pairs of variables. This session will focus on bringing all of that together in order to make more precise inferences and predictions using data.\n\nWe'll continue using the Palmer Penguins dataset to build intuition for regression concepts, then apply these methods to understand what drives penguin body mass and how we can predict it from other physical characteristics.\n\n# Slides\n\nUse the left ⬅️ and right ➡️ arrow keys to navigate through the slides below. To view in a separate tab/window, [follow this link](slides.html){target=\"_blank\"}.\n\n<iframe src=\"slides.html\" height=\"500\" width=\"100%\"></iframe>\n\n## What is Regression?\n\nRegression is a method for modelling the relationship between one or more explanatory variables (or predictors) and an outcome of interest. The goal of regression is to find a function that predicts the outcome from the predictors.\n\nRegression is incredibly flexible in both approach and application. It can fit to all kinds of data problem and can be used for descriptive, explanatory, and predictive analysis. For this reason, it is a foundational tool in statistics and data science.\n\n## Why Build Models?\n\nIn our previous sessions, we compared groups (4/20 vs other days) and examined relationships (penguin flipper length vs body mass). But both approaches had limitations: group comparisons can't handle continuous predictors, and correlation only measures linear association between two variables at a time.\n\nRegression bridges these gaps. It lets us:\n\n- Quantify how much $Y$ changes when $X$ increases by one unit.\n- Include multiple predictors simultaneously.\n- Make predictions for new observations.\n- Test specific hypotheses about relationships.\n- Account for confounding variables.\n\nInstead of just knowing flipper length and body mass are correlated, we can say that each additional millimeter increase in flipper length predicts an $X$ increase in body mass. And we can go much further than this, controlling for environment, treating species as a grouping structure in the data, or facotoring in non-linearity. With regression, the possibilities are practically endless.\n\n### From Association to Inference & Prediction\n\nCorrelation tells us variables move together. Regression tells us by how much and in what direction, and gives us the tools to make inferences and predictions.\n\nConsider these questions that regression can answer but correlation cannot:\n\n- If a penguin's flipper is 200mm, what body mass do we predict?\n- How does the flipper-mass relationship differ between species?\n- What's our uncertainty around these predictions?\n- Which variables matter most for predicting body mass?\n\n## The Linear Regression Model\n\nThe most common type of regression model, and the foundation from which so many other types of regression model are built, is linear regression. Linear regression assumes the relationship between the predictors and the outcome are linear, meaning that when predictors change in value the outcome changes by a constant amount. Visually, this just means fitting a straight line through data. For example, if you are paid £10 an hour, your pay increases by exactly £10 for each additional hour worked. This is a linear relationship.\n\nIf you have a predictor $X$ and an outcome $Y$, linear regression finds the path through the data that best fits these points, otherwise known as the \"line of best fit\". This works by finding a line that passes through the data with the minimum amount of prediction error, which is the observed value minus the predicted value for each data pint (and is oftn referred to as the residual). The most common way for calculating the line of best fit is called Ordinary Least Squares (OLS)[^OLS].\n\n<details>\n  <summary>Calculating the Line of Best fit Using OLS (Click to Expand)</summary>\n\n### Ordinary Least Squares Estimation\n\nWe won't spend too much time on OLS in this session, but the method it uses for calculating the line of best fit is as follows:\n\n1. Calculate the residual for each observation (the actual value minus the predicted value).\n2. Square the residuals (so that positive and negative errors do not cancel each other out).\n3. Sum the squared residuals, sometimes called the Residual Sum of Squares (RSS).\n4. Find the line that produces the smallest RSS.\n\n</details>\n\nThe formula for a simple linear regression model, predicting $Y$ with one predictor $X$: \n\n$$\nY = \n\\underbrace{\\vphantom{\\beta_0} \\overset{\\color{#41B6E6}{\\text{Intercept}}}{\\color{#41B6E6}{\\beta_0}} + \n\\overset{\\color{#005EB8}{\\text{Slope}}}{\\color{#005EB8}{\\beta_1}}X \\space \\space}_{\\text{Explained Variance}} + \n\\overset{\\mathstrut \\color{#ED8B00}{\\text{Error}}}{\\underset{\\text{Unexplained}}{\\color{#ED8B00}{\\epsilon}}}\n$$\n\nThis breaks the problem down into three components, and estimates two parameters:\n\n- $\\beta_1$ - The slope, estimating the effect that $X$ has on the outcome, $Y$. \n- $\\beta_0$ - The intercept, estimating the average value of $Y$ when $X = 0$.\n- $\\epsilon$ - The error term, capturing the remaining variance in the outcome $Y$ that is not explained by the rest of the model.\n\n::: {.callout-note}\nDon't worry if this is intimidating at first. The main thing you need to remember is that this equation is just fitting a straight line through the data.\n:::\n\nThe regression line represents our best guess of the true relationship, and the scatter around the line represents uncertainty.\n\n## Building Our First Model\n\n### Import & Prepare Data\n\n::: {#cell-import-data .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# load penguins data\nurl = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-15/penguins.csv'\npenguins_raw = pd.read_csv(url)\n\n# clean data\ndf = penguins_raw.dropna()\ndf.head()\n```\n\n::: {#import-data .cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>island</th>\n      <th>bill_len</th>\n      <th>bill_dep</th>\n      <th>flipper_len</th>\n      <th>body_mass</th>\n      <th>sex</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.1</td>\n      <td>18.7</td>\n      <td>181.0</td>\n      <td>3750.0</td>\n      <td>male</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.5</td>\n      <td>17.4</td>\n      <td>186.0</td>\n      <td>3800.0</td>\n      <td>female</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>40.3</td>\n      <td>18.0</td>\n      <td>195.0</td>\n      <td>3250.0</td>\n      <td>female</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>36.7</td>\n      <td>19.3</td>\n      <td>193.0</td>\n      <td>3450.0</td>\n      <td>female</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.3</td>\n      <td>20.6</td>\n      <td>190.0</td>\n      <td>3650.0</td>\n      <td>male</td>\n      <td>2007</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Visualising the Relationship\n\n::: {#cell-flipper-mass-scatter .cell execution_count=2}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df, x='flipper_len', y='body_mass', alpha=0.7)\nplt.title('Flipper Length vs Body Mass')\nplt.xlabel('Flipper Length (mm)')\nplt.ylabel('Body Mass (g)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-ipynb/flipper-mass-scatter-output-1.png){#flipper-mass-scatter}\n:::\n:::\n\n\nThe relationship looks strongly linear and positive. This is a good candidate for linear regression.\n\n### Fitting the Model\n\n::: {#simple-regression .cell execution_count=3}\n``` {.python .cell-code}\n# fit simple linear regression\nX = df[['flipper_len']]  # predictor\ny = df['body_mass']      # outcome\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# extract coefficients\nintercept = model.intercept_\nslope = model.coef_[0]\n```\n:::\n\n\nThe intercept ($\\beta_0$) = \\-5872\\.1g. The intercept tells us the predicted body mass when flipper length is 0mm. This intercept value is not meaningful because a flipper length of zero is implausible and ultimately nonsensical (if they have flippers, they must be longer than 0mm)[^Intercept], but we need the intercept to fit the model. \n\nThe slope ($\\beta_1$) = 50\\.2g/mm. The slope tells us that each additional mm increase in flipper length predicts ~50g increase in body mass.\n\n[^Intercept]:\n\nFor explanatory models, it is generally best-practice to transform the data so that the intercept is meaningful. For example, subtracting the mean flipper length from each observation (otherwise known as centering) so that the zero value represents the average flipper length and the intercept represents the average body mass for the average flipper length.\n\n### Visualising the Fitted Line\n\nWe can add the fitted line to our scatterplot and use this to make predictions by tracing the flipper length from which we want to predict body mass and then finding the body mass at that value.\n\n::: {#cell-regression-line .cell execution_count=4}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df, x='flipper_len', y='body_mass', alpha=0.7)\n\n# add regression line\nx_range = np.linspace(df['flipper_len'].min(), df['flipper_len'].max(), 100)\ny_pred = intercept + slope * x_range\nplt.plot(x_range, y_pred, color='red', linewidth=2, label=f'y = {intercept:.0f} + {slope:.1f}x')\n\nplt.title('Linear Regression: Flipper Length Predicting Body Mass')\nplt.xlabel('Flipper Length (mm)')\nplt.ylabel('Body Mass (g)')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-ipynb/regression-line-output-1.png){#regression-line}\n:::\n:::\n\n\n### Making Predictions\n\nUsing a fitted line to make predictions is imprecise. Instead, we can use the linear regression formula detailed earlier to predict penguin body mass from flipper length. We just have to plug in the intercept and slope into the formula, along with the flipper length we want to predict from, and this will give us the predicted body mass.\n\nThe applied formula looks like this:\n\n$$\n\\text{Body Mass} = -5872.1 + 50.2 \\times \\text{Flipper Length}\n$$\n\nOr we can generate predictions using code:\n\n::: {#predictions .cell execution_count=5}\n``` {.python .cell-code}\n# predict body mass for different flipper lengths\nnew_flipper_lengths = np.array([[190], [200], [210], [220]])\npredictions = model.predict(new_flipper_lengths)\n\nprediction_df = pd.DataFrame({\n    'Flipper Length (mm)': new_flipper_lengths.flatten(),\n    'Predicted Body Mass (g)': predictions.round(0)\n})\n\nprint(\"Predictions for new observations:\")\nprint(prediction_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredictions for new observations:\n   Flipper Length (mm)  Predicted Body Mass (g)\n0                  190                   3657.0\n1                  200                   4159.0\n2                  210                   4660.0\n3                  220                   5162.0\n```\n:::\n:::\n\n\n## Statistical Inference\n\nSo far we've focused on prediction. But regression also lets us make inferences about the relationship between variables.\n\n::: {#statistical-inference .cell execution_count=6}\n``` {.python .cell-code}\n# use statsmodels for explanatory models\nX_sm = sm.add_constant(df['flipper_len'])  # add intercept column\nmodel_sm = sm.OLS(df['body_mass'], X_sm).fit()\n\nprint(model_sm.summary().tables[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nconst       -5872.0927    310.285    -18.925      0.000   -6482.472   -5261.713\nflipper_len    50.1533      1.540     32.562      0.000      47.123      53.183\n===============================================================================\n```\n:::\n:::\n\n\nWhen we are interested in explaining rather than predicting, the coefficient (`coef`) is what matters most. This is the intercept and slope values we calculated earlier. The coefficients tell us the size of the effect observed in the data. If you want to explain the relationship between variables, the effect size is the most important part of the model.\n\nThe other outputs are useful for understanding how well your model fits to the data and how certain the model is about the estimates. The standard error (`std error`) tells us how much the actual values vary around the fitted line, quantifying uncertainty. The 95% confidence intervals (`[0.025` & `0.975]`) are another way of expressing this idea. They effectively show a range of plausible coefficient values.\n\nFinally, the t-statistic (`t`) is a test statistic[^T] and the p-value (`P>|t|`) is the probability of observing an effect as large (or larger) as observed in the data if there was actually no relationship between flipper length and body mass (and if the assumptions of the model are valid). You can think of the p-value as an expression of how surprising it would be to see a coefficient as large as the one we found if there wasn't actually a relationship between flipper length and body mass.\n\n[^T]:\n\nWe will not discuss test statistics in detail here, but they are standardised measures of how unusual your results are. It is used to calculate the p-value. The t-statistic is the coefficient divided by its standard error.\n\n## Summary\n\nLinear regression transforms correlation into a predictive and explanatory tool. We can quantify relationships and make predictions. Our penguin model shows that flipper length strongly predicts body mass, with each additional mm corresponding to ~50g increase in weight.\n\nThe key insights from regression go beyond simple correlation: we can make specific predictions, quantify uncertainty, and begin to understand the mechanisms behind relationships. However, real-world relationships are often more complex than simple linear models can capture.\n\nIn the next session, we will go into greater detail about the implementation of linear regression, looking at how we fit models, their assumptions, and how we interpret, present, and communicate our outputs.\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n    path: C:\\Users\\paul.johnson\\git\\code-club\\.venv\\share\\jupyter\\kernels\\python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.12.5\n---\n",
    "supporting": [
      "index_files\\figure-ipynb"
    ],
    "filters": []
  }
}