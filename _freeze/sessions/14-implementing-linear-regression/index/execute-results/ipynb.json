{
  "hash": "6a1293f12257a2984bdbb843cde7f09f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Regression Fundamentals: Implementing Linear Regression\"\nformat:\n  html: default\n  ipynb: default\n---\n\nIn the previous session, we explored what linear regression is, how it works conceptually, and why it's such a powerful tool. We saw how regression finds the line of best fit by minimising prediction errors, and we understood the core components: intercepts, slopes, and residuals.\n\nNow it's time to put that theory into practice. In this session, we'll learn how to actually build regression models in Python, interpret their outputs, check whether our models are valid, and generate insights from them.\n\nWe'll cover two main approaches to regression in Python:\n\n- `scikit-learn` - Focused on prediction and machine learning workflows.\n- `statsmodels` - Focused on statistical inference and hypothesis testing.\n\nLet's get started by loading our data and packages.\n\n::: {#setup .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# set visualisation style\nsns.set_theme(style=\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n```\n:::\n\n\n::: {#import-data .cell execution_count=2}\n``` {.python .cell-code}\n# load palmer penguins data\nurl = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-15/penguins.csv'\ndf = pd.read_csv(url)\n\n# drop missing values for simplicity\ndf = df.dropna()\n```\n:::\n\n\n## Building Predictive Models Using Scikit-Learn\n\n`scikit-learn` (`sklearn`) is Python's most popular library for machine learning. Its regression tools are designed for building predictive models, and it is optimised for generalising well on new data.\n\n### Simple Linear Regression\n\nA simple linear regression has one predictor variable. Let's predict body mass from flipper length.\n\n::: {#simple-lm-sklearn .cell execution_count=3}\n``` {.python .cell-code}\n# prepare the data\n# sklearn requires 2D arrays for X (predictors) and 1D arrays for y (outcome)\nX = df[['flipper_len']]  # double brackets create a dataframe\ny = df['body_mass']       # single bracket creates a series\n\n# create and fit the model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# get the intercept\nintercept = model.intercept_\nprint(f\"Intercept = {intercept:.2f}g\")\n\n# get the slope\nslope = model.coef_[0]\nprint(f\"Slope = {slope:.2f}g per mm\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercept = -5872.09g\nSlope = 50.15g per mm\n```\n:::\n:::\n\n\nThe fitted model object contains everything we need: coefficients, predictions, and model performance metrics.\n\nThe intercept ($\\beta_0$) = \\-5872\\.1g. The intercept tells us the predicted body mass when flipper length is 0mm. This intercept value is not meaningful because a flipper length of zero is implausible and ultimately nonsensical (if they have flippers, they must be longer than 0mm)[^Intercept], but we need the intercept to fit the model. \n\nThe slope ($\\beta_1$) = 50\\.2g/mm. The slope tells us that each additional mm increase in flipper length predicts ~50g increase in body mass.\n\n### Making Predictions\n\nOnce we have a fitted model, we can use it to predict body mass for any flipper length.\n\n::: {#predictions .cell execution_count=4}\n``` {.python .cell-code}\n# predict body mass for all penguins in our dataset\ny_pred = model.predict(X)\n\n# add predictions to our dataframe\ndf['predicted_mass'] = y_pred\ndf['residual'] = df['body_mass'] - df['predicted_mass']\n\n# look at some predictions\nprint(\"Sample predictions:\")\nprint(df[['flipper_len', 'body_mass', 'predicted_mass', 'residual']].head(10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSample predictions:\n    flipper_len  body_mass  predicted_mass    residual\n0         181.0     3750.0     3205.648453  544.351547\n1         186.0     3800.0     3456.414782  343.585218\n2         195.0     3250.0     3907.794176 -657.794176\n4         193.0     3450.0     3807.487644 -357.487644\n5         190.0     3650.0     3657.027846   -7.027846\n6         181.0     3625.0     3205.648453  419.351547\n7         195.0     4675.0     3907.794176  767.205824\n12        182.0     3200.0     3255.801719  -55.801719\n13        191.0     3800.0     3707.181112   92.818888\n14        198.0     4400.0     4058.253974  341.746026\n```\n:::\n:::\n\n\nWe can also predict for new data:\n\n::: {#predictions-new-data .cell execution_count=5}\n``` {.python .cell-code}\n# predict for hypothetical penguins\nnew_penguins = pd.DataFrame({\n    'flipper_len': [180, 200, 220]\n})\n\nnew_predictions = model.predict(new_penguins)\n\nprint(\"\\nPredictions for new penguins:\")\nfor flipper, mass in zip(new_penguins['flipper_len'], new_predictions):\n    print(f\"Flipper length: {flipper}mm --> Predicted mass: {mass:.0f}g\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nPredictions for new penguins:\nFlipper length: 180mm --> Predicted mass: 3155g\nFlipper length: 200mm --> Predicted mass: 4159g\nFlipper length: 220mm --> Predicted mass: 5162g\n```\n:::\n:::\n\n\n### Model Performance Metrics\n\nAn important part of predictive modelling is evaluation. We need to test how well our model is performing! `sklearn` provides several a wide variety of metrics for assessing model performance. We will look at $\\text{R}^2$ and RMSE.\n\n::: {#metrics .cell execution_count=6}\n``` {.python .cell-code}\n# r-squared: proportion of variance explained\nr2 = r2_score(y, y_pred)\n\n# root mean squared error: average prediction error in grams\nrmse = np.sqrt(mean_squared_error(y, y_pred))\n```\n:::\n\n\nThe $\\text{R}^2$ = 0\\.76, which suggests that flipper length explains a significant amount of the variance in penguin body mass, while the RMSE indicates that the average prediction error is 392\\.16g.\n\n### Multiple Linear Regression\n\nNow let's add more predictors. Multiple regression lets us understand the effect of each variable while controlling for the others.\n\n::: {#multiple-lm-sklearn .cell execution_count=7}\n``` {.python .cell-code}\n# add bill length and bill depth as predictors\nX_multi = df[['flipper_len', 'bill_len', 'bill_dep']]\ny = df['body_mass']\n\n# fit the model\nmodel_multi = LinearRegression()\nmodel_multi.fit(X_multi, y)\n\n# extract coefficients\nprint(\"Multiple regression coefficients:\")\nprint(f\"Intercept: {model_multi.intercept_:.2f}g\")\nfor name, coef in zip(X_multi.columns, model_multi.coef_):\n    print(f\"  {name}: {coef:.2f}g per unit\")\n\n# compare model performance\ny_pred_multi = model_multi.predict(X_multi)\nr2_multi = r2_score(y, y_pred_multi)\nrmse_multi = np.sqrt(mean_squared_error(y, y_pred_multi))\n\nprint(f\"\\nModel comparison:\")\nprint(f\"Simple model (flipper only):   R² = {r2:.3f}, RMSE = {rmse:.1f}g\")\nprint(f\"Multiple model (three vars):   R² = {r2_multi:.3f}, RMSE = {rmse_multi:.1f}g\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMultiple regression coefficients:\nIntercept: -6445.48g\n  flipper_len: 50.76g per unit\n  bill_len: 3.29g per unit\n  bill_dep: 17.84g per unit\n\nModel comparison:\nSimple model (flipper only):   R² = 0.762, RMSE = 392.2g\nMultiple model (three vars):   R² = 0.764, RMSE = 390.6g\n```\n:::\n:::\n\n\nAdding bill measurements improves our model slightly. Each predictor contributes information beyond what the others provide.\n\n## Building Inferential Models Using Statsmodels\n\n`statsmodels` is designed for statistical inference. It provides detailed output about uncertainty, hypothesis tests, and model diagnostics. This is what you want when your goal is understanding relationships rather than pure prediction.\n\nAdding predictors is straightforward with the formula interface:\n\n::: {#lm-statsmodels .cell execution_count=8}\n``` {.python .cell-code}\n# multiple regression with formula\nmodel_sm = smf.ols('body_mass ~ flipper_len + bill_len + bill_dep', data=df).fit()\n\nprint(model_sm.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              body_mass   R-squared:                       0.764\nModel:                            OLS   Adj. R-squared:                  0.762\nMethod:                 Least Squares   F-statistic:                     354.9\nDate:                Wed, 05 Nov 2025   Prob (F-statistic):          9.26e-103\nTime:                        11:47:56   Log-Likelihood:                -2459.8\nNo. Observations:                 333   AIC:                             4928.\nDf Residuals:                     329   BIC:                             4943.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept   -6445.4760    566.130    -11.385      0.000   -7559.167   -5331.785\nflipper_len    50.7621      2.497     20.327      0.000      45.850      55.675\nbill_len        3.2929      5.366      0.614      0.540      -7.263      13.849\nbill_dep       17.8364     13.826      1.290      0.198      -9.362      45.035\n==============================================================================\nOmnibus:                        5.596   Durbin-Watson:                   1.982\nProb(Omnibus):                  0.061   Jarque-Bera (JB):                5.469\nSkew:                           0.312   Prob(JB):                       0.0649\nKurtosis:                       3.068   Cond. No.                     5.44e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.44e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\nThe statsmodels summary is dense but incredibly informative. Here's what matters most.\n\n- **coef**: The estimated effect size (this is what matters most!)\n- **std err**: Uncertainty in the estimate\n- **t**: Test statistic (coef / std err)\n- **P>|t|**: p-value for null hypothesis that coefficient = 0\n- **[0.025, 0.975]**: 95% confidence interval\n\nEach coefficient represents the effect of that variable **holding the others constant**. This is crucial for interpretation.\n\nFocus on effect sizes first, then uncertainty. The coefficient tells you the magnitude of the relationship. The confidence interval tells you how precisely we've estimated it. The p-value is useful as a diagnostic, indicating whether you have enough data, but it doesn't tell you whether the effect is meaningful or important.\n\nFinally, don't obsess over $\\text{R}^2$. A model with $\\text{R}^2$ = 0.3 can still be incredibly useful if the effects are meaningful and the predictions are good enough for your purpose.\n\nWe can also produce a simpler regression table, if the above is overwhelming.\n\n::: {#regression-table .cell execution_count=9}\n``` {.python .cell-code}\n# extract just the coefficients table\nprint(model_sm.summary().tables[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept   -6445.4760    566.130    -11.385      0.000   -7559.167   -5331.785\nflipper_len    50.7621      2.497     20.327      0.000      45.850      55.675\nbill_len        3.2929      5.366      0.614      0.540      -7.263      13.849\nbill_dep       17.8364     13.826      1.290      0.198      -9.362      45.035\n===============================================================================\n```\n:::\n:::\n\n\n<details>\n  <summary> Styled Tables (Click to Expand)</summary> \n\nFor presentations and reports, we can create publication-ready tables:\n\n::: {#cell-great-regression-table .cell execution_count=10}\n``` {.python .cell-code}\nfrom great_tables import GT, md, html\nfrom great_tables import loc, style\n\n# fit three models for comparison\nmodel1 = smf.ols('body_mass ~ flipper_len', data=df).fit()\nmodel2 = smf.ols('body_mass ~ flipper_len + bill_len + bill_dep', data=df).fit()\nmodel3 = smf.ols('body_mass ~ flipper_len + bill_len + bill_dep + C(species)', data=df).fit()\n\n# create a dataframe with model results\nresults_df = pd.DataFrame({\n    'Variable': ['Intercept', 'Flipper Length', 'Bill Length', 'Bill Depth', \n                 'Chinstrap', 'Gentoo'],\n    'Model 1': [\n        f\"{model1.params['Intercept']:.1f}\",\n        f\"{model1.params['flipper_len']:.2f}***\",\n        '—', '—', '—', '—'\n    ],\n    'Model 2': [\n        f\"{model2.params['Intercept']:.1f}\",\n        f\"{model2.params['flipper_len']:.2f}***\",\n        f\"{model2.params['bill_len']:.2f}***\",\n        f\"{model2.params['bill_dep']:.2f}\",\n        '—', '—'\n    ],\n    'Model 3': [\n        f\"{model3.params['Intercept']:.1f}\",\n        f\"{model3.params['flipper_len']:.2f}***\",\n        f\"{model3.params['bill_len']:.2f}***\",\n        f\"{model3.params['bill_dep']:.2f}**\",\n        f\"{model3.params['C(species)[T.Chinstrap]']:.1f}***\",\n        f\"{model3.params['C(species)[T.Gentoo]']:.1f}***\"\n    ]\n})\n\n# add model statistics\nstats_df = pd.DataFrame({\n    'Variable': ['', 'N', 'R²', 'Adj. R²', 'AIC'],\n    'Model 1': ['', f\"{int(model1.nobs)}\", f\"{model1.rsquared:.3f}\", \n                f\"{model1.rsquared_adj:.3f}\", f\"{model1.aic:.1f}\"],\n    'Model 2': ['', f\"{int(model2.nobs)}\", f\"{model2.rsquared:.3f}\", \n                f\"{model2.rsquared_adj:.3f}\", f\"{model2.aic:.1f}\"],\n    'Model 3': ['', f\"{int(model3.nobs)}\", f\"{model3.rsquared:.3f}\", \n                f\"{model3.rsquared_adj:.3f}\", f\"{model3.aic:.1f}\"]\n})\n\ncombined_df = pd.concat([results_df, stats_df], ignore_index=True)\n\n# create styled table\ntable = (\n    GT(combined_df)\n    .tab_header(\n        title=\"Regression Models Predicting Penguin Body Mass\",\n        subtitle=\"Coefficients with significance stars (* p<0.05, ** p<0.01, *** p<0.001)\"\n    )\n    .tab_spanner(\n        label=\"Model Specifications\",\n        columns=['Model 1', 'Model 2', 'Model 3']\n    )\n    .tab_style(\n        style=style.text(weight=\"bold\"),\n        locations=loc.body(rows=[6, 7, 8, 9, 10])\n    )\n    .tab_style(\n        style=style.borders(sides=\"top\", weight=\"2px\"),\n        locations=loc.body(rows=[6])\n    )\n    .cols_align(\n        align=\"center\",\n        columns=['Model 1', 'Model 2', 'Model 3']\n    )\n    .cols_align(\n        align=\"left\",\n        columns=['Variable']\n    )\n)\n\ntable\n```\n\n::: {#great-regression-table .cell-output .cell-output-display execution_count=10}\n```{=html}\n<div id=\"jdlrduudaa\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>\n#jdlrduudaa table {\n          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n          -webkit-font-smoothing: antialiased;\n          -moz-osx-font-smoothing: grayscale;\n        }\n\n#jdlrduudaa thead, tbody, tfoot, tr, td, th { border-style: none; }\n tr { background-color: transparent; }\n#jdlrduudaa p { margin: 0; padding: 0; }\n #jdlrduudaa .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n #jdlrduudaa .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n #jdlrduudaa .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n #jdlrduudaa .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n #jdlrduudaa .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #jdlrduudaa .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #jdlrduudaa .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #jdlrduudaa .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n #jdlrduudaa .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n #jdlrduudaa .gt_column_spanner_outer:first-child { padding-left: 0; }\n #jdlrduudaa .gt_column_spanner_outer:last-child { padding-right: 0; }\n #jdlrduudaa .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n #jdlrduudaa .gt_spanner_row { border-bottom-style: hidden; }\n #jdlrduudaa .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n #jdlrduudaa .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n #jdlrduudaa .gt_from_md> :first-child { margin-top: 0; }\n #jdlrduudaa .gt_from_md> :last-child { margin-bottom: 0; }\n #jdlrduudaa .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n #jdlrduudaa .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n #jdlrduudaa .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n #jdlrduudaa .gt_row_group_first td { border-top-width: 2px; }\n #jdlrduudaa .gt_row_group_first th { border-top-width: 2px; }\n #jdlrduudaa .gt_striped { background-color: rgba(128,128,128,0.05); }\n #jdlrduudaa .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #jdlrduudaa .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n #jdlrduudaa .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n #jdlrduudaa .gt_left { text-align: left; }\n #jdlrduudaa .gt_center { text-align: center; }\n #jdlrduudaa .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n #jdlrduudaa .gt_font_normal { font-weight: normal; }\n #jdlrduudaa .gt_font_bold { font-weight: bold; }\n #jdlrduudaa .gt_font_italic { font-style: italic; }\n #jdlrduudaa .gt_super { font-size: 65%; }\n #jdlrduudaa .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n #jdlrduudaa .gt_asterisk { font-size: 100%; vertical-align: 0; }\n \n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n<thead>\n\n  <tr class=\"gt_heading\">\n    <td colspan=\"4\" class=\"gt_heading gt_title gt_font_normal\">Regression Models Predicting Penguin Body Mass</td>\n  </tr>\n  <tr class=\"gt_heading\">\n    <td colspan=\"4\" class=\"gt_heading gt_subtitle gt_font_normal gt_bottom_border\">Coefficients with significance stars (* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001)</td>\n  </tr>\n<tr class=\"gt_col_headings gt_spanner_row\">\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"2\" colspan=\"1\" scope=\"col\" id=\"Variable\">Variable</th>\n  <th class=\"gt_center gt_columns_top_border gt_column_spanner_outer\" rowspan=\"1\" colspan=\"3\" scope=\"colgroup\" id=\"Model-Specifications\">\n    <span class=\"gt_column_spanner\">Model Specifications</span>\n  </th>\n</tr>\n<tr class=\"gt_col_headings\">\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Model-1\">Model 1</th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Model-2\">Model 2</th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_center\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Model-3\">Model 3</th>\n</tr>\n</thead>\n<tbody class=\"gt_table_body\">\n  <tr>\n    <td class=\"gt_row gt_left\">Intercept</td>\n    <td class=\"gt_row gt_center\">-5872.1</td>\n    <td class=\"gt_row gt_center\">-6445.5</td>\n    <td class=\"gt_row gt_center\">-4282.1</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">Flipper Length</td>\n    <td class=\"gt_row gt_center\">50.15***</td>\n    <td class=\"gt_row gt_center\">50.76***</td>\n    <td class=\"gt_row gt_center\">20.23***</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">Bill Length</td>\n    <td class=\"gt_row gt_center\">—</td>\n    <td class=\"gt_row gt_center\">3.29***</td>\n    <td class=\"gt_row gt_center\">39.72***</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">Bill Depth</td>\n    <td class=\"gt_row gt_center\">—</td>\n    <td class=\"gt_row gt_center\">17.84</td>\n    <td class=\"gt_row gt_center\">141.77**</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">Chinstrap</td>\n    <td class=\"gt_row gt_center\">—</td>\n    <td class=\"gt_row gt_center\">—</td>\n    <td class=\"gt_row gt_center\">-496.8***</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">Gentoo</td>\n    <td class=\"gt_row gt_center\">—</td>\n    <td class=\"gt_row gt_center\">—</td>\n    <td class=\"gt_row gt_center\">965.2***</td>\n  </tr>\n  <tr>\n    <td style=\"font-weight: bold; border-top: 2px solid #000000;\" class=\"gt_row gt_left\"></td>\n    <td style=\"font-weight: bold; border-top: 2px solid #000000;\" class=\"gt_row gt_center\"></td>\n    <td style=\"font-weight: bold; border-top: 2px solid #000000;\" class=\"gt_row gt_center\"></td>\n    <td style=\"font-weight: bold; border-top: 2px solid #000000;\" class=\"gt_row gt_center\"></td>\n  </tr>\n  <tr>\n    <td style=\"font-weight: bold;\" class=\"gt_row gt_left\">N</td>\n    <td style=\"font-weight: bold;\" class=\"gt_row gt_center\">333</td>\n    <td style=\"font-weight: bold;\" class=\"gt_row gt_center\">333</td>\n    <td style=\"font-weight: bold;\" class=\"gt_row gt_center\">333</td>\n  </tr>\n  <tr>\n    <td style=\"font-weight: bold;\" class=\"gt_row gt_left\">R²</td>\n    <td style=\"font-weight: bold;\" class=\"gt_row gt_center\">0.762</td>\n    <td style=\"font-weight: bold;\" class=\"gt_row gt_center\">0.764</td>\n    <td style=\"font-weight: bold;\" class=\"gt_row gt_center\">0.849</td>\n  </tr>\n  <tr>\n    <td style=\"font-weight: bold;\" class=\"gt_row gt_left\">Adj. R²</td>\n    <td style=\"font-weight: bold;\" class=\"gt_row gt_center\">0.761</td>\n    <td style=\"font-weight: bold;\" class=\"gt_row gt_center\">0.762</td>\n    <td style=\"font-weight: bold;\" class=\"gt_row gt_center\">0.847</td>\n  </tr>\n  <tr>\n    <td style=\"font-weight: bold;\" class=\"gt_row gt_left\">AIC</td>\n    <td style=\"font-weight: bold;\" class=\"gt_row gt_center\">4926.1</td>\n    <td style=\"font-weight: bold;\" class=\"gt_row gt_center\">4927.6</td>\n    <td style=\"font-weight: bold;\" class=\"gt_row gt_center\">4781.7</td>\n  </tr>\n</tbody>\n\n\n</table>\n\n</div>\n        \n```\n:::\n:::\n\n\n</details>\n\n## Adding Categorical Variables\n\n`statsmodels` handles categorical variables automatically with the formula interface:\n\n::: {#categorical-variables .cell execution_count=11}\n``` {.python .cell-code}\n# add species as a categorical predictor\n# statsmodels automatically creates dummy variables\nmodel_species_sm = smf.ols('body_mass ~ flipper_len + bill_len + bill_dep + C(species)', \n                           data=df).fit()\n\nprint(model_species_sm.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              body_mass   R-squared:                       0.849\nModel:                            OLS   Adj. R-squared:                  0.847\nMethod:                 Least Squares   F-statistic:                     369.1\nDate:                Wed, 05 Nov 2025   Prob (F-statistic):          4.22e-132\nTime:                        11:47:57   Log-Likelihood:                -2384.8\nNo. Observations:                 333   AIC:                             4782.\nDf Residuals:                     327   BIC:                             4805.\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n===========================================================================================\n                              coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------------\nIntercept               -4282.0802    497.832     -8.601      0.000   -5261.438   -3302.723\nC(species)[T.Chinstrap]  -496.7583     82.469     -6.024      0.000    -658.995    -334.521\nC(species)[T.Gentoo]      965.1983    141.770      6.808      0.000     686.301    1244.096\nflipper_len                20.2264      3.135      6.452      0.000      14.059      26.394\nbill_len                   39.7184      7.227      5.496      0.000      25.501      53.936\nbill_dep                  141.7714     19.163      7.398      0.000     104.072     179.470\n==============================================================================\nOmnibus:                        7.321   Durbin-Watson:                   2.248\nProb(Omnibus):                  0.026   Jarque-Bera (JB):                7.159\nSkew:                           0.348   Prob(JB):                       0.0279\nKurtosis:                       3.179   Cond. No.                     6.01e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 6.01e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\nThe `C()` wrapper tells `statsmodels` to treat species as categorical. It automatically creates dummy variables and chooses a reference category (alphabetically first by default).\n\n::: {#category-effects .cell execution_count=12}\n``` {.python .cell-code}\nprint(\"\\nSpecies effects relative to Adelie:\")\nfor var in model_species_sm.params.index:\n    if 'species' in var:\n        coef = model_species_sm.params[var]\n        ci = model_species_sm.conf_int().loc[var]\n        species = var.split('[T.')[1].rstrip(']')\n        print(f\"{species:12s}: {coef:7.0f}g heavier [{ci[0]:7.0f}, {ci[1]:7.0f}]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSpecies effects relative to Adelie:\nChinstrap   :    -497g heavier [   -659,    -335]\nGentoo      :     965g heavier [    686,    1244]\n```\n:::\n:::\n\n\n## Coefficient Plots\n\nCoefficient plots show effect sizes and their uncertainty at a glance.\n\n::: {#cell-coefficient-plot .cell execution_count=13}\n``` {.python .cell-code}\n# extract coefficients and confidence intervals\ncoefs = model_species_sm.params.drop('Intercept')\nconf_int = model_species_sm.conf_int().drop('Intercept')\n\n# create dataframe for plotting\ncoef_data = pd.DataFrame({\n    'Variable': coefs.index,\n    'Coefficient': coefs.values,\n    'Lower': conf_int[0].values,\n    'Upper': conf_int[1].values\n})\n\n# plot\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(coef_data['Coefficient'], range(len(coef_data)), s=100, color='#005EB8')\nax.hlines(range(len(coef_data)), coef_data['Lower'], coef_data['Upper'], \n         color='#005EB8', linewidth=2)\nax.axvline(x=0, color='red', linestyle='--', linewidth=1)\nax.set_yticks(range(len(coef_data)))\nax.set_yticklabels(coef_data['Variable'])\nax.set_xlabel('Coefficient (Effect on Body Mass, g)')\nax.set_title('Coefficient Plot with 95% Confidence Intervals')\nax.invert_yaxis()\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-ipynb/coefficient-plot-output-1.png){#coefficient-plot}\n:::\n:::\n\n\nVariables whose CI doesn't cross zero have effects distinguishable from zero. But remember: focus on effect sizes, not just significance!\n\n# Regression Assumptions and Diagnostics\n\nLinear regression makes several assumptions. When these hold, OLS gives us the Best Linear Unbiased Estimator (BLUE). When they don't, we need to either fix the problem or acknowledge the limitations.\n\n## The Gauss-Markov Theorem\n\nUnder certain conditions, OLS is BLUE:\n\n1. **Linearity**: The relationship between X and Y is linear\n2. **Independence**: Observations are independent of each other\n3. **Homoscedasticity**: Variance of errors is constant across X\n4. **No perfect multicollinearity**: Predictors aren't perfectly correlated\n5. **Exogeneity**: Errors have mean zero (no systematic bias)\n\nAdditionally, for valid inference (confidence intervals, p-values), we need:\n\n6. **Normality**: Errors are normally distributed (or we have a large sample)\n\nLet's check these assumptions systematically.\n\n## Diagnostic Plots\n\nWe'll create a set of diagnostic plots to check our assumptions. First, let's fit a model to diagnose:\n\n::: {#diagnostics-data .cell execution_count=14}\n``` {.python .cell-code}\n# fit our model with species\nmodel = model_species_sm  # use the statsmodels model from earlier\n\n# get predictions and residuals\ndf['fitted'] = model.fittedvalues\ndf['residuals'] = model.resid\ndf['standardised_residuals'] = model.resid_pearson\n```\n:::\n\n\n### 1. Residuals vs Fitted (Linearity & Homoscedasticity)\n\n::: {#cell-linearity-plot .cell execution_count=15}\n``` {.python .cell-code}\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.scatterplot(x=df['fitted'], y=df['residuals'], alpha=0.6, ax=ax)\nax.axhline(y=0, color='red', linestyle='--', linewidth=2)\nax.set_xlabel('Fitted Values')\nax.set_ylabel('Residuals')\nax.set_title('Residuals vs Fitted Values')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-ipynb/linearity-plot-output-1.png){#linearity-plot}\n:::\n:::\n\n\nThe observations should appear to be randomly scattered around zero, and the spread around zero should be constant across the range of fitted values. If you can see a pattern or trend, this suggests the linearity assumption is violated, and if the observations are funnel-shaped, this suggests that the homoscedasticity assumption is violated.\n\n### 2. Q-Q Plot (Normality)\n\n::: {#cell-normality-plot .cell execution_count=16}\n``` {.python .cell-code}\nfrom scipy import stats\n\nfig, ax = plt.subplots(figsize=(10, 6))\nstats.probplot(df['residuals'], dist=\"norm\", plot=ax)\nax.set_title('Q-Q Plot')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-ipynb/normality-plot-output-1.png){#normality-plot}\n:::\n:::\n\n\nThe closer to the red line, the better. However, some deviation is generally fine, and as sample size increases this becomes less of a concern. The very small deviations from the line above are not a cause for concern.\n\n### 3. Scale-Location Plot (Homoscedasticity)\n\n::: {#cell-homoscedasticity-plot .cell execution_count=17}\n``` {.python .cell-code}\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.scatterplot(x=df['fitted'], y=np.sqrt(np.abs(df['standardised_residuals'])), alpha=0.6, ax=ax)\nax.set_xlabel('Fitted Values')\nax.set_ylabel('√|Standardised Residuals|')\nax.set_title('Scale-Location Plot')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-ipynb/homoscedasticity-plot-output-1.png){#homoscedasticity-plot}\n:::\n:::\n\n\nIf there is an observable trend or pattern in your Scale-Location plot, this indicates heteroscedasticity. \n\n### 4. Residuals vs Leverage (Influential Points)\n\n::: {#cell-outlier-plot .cell execution_count=18}\n``` {.python .cell-code}\nfrom statsmodels.stats.outliers_influence import OLSInfluence\n\ninfluence = OLSInfluence(model)\nleverage = influence.hat_matrix_diag\ncooks_d = influence.cooks_distance[0]\n\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.scatterplot(x=leverage, y=df['standardised_residuals'], alpha=0.6, ax=ax)\nax.axhline(y=0, color='red', linestyle='--', linewidth=1)\nax.set_xlabel('Leverage')\nax.set_ylabel('Standardised Residuals')\nax.set_title('Residuals vs Leverage')\n\n# highlight high influence points\nhigh_influence = cooks_d > 4/len(df)\nif high_influence.any():\n    ax.scatter(leverage[high_influence], df['standardised_residuals'][high_influence], \n               color='red', s=100, alpha=0.6, label='High influence')\n    ax.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-ipynb/outlier-plot-output-1.png){#outlier-plot}\n:::\n:::\n\n\nPoints that are significantly higher or lower than zero and with high leverage are potentially a problem.\n\nHowever, I would caution against just removing outliers because they have a significant influence. If those outliers are real observations, should they really be discarded?\n\n<details>\n  <summary> Quick Diagnostic Function (Click to Expand)</summary> \n\nLet's create a helper function to generate all diagnostic plots at once:\n\n::: {#cell-diagnostics-function .cell execution_count=19}\n``` {.python .cell-code}\ndef plot_diagnostics(model, data=None):\n    \"\"\"\n    Create a 2x2 grid of diagnostic plots for a statsmodels regression model.\n    \n    Parameters:\n    -----------\n    model : statsmodels regression model\n        A fitted statsmodels OLS model\n    data : pandas DataFrame, optional\n        The data used to fit the model (for some plot enhancements)\n    \"\"\"\n    # get residuals and fitted values\n    fitted = model.fittedvalues\n    residuals = model.resid\n    standardised_residuals = model.resid_pearson\n    \n    # calculate leverage and influence\n    influence = OLSInfluence(model)\n    leverage = influence.hat_matrix_diag\n    \n    # create 2x2 plot\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # 1. residuals vs fitted\n    sns.scatterplot(x=fitted, y=residuals, alpha=0.6, ax=axes[0, 0])\n    axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n    axes[0, 0].set_xlabel('Fitted Values')\n    axes[0, 0].set_ylabel('Residuals')\n    axes[0, 0].set_title('Residuals vs Fitted')\n    \n    # 2. q-q plot\n    stats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\n    axes[0, 1].set_title('Q-Q Plot')\n    \n    # 3. scale-location\n    sns.scatterplot(x=fitted, y=np.sqrt(np.abs(standardised_residuals)), alpha=0.6, ax=axes[1, 0])\n    axes[1, 0].set_xlabel('Fitted Values')\n    axes[1, 0].set_ylabel('√|Standardised Residuals|')\n    axes[1, 0].set_title('Scale-Location')\n    \n    # 4. residuals vs leverage\n    sns.scatterplot(x=leverage, y=standardised_residuals, alpha=0.6, ax=axes[1, 1])\n    axes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=1)\n    axes[1, 1].set_xlabel('Leverage')\n    axes[1, 1].set_ylabel('Standardised Residuals')\n    axes[1, 1].set_title('Residuals vs Leverage')\n    \n    plt.tight_layout()\n    plt.show()\n\n# use the function\nplot_diagnostics(model_species_sm)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-ipynb/diagnostics-function-output-1.png){#diagnostics-function}\n:::\n:::\n\n\n</details>\n\n## Checking for Multicollinearity\n\nWhen predictors are highly correlated with each other, coefficient estimates become unstable. We can check this using Variance Inflation Factors (VIF):\n\n::: {#multicollinearity-plot .cell execution_count=20}\n``` {.python .cell-code}\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# calculate VIF for each predictor\n# need to add constant for VIF calculation\nX_with_const = sm.add_constant(df[['flipper_len', 'bill_len', 'bill_dep']])\n\nvif_data = pd.DataFrame()\nvif_data['Variable'] = X_with_const.columns\nvif_data['VIF'] = [variance_inflation_factor(X_with_const.values, i) \n                   for i in range(X_with_const.shape[1])]\n\nprint(\"Variance Inflation Factors:\")\nprint(vif_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVariance Inflation Factors:\n      Variable         VIF\n0        const  691.005294\n1  flipper_len    2.633327\n2     bill_len    1.850958\n3     bill_dep    1.593411\n```\n:::\n:::\n\n\nThe rule of thumb:\n\n- VIF of greater than five = No multicollinearity\n- VIF of between five and ten = Moderate multicollinearity\n- VIF greater than ten = High multicollinearity\n\n## What Happens When Assumptions Are Violated?\n\nUnderstanding assumptions helps you know when you can bend or break the rules.\n\n- Linearity - Coefficients are biased. Try transformations (log, polynomial) or non-linear models.\n- Independence - Standard errors are wrong (usually too small). Use clustered standard errors or multilevel models.\n- Homoscedasticity - Standard errors are wrong. Use robust standard errors (HC3, HC4). Predictions are still unbiased.\n- Normality - With large samples, not a big problem due to central limit theorem. With small samples, confidence intervals may be wrong.\n- Multicollinearity - Coefficients are unstable and hard to interpret, but predictions are fine. Remove or combine correlated predictors.\n\nAs you gain experience, you'll learn when violations matter and when they don't. For now, check diagnostics and flag any concerns.\n\n# Summary\n\n- `sklearn` for prediction - Fast, consistent API for building models.\n- `statsmodels` for inference - Detailed statistical output for understanding.\n-  Assumptions matter - Check diagnostics, understand when violations are problematic.\n- Effect sizes over p-values - Focus on what matters practically.\n\nLinear regression is a powerful tool, and learning the principles of linear regression (thinking about predictions, uncertainty, assumptions, and interpretation) apply to much more complex models.\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n    path: C:\\Users\\paul.johnson\\git\\code-club\\.venv\\share\\jupyter\\kernels\\python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.12.5\n---\n",
    "supporting": [
      "index_files\\figure-ipynb"
    ],
    "filters": []
  }
}