{
  "hash": "2827783218a2847524b89176f10d8613",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Regression Fundamentals: Analysing Relationships\"\nformat:\n  html: default\n  ipynb: default\n---\n\nThis is the second in our series of sessions that builds the regression foundations. Here we will look at how to explore relationships in data, using both quantitative measures such as correlation and a range of visualisations methods.\n\nThis session discusses what it means to analyse relationships between variables, what is possible with different types of variables (and how this links to the previous session that looked at [comparing samples](/sessions/11-comparing-samples)), and what these methods can and cannot tell us.\n\nWe will use the [**Palmer Penguins**](https://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-04-15/readme.md) dataset throughout this session, taken from the [TidyTuesday](https://github.com/rfordatascience/tidytuesday/) GitHub repository (originally from the [palmerpenguins](https://allisonhorst.github.io/palmerpenguins/) R package[^Datasets]). We will import the data directly from GitHub, but if you would prefer to download it instead, click <a href=\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-15/penguins.csv\" download>here</a>.\n\n[^Datasets]:\n\nAnd now a part of the Base R datasets package included as default in R installations (R >= 4.5.0).\n\n## Why Analyse Relationships?\n\nIn the [first session](/sessions/11-comparing-samples) in the regression foundations series we considered how to compare samples, and what comparisons between groups can tell us. We compared fatalities from car crashes between 4/20 and other days. But what if we wanted to examine how the number of fatalities changes as temperature increases or decreases? Or maybe we want to account for the number of cars on the road at different times of year? These questions require different tools.\n\n### From Categorical to Continuous\n\nThe type of variables you are dealing with dictates what you can do to make inferences from your data. In our previous session, comparing samples, we were making comparisons by splitting our data into groups and comparing those groups. We compared the average number of car crash fatalities on 4/20 with the average number of car crash fatalities on all other days of the year. This is comparing the value of a continuous variable (fatalities) but split into categorical groups (4/20 and not 4/20). Here we will compare two continuous variables, considering how one variable (the outcome) changes in response to changes in the other variable (the predictor).\n\nThere is only a subtle difference between the idea of comparing samples and analysing relationships. You can frame a comparison between groups as analysing the relationship between the groups and the continuous variable, but you are still comparing the central tendency[^Central_Tendency] and dispersion[^Dispersion] for each group and inferring the relationship (or association) from this. When comparing two continuous variables, you can't reduce either to their average, and are instead making statements about the way they vary together.\n\n[^Central-Tendency]:\n\nThe central tendency describes the \"typical\" or \"midpoint\" value of your data. There are many ways to measure central tendency, but the most common are the mean (average value), median (middle value), and the mode (most common value).\n\n[^Dispersion]:\n\nDispersion describes how much your data varies. Low dispersion means values are clustered tightly around the midpoint value, while high dispersion means your data can take a wide range of values, some much higher or lower than the midpoint. The most common measure of dispersion is standard deviation, which effectively measures the average distance that values fall from the mean.\n\n#### Variable Types\n\nAt this point, it might be necessary to walk through variable types and what continuous and categorical variables really mean.\n\n\n| Variable Type             | Example Values       | Typical Question        |\n| ------------------------- | -------------------- | ----------------------- |\n| **Continuous**            | 5.2, 7.8, 102.3      | *How much?*             |\n| **Discrete**              | 1, 2, 3, 10          | *How many?*             |\n| **Categorical (Nominal)** | Red, Blue, Green     | *Which type?*           |\n| **Categorical (Ordinal)** | Low, Medium, High    | *Which level?*          |\n| **Binary (Dichotomous)**  | Yes/No, Pass/Fail    | *Yes or no?*            |\n| **Time/Date**             | 2025-06-10, 12:30 PM | *When?*                 |\n| **Identifier**            | ID12345, username987 | *Who or what? (unique)* |\n\nA nominal category has no natural order, while ordinal categories do.\n\n### From Comparing Groups to Estimating Associations\n\nGroup comparison tells us whether Group A's sample mean is meaningfully different from Group B's, but this comparison doesn't really tell us the size of the difference or the pattern in which the differences occur. If two samples are not drawn from the same population distribution, what does this really tell us? It is possible to structure our analysis such that this could be quite meaningful, but in many cases it won't be.\n\nThis is why we need to take the next step, to analysing relationships. How do two variables move together? Strictly speaking, what we will be analysing today is less \"relationships\", which implies causality, and more \"associations\". We are unable to make claims about causality with the methods we are using, because we are only considering how variables vary together, and not directly estimating how one variable causes changes in another. Still, the methods we will discuss here get us one step closer to being able to measure effects and infer causality.\n\nIn this session, weâ€™ll explore the penguins dataset to see how body mass relates to flipper length, how bill length relates to bill depth, and how to identify relationships between continuous traits.\n\n## Penguins with Long Characteristics\n\nWe will use the [**Palmer Penguins**](https://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-04-15/readme.md) dataset to analyse how the length of penguins' body mass changes with flipper length and how penguin bill length is associated with bill depth. We can't draw conclusions about causality from our data, but we are able to move from identifying differences to quantifying those differences. This takes us one step closer to making meaningful inferences.\n\n### Import & Process Data\n\n::: {#cell-import-data .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\n# load penguins data from TidyTuesday URL\nurl = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-15/penguins.csv'\npenguins_raw = pd.read_csv(url)\npenguins_raw.head()\n```\n\n::: {#import-data .cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>species</th>\n      <th>island</th>\n      <th>bill_len</th>\n      <th>bill_dep</th>\n      <th>flipper_len</th>\n      <th>body_mass</th>\n      <th>sex</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.1</td>\n      <td>18.7</td>\n      <td>181.0</td>\n      <td>3750.0</td>\n      <td>male</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>39.5</td>\n      <td>17.4</td>\n      <td>186.0</td>\n      <td>3800.0</td>\n      <td>female</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>40.3</td>\n      <td>18.0</td>\n      <td>195.0</td>\n      <td>3250.0</td>\n      <td>female</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Adelie</td>\n      <td>Torgersen</td>\n      <td>36.7</td>\n      <td>19.3</td>\n      <td>193.0</td>\n      <td>3450.0</td>\n      <td>female</td>\n      <td>2007</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThere are several missing values in this dataset. While we should generally be a little careful when discarding missing values, we will do so here just to simplify the process.\n\n::: {#cell-clean-data .cell execution_count=2}\n``` {.python .cell-code}\n# drop missing values\ndf = penguins_raw.dropna()\ndf.shape\n```\n\n::: {#clean-data .cell-output .cell-output-display execution_count=2}\n```\n(333, 8)\n```\n:::\n:::\n\n\n### Visualing Relationships\n\nLets start by visualising how body mass varies by flipper length. We will split the data by species as well, in order to identify whether penguin species is a confounding factor.\n\n::: {#cell-fig-flipper-mass-scatter-plot .cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['figure.figsize'] = [8,6]\n\n# scatter of flipper length vs. body mass\nsns.scatterplot(\n    data=df,\n    x='flipper_len',\n    y='body_mass',\n    hue='species',\n    alpha=0.7\n)\n\nplt.xlabel('Flipper Length (mm)')\nplt.ylabel('Body Mass (g)')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The Relationship Between Flipper Length & Body Mass](index_files/figure-ipynb/fig-flipper-mass-scatter-plot-output-1.png){#fig-flipper-mass-scatter-plot}\n:::\n:::\n\n\nWhat patterns do you see in @fig-flipper-mass-scatter-plot? How does body mass change when flipper length increases? And how does species moderate the association between flipper length and body mass?\n\n@fig-flipper-mass-scatter-plot shows that, when flipper length increases, body mass appears to increase. There is a clear pattern in the data, and though there are differences in the average value of flipper length and body mass when split by species, the species doesn't appear to have a significant impact on the relationship between flipper length and body mass.\n\nWe can use the same plot to visualise the association between bill length and bill depth.\n\n::: {#cell-fig-bill-scatter-plot .cell execution_count=4}\n``` {.python .cell-code}\n# scatter of bill length vs. bill depth\nsns.scatterplot(\n    data=df,\n    x='bill_len',\n    y='bill_dep',\n    hue='species',\n    alpha=0.7\n)\n\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The Relationship Between Bill Length & Bill Depth](index_files/figure-ipynb/fig-bill-scatter-plot-output-1.png){#fig-bill-scatter-plot}\n:::\n:::\n\n\nHow does the pattern in @fig-bill-scatter-plot differ from @fig-flipper-mass-scatter-plot? What happens to bill depth when bill length changes, and how does species impact these changes?\n\nWhen you look at all the data in @fig-bill-scatter-plot without accounting for species, the relationship appears to be very noisy. However, when factoring in species differences, the association looks positive. \n\nWe can also fit a regression line to our data, shown below in @fig-bill-regression-plot, which will show us the \"line of best fit\" through bill length and bill depth. \n\n::: {#cell-fig-bill-regression-plot .cell execution_count=5}\n``` {.python .cell-code}\n# add linear fit line\nsns.regplot(\n    data=df,\n    x='bill_len',\n    y='bill_dep',\n    scatter=True,\n    ci=95\n)\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The Line of Best Fit Between Bill Length & Bill Depth](index_files/figure-ipynb/fig-bill-regression-plot-output-1.png){#fig-bill-regression-plot}\n:::\n:::\n\n\nIt is clear that any relationship between bill length and bill depth is more complicated than the relationship between flipper length and body mass. Not only does ignoring species make the association between these variables appear a lot more noisy, it also suggests that bill length is negatively associated with bill depth, which is the opposite conclusion to the conclusion we draw from @fig-bill-scatter-plot.\n\n### Computing Correlations\n\nVisualising continuous variables using scatterplots can give us an indication of how two variables change together, but we are unable to quantify this association between two variables only using visualisations. We can put a number to what we are seeing in the plots by calculating the correlation between variables.\n\n#### Pairwise Correlation\n\nWe can compute the correlation between two variables, using `scipy.stats`.\n\n::: {#flipper-mass-correlation .cell execution_count=6}\n``` {.python .cell-code}\nfrom scipy.stats import pearsonr\n\nr, p = pearsonr(df['flipper_len'], df['body_mass'])\nprint(f\"Correlation (r) = {r:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorrelation (r) = 0.87\n```\n:::\n:::\n\n\nA correlation of 0.87 is very strong. There is clearly a very strong association between flipper length and body mass. However, we can't claim that flipper length causes body mass just based off this. Correlation does not imply causation[^Correlation].\n\nWhen we visualised the relationship between bill length and bill depth, there appeared to be a grouping structure going on that complicated things, and the overall relationship appeared pretty noisy.\n\n::: {#bill-correlation .cell execution_count=7}\n``` {.python .cell-code}\nr, p = pearsonr(df['bill_len'], df['bill_dep'])\nprint(f\"Correlation (r) = {r:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorrelation (r) = -0.23\n```\n:::\n:::\n\n\nAs a result, the correlation score is much lower. A correlation of -0.23 tells us two things:\n\n- The negative correlation means that when bill length increases, bill depth tends to decrease.\n- The weaker correlation suggests that this decrease is a lot noisier, and it is much harder to estimate a penguin's bill depth using their bill length.\n\nA correlation of +/- ~0.2 doesn't necessarily mean there is no relationship. There are lots of ways correlation can mislead, because it is a limited measure. Visualising the relationship between bill length and bill depth showed us that species is highly relevant, and not factoring this in limits what we can say about this relationship.\n\n[^Correlation]:\n\nCorrelation might not imply causation, but it is important to realise that the presence of correlation does not mean causation is _not_ present. You just can't conclude causation exists simply because you observe a correlation.\n\n#### Correlation Matrix\n\nWe may be interested in the pairwise correlation between multiple variables. If so, computing each correlation between pairs of variables is very cumbersome. Instead, we can compute a correlation matrix.\n\n::: {#cell-correlation-matrix .cell execution_count=8}\n``` {.python .cell-code}\n# compute correlation matrix\n(\n    df.select_dtypes(include='number')\n    .corr()\n    .round(2)\n)\n```\n\n::: {#correlation-matrix .cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bill_len</th>\n      <th>bill_dep</th>\n      <th>flipper_len</th>\n      <th>body_mass</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>bill_len</th>\n      <td>1.00</td>\n      <td>-0.23</td>\n      <td>0.65</td>\n      <td>0.59</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>bill_dep</th>\n      <td>-0.23</td>\n      <td>1.00</td>\n      <td>-0.58</td>\n      <td>-0.47</td>\n      <td>-0.05</td>\n    </tr>\n    <tr>\n      <th>flipper_len</th>\n      <td>0.65</td>\n      <td>-0.58</td>\n      <td>1.00</td>\n      <td>0.87</td>\n      <td>0.15</td>\n    </tr>\n    <tr>\n      <th>body_mass</th>\n      <td>0.59</td>\n      <td>-0.47</td>\n      <td>0.87</td>\n      <td>1.00</td>\n      <td>0.02</td>\n    </tr>\n    <tr>\n      <th>year</th>\n      <td>0.03</td>\n      <td>-0.05</td>\n      <td>0.15</td>\n      <td>0.02</td>\n      <td>1.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe can also visualise a correlation matrix, shown below in @fig-correlation-matrix-plot.\n\n::: {#cell-fig-correlation-matrix-plot .cell execution_count=9}\n``` {.python .cell-code}\n# add correlation matrix to summarise relationships\ncorr_matrix = df.select_dtypes(include='number').corr()\n\n# plot heatmap\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Correlation Matrix](index_files/figure-ipynb/fig-correlation-matrix-plot-output-1.png){#fig-correlation-matrix-plot}\n:::\n:::\n\n\nIf you are concerned with a certain outcome and you want to quickly look at the correlation between all other continuous variables and the outcome, you can also compute this.\n\n::: {#cell-body-mass-correlations .cell execution_count=10}\n``` {.python .cell-code}\n# correlations of all numeric variables with body mass\n(\n    df.select_dtypes(include='number')\n    .corr()['body_mass']\n    .drop('body_mass')\n    .round(2)\n)\n```\n\n::: {#body-mass-correlations .cell-output .cell-output-display execution_count=10}\n```\nbill_len       0.59\nbill_dep      -0.47\nflipper_len    0.87\nyear           0.02\nName: body_mass, dtype: float64\n```\n:::\n:::\n\n\n### Correlation's Limitations\n\nComputing correlation can be very informative, but there are a lot of ways it is limited. The most common method for calculating correlation, Pearson's $r$, assumes a linear pairwise relationship between variables. This means it is unable to capture certain relationships that do not meet these assumptions.\n\n#### Non-Linearity\n\nA correlation coefficient will miss strong non-linear relationships in data. We can demonstrate this by simulating a parabolic relationship (a u-shaped curve) between two variables, $x$ and $y$, where $y$ is a function of $x$. \n\n::: {#simulate-non-linear-relationship .cell execution_count=11}\n``` {.python .cell-code}\nimport numpy as np\n\n# simulate a uâ€‘shaped relationship example\nx_sim = np.linspace(-3, 3, 200)\ny_sim = x_sim**2 + np.random.normal(0, 1, 200)\nsim_data = pd.DataFrame({'x': x_sim, 'y': y_sim})\n```\n:::\n\n\nWe know that the relationship between $x$ and $y$ is meaningful because we generated $y$ from $x$. However, when we calculate their correlation, it is tiny.\n\n::: {#non-linear-correlation .cell execution_count=12}\n``` {.python .cell-code}\nr, p = pearsonr(sim_data['x'], sim_data['y'])\nprint(f\"Correlation (r) = {r:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorrelation (r) = -0.01\n```\n:::\n:::\n\n\n@fig-non-linear-relationship visualises the relationship between $x$ and $y$, demonstrating that there is clearly a relationship between the two variables.\n\n::: {#cell-fig-non-linear-relationship .cell execution_count=13}\n``` {.python .cell-code}\nsns.scatterplot(data=sim_data, x='x', y='y')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![A Non-Linear Relationship](index_files/figure-ipynb/fig-non-linear-relationship-output-1.png){#fig-non-linear-relationship}\n:::\n:::\n\n\nWhile there are correlation measures that handle non-linearity (such as Spearman's rank correlation and Kendall's tau ($\\tau$)), they are much less common than Pearson's $r$.\n\n#### Complexity\n\nWhile the linearity assumption can cause Pearson's $r$ to miss strong non-linear relationships between variables, the biggest limiting factor with measuring correlations is the pairwise assumption. Correlation compares pairs of variables, which means treating the relationship between those pairs as independent, ignoring potential interactions with other variables. Very few relationships in the real world are strictly pairwise.\n\nWe saw an example of this earlier with correlation missing the relationship between bill length and bill depth because it couldn't account for the group-level species effect.\n\nWe can account for grouping structures in our data visually, by plotting regression lines for each group. @fig-grouping-structures identifies the real story that correlation missed.\n\n::: {#cell-fig-grouping-structures .cell execution_count=14}\n``` {.python .cell-code}\nsns.lmplot(data=df, x=\"bill_len\", y=\"bill_dep\", hue=\"species\", height=6, aspect=1.2)\n\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Accounting for Grouping Structures](index_files/figure-ipynb/fig-grouping-structures-output-1.png){#fig-grouping-structures}\n:::\n:::\n\n\n::: {.callout-note}\nThis also demonstrates why it is important to approach exploratory data analysis in a few different ways. While a correlation coefficient would have missed the relationship between bill length and bill depth, @fig-bill-regression-plot also would have suggested a negative relationship between two variables that are actually positively associated (though it is unclear if the relationship is causal). Meanwhile, @fig-bill-scatter-plot suggested that species has a moderating effect on the relationship between the length of a penguin's bill and its depth, and @fig-grouping-structures confirms the suspicions. \n\n:::\n\nCorrelation will not account for grouping structures, but it will also miss any other way that other variables can complicate the relationship between a pair of variables. Outcomes in the real world are rarely as simple as a pairwise relationship. We need different tools to capture this complexity.\n\n## Summary\n\nVisualising continuous variables and calculating their correlations can tell us a lot. These methods let us move beyond simple group comparisons to examine continuous associations. However, both approaches have important limitations: they struggle with non-linear patterns, they struggle with complexity, and most importantly, they cannot tell us whether the relationship between variables is causal.\n\nReal-world relationships rarely exist in isolation, so we need methods that can handle the complexity that occurs in the wild. This is where we turn to regression. Regression can handle multiple variables, account for confounding factors, and provide a more complete picture of how variables relate to each other in complex systems.\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n    path: c:\\Users\\paul.johnson\\git\\code-club\\.venv\\share\\jupyter\\kernels\\python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.12.5\n---\n",
    "supporting": [
      "index_files\\figure-ipynb"
    ],
    "filters": []
  }
}