{
  "hash": "9582ef4d9cbaae9b062214aa4e1b4561",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Regression Fundamentals: Comparing Samples\"\nformat:\n  html: default\n  ipynb: default\n---\n\nThis notebook is the first in a five-part series covering foundational statistics and the building blocks for regression modelling. This session walks through the process of comparing samples, demonstrating **why** comparisons matter and how we approach them, and applies these ideas using data on fatal car crashes in the U.S.\n\nWe will discuss why we compare groups in data analysis, the differences between population and sample data, how to visualise and interpret group differences, and how to assess whether differences are statistically meaningful.\n\nThe goal of this session is for everyone to understand the role of comparison in statistics, as well as to recognise what comparisons between groups can tell us and what their limitations are.\n\n## Why Compare?\n\nComparison is fundamental to how we learn from data. When we observe something in data, whether it is a specific observation, a broader pattern across observations, or the outcome of a calculation, we need context to understand its meaning. Is this value high or low? Is it unusual? Should we pay more attention to it?\n\nImagine an online store making changes to its website with the intention of boosting sales. After a week, the changes made generated £250k in sales. Is that good? We don't know because we don't have anything to compare against. The store could compare this figure to the sales from the previous week. Better yet, they could run the original version of the website concurrently alongside the new version, serving different versions of the website to users at random, to more directly compare sales. If the original version of the website generated only £230k in sales, we now have a meaningful comparison that suggests that the new version of the site may boost sales.\n\nRaw numbers rarely tell us complete stories. A patient's blood pressure is only meaningful because we know what a healthy range is, and we can use this for comparison. Without comparison, data is meaningless.\n\n### What does Comparison Really Tell Us?\n\nWhen we compare two groups in our data, what we _really_ want to know is whether those groups differ in the real world, not just whether they differ in the data. The comparison in the data serves as a proxy for understanding differences in the wild. The data is a \"sample\" of what the real world (the population) looks like. But suppose we see a difference between two groups in our data. How do we know if that reflects a difference that is occurring in the population, instead of being caused by random variation in the data?\n\nSeparating real patterns, or signal, from the noise in data is a fundamental part of statistics and is the driving force behind everything in statistical inference. Good comparisons account for the possibility of random variation and consider the ways in which the comparisons we are making may be flawed or incomplete. If the online store compared the previous week's sales, this would still be useful, but what if the previous week included a holiday that led to a significant boost of sales, or the week the new site was launched was payday for a lot of customers? It is important to consider whether your comparison is really meaningful.\n\n**Questions:**\n\n- How do you currently decide whether the difference you observe in your data is real or occurred by chance?\n- Why is it important to know if differences observed in data occurred by chance?\n\n### Population vs. Sample\n\nThe population is every possible unit or observation relevant to what you are studying, while the sample is a subset of the population. If you wanted to estimate how income affects housing prices in UK cities, the population would be every city in the UK. A sample would be data covering a handful of (hopefully representative) cities.\n\n![Source: [Martijn Wieling](https://www.let.rug.nl/wieling/Statistiek-I/HC2/)](images/popsample.png)\n\nIf we had access to the entire population, comparisons would be straightforward. However, we usually don't, so we have to take a sample of the population and make inferences about the population based on our sample. That means dealing with uncertainty, variation, and potential bias.\n\nTo compare groups responsibly, we need to consider how sampling affects what we observe and how it may limit our ability to make accurate comparisons. The sample is a small snapshot of the population, and there are several reasons why it might not be representative of the wider population.\n\nBelow is an example illustrating the difference between the population and the sample, simulating drawing ten cards from a standard deck and calculating the average value of the cards drawn.\n\n::: {#population-vs-sample .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport random\n\n# simulate drawing 10 cards from a standard deck\ndeck = list(range(1, 14)) * 4\n\n# draw two random samples of ten cards\nsample1 = random.sample(deck, 5)\nsample2 = random.sample(deck, 5)\n\n# compute sample means\nsample_mean1 = np.mean(sample1)\nsample_mean2 = np.mean(sample2)\n\n# compute population mean\npopulation_mean = np.mean(deck)\n\nprint(f\"Sample means: {sample_mean1}, {sample_mean2}\")\nprint(f\"Population mean: {population_mean}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSample means: 5.2, 7.4\nPopulation mean: 7.0\n```\n:::\n:::\n\n\nWe have taken two samples from the population. Their mean values are 5\\.2 and 5\\.2, which vary slightly from the population mean (7\\.0). Why do the sample means differ from each other and the population mean?\n\nSampling variability is inevitable. Each sample captures only a slice of the full population, and in small samples, this can lead to significant variances in the sample and population means. Perhaps the first ten cards drawn from the deck have a high number of face cards, or the second sample has lots of 2s, 3s, and 4s. Even if the process for drawing a sample is fair, individual samples will always vary. This is a core challenge of inference. We rely on well-designed comparisons to manage these uncertainties, using statistical tools that help us determine whether sample-level observations likely reflect real population-level differences.\n\n## Comparing Car Crash Fatalities - High or Not?\n\nNow we can apply this logic to a real-world dataset. We will use a dataset that records the daily count of fatal U.S. car crashes from 1992–2016, taken from a [study into the effects of the annual cannabis holiday, 4/20, on fatal car accidents](https://injuryprevention.bmj.com/content/25/5/433). Previous research has concluded that fatalities are higher on 4/20, suggesting that the holiday is the cause of the increase.\n\nWe are using a dataset, provided by [Tidy Tuesday](https://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-04-22/readme.md), which includes an indicator for April 20th (4/20). We will investigate whether 4/20 sees more crashes than expected.\n\n### Import & Process Data\n\n::: {#cell-import-data .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\n\n# load data\nraw_420 = pd.read_csv('data/daily_accidents_420.csv', parse_dates=['date'])\n\n# inspect data\nraw_420.head()\n```\n\n::: {#import-data .cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>e420</th>\n      <th>fatalities_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1992-01-01</td>\n      <td>False</td>\n      <td>144</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1992-01-02</td>\n      <td>False</td>\n      <td>111</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1992-01-07</td>\n      <td>False</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1992-01-12</td>\n      <td>False</td>\n      <td>127</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1992-01-03</td>\n      <td>False</td>\n      <td>182</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#cell-count-missing-values .cell execution_count=3}\n``` {.python .cell-code}\n# count missing values\nraw_420.isna().sum()\n```\n\n::: {#count-missing-values .cell-output .cell-output-display execution_count=3}\n```\ndate                 0\ne420                13\nfatalities_count     0\ndtype: int64\n```\n:::\n:::\n\n\n::: {#cell-inspect-missing-values .cell execution_count=4}\n``` {.python .cell-code}\n# inspect missing values\nraw_420[raw_420['e420'].isna()]\n```\n\n::: {#inspect-missing-values .cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>e420</th>\n      <th>fatalities_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1099</th>\n      <td>1994-04-20</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1428</th>\n      <td>1995-04-20</td>\n      <td>NaN</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1834</th>\n      <td>1996-04-20</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2201</th>\n      <td>1997-04-20</td>\n      <td>NaN</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3301</th>\n      <td>2000-04-20</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4400</th>\n      <td>2003-04-20</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5485</th>\n      <td>2006-04-20</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5867</th>\n      <td>2007-04-20</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6234</th>\n      <td>2008-04-20</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6968</th>\n      <td>2010-04-20</td>\n      <td>NaN</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>7702</th>\n      <td>2012-04-20</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8393</th>\n      <td>2014-04-20</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8802</th>\n      <td>2015-04-20</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe will transform the data to remove missing values, make the column names more meaningful, and select the columns we want to use in our analysis. \n\n::: {#transform-data .cell execution_count=5}\n``` {.python .cell-code}\ndf = (\n    raw_420\n    # convert e420 to boolean and rename\n    .assign(is_420=raw_420['e420'].astype(bool))\n    # drop missing values\n    .dropna()\n    # select relevant columns\n    [['date', 'is_420', 'fatalities_count']]\n)\n```\n:::\n\n\n::: {#cell-check-data .cell execution_count=6}\n``` {.python .cell-code}\ndf.head()\n```\n\n::: {#check-data .cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>is_420</th>\n      <th>fatalities_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1992-01-01</td>\n      <td>False</td>\n      <td>144</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1992-01-02</td>\n      <td>False</td>\n      <td>111</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1992-01-07</td>\n      <td>False</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1992-01-12</td>\n      <td>False</td>\n      <td>127</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1992-01-03</td>\n      <td>False</td>\n      <td>182</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Visual Comparisons\n\nWe can start by plotting our data to better understand how fatalities vary, and using visual comparisons between fatalities on 4/20 and other days.\n\n::: {#cell-group-means .cell execution_count=7}\n``` {.python .cell-code}\ndf.groupby('is_420')['fatalities_count'].mean()\n```\n\n::: {#group-means .cell-output .cell-output-display execution_count=7}\n```\nis_420\nFalse    144.92477\nTrue      54.60000\nName: fatalities_count, dtype: float64\n```\n:::\n:::\n\n\n::: {#cell-time-series .cell execution_count=8}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(15, 8))\n\n# line plot of daily fatalities\nsns.lineplot(data=df, x='date', y='fatalities_count', color=\"#0081a7\", linewidth=0.5)\n\n# scatter plot for 4/20 days, filter using .loc to avoid NA issues\nsns.scatterplot(\n    data=df.loc[df['is_420'] == True],\n    x='date', y='fatalities_count',\n    color='#ef233c', label='4/20'\n    )\n\nplt.title('Daily Fatalities (1992-2016)')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-ipynb/time-series-output-1.png){#time-series}\n:::\n:::\n\n\n::: {#cell-raw-distributions .cell execution_count=9}\n``` {.python .cell-code}\nplt.figure(figsize=(12, 6))\n\n# define colour palette\ncustom_palette = {False: '#0081a7', True: '#ef233c'}\n\n# histogram\nplt.subplot(1, 2, 1)\nsns.histplot(data=df, x='fatalities_count', hue='is_420', kde=True, palette=custom_palette)\nplt.title('Histogram')\n\n# boxplot\nplt.subplot(1, 2, 2)\nsns.boxplot(data=df, x='is_420', y='fatalities_count', hue='is_420', palette=custom_palette, legend=False)\nplt.xticks([0, 1], ['Other days', '4/20'])\nplt.title('Boxplot')\n\n# figure title\nplt.suptitle('Distribution of Fatalities', fontsize=14)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-ipynb/raw-distributions-output-1.png){#raw-distributions}\n:::\n:::\n\n\nThe imbalance between 4/20 and other days in the year makes it impossible to really see what is going on in our histogram. We can normalise the two distributions such that the total area of both equals one. This preserves their shape but accounts for the count imbalance between the two.\n\nWe can also replace the boxplot with a violin plot, which will give us a little more intuition for the shape of the two groups.\n\n::: {#cell-normalised-distributions .cell execution_count=10}\n``` {.python .cell-code}\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nsns.histplot(data=df, x='fatalities_count', hue='is_420', palette=custom_palette, kde=True, stat='density', common_norm=False)\nplt.title('Density-Normalised Histogram')\n\nplt.subplot(1, 2, 2)\nsns.violinplot(data=df, x='is_420', y='fatalities_count', inner='quart', hue='is_420', palette=custom_palette, legend=False)\nplt.xticks([0, 1], ['Other days', '4/20'])\nplt.title('Violin Plot')\n\nplt.suptitle('Distribution of Fatalities', fontsize=14)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-ipynb/normalised-distributions-output-1.png){#normalised-distributions}\n:::\n:::\n\n\nWhat do you notice about the distribution (centre, spread, outliers, etc.)?\n\n### Testing Comparisons\n\nVisual and descriptive comparisons are limited because they only tell us whether there is a difference. They don't help us infer whether those difference occurred due to random variation or if there is something real going on. Visual comparisons cannot tell us whether we should expect to observe the differences we see in our samples in the population.\n\nThat's where statistical tests come in! Once we’ve visualized potential differences, we can test whether they’re statistically significant, using a two-sample *t*-test.\n\nA *t*-test is a statistical test used to compare the means of two groups to determine if the difference between them is statistically significant. It takes into account:\n\n- The size of the difference between the two group means.\n- The variability (spread) of the data within each group.\n- The sample size (number of observations in each group).\n\nThe *t*-test calculates a p-value, which is the probability of observing a difference as extreme or more extreme than the one found, assuming there is no true difference between the groups in the population (i.e., the null hypothesis is true). If the p-value is small enough (below a threshold like 0.05), you reject the null hypothesis and conclude that the difference between the groups is likely real and not due to random chance. If we compute a correlation of 0.45 with a *p*-value of 0.01, that means we’d expect to see a correlation this strong less than 1% of the time by chance alone, assuming no true relationship.\n\n{{< video https://youtu.be/0oc49DyA3hU?si=0x24ncYVQKbJP2sY >}}\n\n::: {#t-test .cell execution_count=11}\n``` {.python .cell-code}\nfrom scipy.stats import ttest_ind\n\n# create our samples for comparison\ngroup_420 = df.loc[df.is_420, 'fatalities_count']\ngroup_other = df.loc[~df.is_420, 'fatalities_count']\n\n# calculate t-statistic and p-value\nt_stat, p_val = ttest_ind(group_420, group_other, equal_var=False)\nprint(f\"t-statistic = {t_stat:.3f}, p-value = {p_val:.3f}\")\n\n# calculate mean difference\nmean_diff = group_420.mean() - group_other.mean()\n# calculate standard errors\nse_diff = np.sqrt(\n    group_420.var(ddof=1)/len(group_420)\n    + group_other.var(ddof=1)/len(group_other)\n    )\n\n# ci_lower = mean_diff - 1.96 * se_diff\n# ci_upper = mean_diff + 1.96 * se_diff\n# print(f\"Mean difference = {mean_diff:.2f} (95% CI: {ci_lower:.2f}, {ci_upper:.2f})\")\n\nprint(f\"Mean difference = {mean_diff:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nt-statistic = -29.369, p-value = 0.000\nMean difference = -90.32\n```\n:::\n:::\n\n\nIf *p* < 0.05, we reject the null that there is no true difference between the groups in the population. The *p*-value is below 0.05 — suggesting that 4/20 days see **significantly fewer** fatal crashes than other days. The evidence supports a real difference.\n\n{{< video https://youtu.be/vemZtEM63GY?si=8l3GMwdxxdUI2Rod >}}\n\n::: {.callout-note}\n\n#### Why Significance Is a Thorny Issue\n\nWhile *p*-values are widely used, they come with caveats:\n\n- Binary thinking - Treating *p* < 0.05 as “proof” and *p* > 0.05 as “nothing” is misleading. Evidence exists on a continuum.\n- Sample size matters - With huge samples, even tiny effects can be “significant.” With small samples, meaningful effects might not reach the threshold.\n- Misinterpretation - A *p*-value doesn’t tell us how big or important an effect is, nor the probability that the hypothesis is true.\n- Multiple testing - The more tests we run, the more likely we’ll find “significant” results by chance (false positives).\n\nThe better approach is to treat *p*-values as one piece of evidence. Pair them with effect sizes, confidence intervals, visualizations, and context.\n\n::: \n\n<details>\n  <summary>Using Simulation for Comparisons (Click to Expand)</summary>\n\n#### Simulation-Based Tests\n\nLet’s briefly replicate the comparison using a simulation-based method. This avoids strong distributional assumptions.\n\n::: {#simulation-function .cell execution_count=12}\n``` {.python .cell-code}\ndef simulate_two_groups(data1, data2):\n\n    n, m = len(data1), len(data2)\n    data = np.append(data1, data2)\n    np.random.shuffle(data)\n    group1 = data[:n]\n    group2 = data[n:]\n    return group1.mean() - group2.mean()\n```\n:::\n\n\n::: {#cell-simulation-test .cell execution_count=13}\n``` {.python .cell-code}\n# run 5000 simulations to test null\nnp.random.seed(42)\nsimulated_diffs = [simulate_two_groups(group_420, group_other) for _ in range(5000)]\n\n# observed mean difference\nobserved_diff = group_420.mean() - group_other.mean()\n\n# calculate p-value\ndiffs = np.array(simulated_diffs)\np_sim = np.mean(np.abs(diffs) >= np.abs(observed_diff))\n\n# plot distribution of simulated differences with p-value\nplt.figure(figsize=(12, 6))\nsns.histplot(diffs, kde=True, color='#0081a7')\nplt.axvline(observed_diff, color='#ef233c', linewidth=3, linestyle=\"--\", label='Observed Mean Difference')\nplt.legend(loc='upper right')\nplt.title('Simulated Mean Differences (Permutation Test)')\n\n# annotate p-value on the plot\nplt.text(\n    x=observed_diff+5,\n    y=plt.gca().get_ylim()[1]*0.9,\n    s=f'p-value = {p_sim:.4f}'\n    )\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-ipynb/simulation-test-output-1.png){#simulation-test}\n:::\n:::\n\n\nSimulation confirms the result and emphasizes flexibility: even when assumptions are questionable, we can still test meaningfully. It also reinforces that inference is about what would happen if we repeated the experiment many times.\n\n</details>\n\n## Limitations of Comparison\n\nComparing samples of data can be very useful. There is descriptive value in just knowing that differences exist in the data, and this may point to a meaningful difference in the population. However, if you are trying to understand what _caused_ the differences between the two samples, comparison is not enough.\n\nOur analysis shows that there are fewer crashes on 4/20 than other days, but is the comparison we are making fair? Our comparison assumes that the only difference between the 4/20 and other days in the data is the date itself, and the cultural holiday that takes place on this date. We haven't accounted for other causes of variation in the number of crashes. We haven't accounted for other holidays, weather patterns, or daily differences in the number of people on the road.\n\nComparisons only measure what we observe, not necessarily what we want to know. It is important to consider how your comparison might not answer the question you are actually asking. And it is important to consider ways that your comparison may be flawed, and what else may be going on in the data.\n\n## Wrapping Up\n\nWe've walked the workflow for carrying out comparisons using statistical methods. With these methods we can make meaningful comparisons between samples in our data. This gives us a solid foundation for carrying out analysis. We can identify differences, quantify uncertainty, and make inferences from data. But these methods have limitations. They don’t account for multiple variables or continuous predictors. Context and sample size also matter.\n\nIn future sessions we will take this a step further, analysing how variables relate to each other. That lets us ask new types of questions. We’ll explore how variables change together, detect trends, and lay the foundation for regression.\n\n**Potential extension:** Apply today's workflow to July 4th or New Year's Day. Do these dates show different crash patterns? What factors might explain any differences you find? This practice will deepen your intuition for when group comparisons work well and when more sophisticated approaches are needed.\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n    path: c:\\Users\\paul.johnson\\git\\code-club\\.venv\\share\\jupyter\\kernels\\python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.12.5\n---\n",
    "supporting": [
      "index_files\\figure-ipynb"
    ],
    "filters": []
  }
}