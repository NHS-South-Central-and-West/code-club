{
  "hash": "e049ead22540eefb051c13a8fee1fef3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Regression Fundamentals: Beyond Linearity\"\nformat:\n  html: default\n  ipynb: default\n---\n\nIn previous sessions, we learned how linear regression works: fit a straight line to data, minimise prediction error, and use that line for prediction and inference. Linear regression is powerful, but it assumes relationships are linear. In reality, data often follows curves, has constraints, or exhibits patterns that straight lines can't capture.\n\nThis session explores how the linear regression framework adapts to non-linear patterns through **link functions**. We'll see that the same core idea—fitting a model to data—works for binary outcomes, counts, and other constrained data types. The key is transforming the scale so linear models can work.\n\nWe'll focus on **logistic regression** for binary outcomes, implement it with real data, and briefly introduce other extensions like multilevel models and GAMs.\n\n# Slides\n\nUse the left ⬅️ and right ➡️ arrow keys to navigate through the slides below. To view in a separate tab/window, [follow this link](slides.html){target=\"_blank\"}.\n\n<iframe src=\"slides.html\" height=\"500\" width=\"100%\"></iframe>\n\n## When Linear Regression Fails\n\nLinear regression assumes the relationship between predictors and outcome is linear. This works when each unit increase in $X$ produces a constant change in $Y$. But many real-world relationships don't follow this pattern.\n\n### Examples of Non-Linear Data\n\n- Binary outcomes - Survived/died, yes/no, pass/fail. Outcomes are 0 or 1, not continuous.\n- Count data - Number of hospital visits, customer complaints. Must be non-negative integers.\n- Proportions - Percentage passing an exam, recovery rates. Bounded between 0 and 1.\n- Growth curves - Disease spread, population growth. Exponential or S-shaped patterns.\n\nForcing linear regression onto these data types produces nonsensical predictions (probabilities above 1, negative counts) and violates model assumptions.\n\n## The Problem Illustrated\n\n::: {#cell-problem-demo .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\n\n# set visualisation style\nsns.set_theme(style=\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# simulate binary outcome data\nnp.random.seed(42)\nage = np.linspace(20, 80, 100)\nprob_true = 1 / (1 + np.exp(-(age - 50) / 8))\ndisease = np.random.binomial(1, prob_true)\n\n# fit linear regression\nmodel_linear = LinearRegression()\nmodel_linear.fit(age.reshape(-1, 1), disease)\npred_linear = model_linear.predict(age.reshape(-1, 1))\n\n# plot\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.scatterplot(x=age, y=disease, facecolors='white', edgecolors='black',\n                s=60, linewidths=1, alpha=0.7, ax=ax)\nsns.lineplot(x=age, y=pred_linear, color='#ED8B00', linewidth=2, ax=ax, label='Linear Model')\nax.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Valid Range')\nax.axhline(y=1, color='red', linestyle='--', alpha=0.5)\nax.set_xlabel('Age')\nax.set_ylabel('Disease (0 = No, 1 = Yes)')\nax.set_title('Linear Regression on Binary Data: Predictions Outside Valid Range')\nax.set_ylim(-0.2, 1.2)\nax.legend()\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/problem-demo-output-1.png){#problem-demo}\n:::\n:::\n\n\nLinear regression predicts values below 0 and above 1, which make no sense for binary outcomes.\n\n## The Solution - Link Functions\n\nLink functions transform the scale so linear models work. Instead of fitting a line directly to constrained data, we:\n\n1. Transform the outcome to an unbounded scale\n2. Fit a linear model on the transformed scale\n3. Transform predictions back to the original scale\n\nThis preserves the linearity of the model while respecting data constraints.\n\n<details>\n  <summary> How Logistic Regression Works (Click to Expand)</summary> \n\n### The Logit Link\n\nFor binary outcomes, we use the logit link, which models the log-odds.\n\n$$\n\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1X\n$$\n\nThe left side of this equation is the log-odds for an outcome of probability $p$. This can be any value from $-\\infty$ to $+\\infty$. This is equivalent to the right side of the equation, which is the linear predictor we have encountered in previous sessions. \n\nTo get probabilities, we transform back using the inverse logit.\n\n$$\np = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X)}}\n$$\n\nThis produces an S-shaped curve that stays between 0 and 1.\n\n### Why Log-Odds?\n\nOdds represent the ratio of success to failure: $\\frac{p}{1-p}$. If $p = 0.8$, odds are $\\frac{0.8}{0.2} = 4$ (4 to 1 in favour). Taking the log makes this unbounded:\n\n- $p = 0.5$ → odds = 1 → log-odds = 0\n- $p$ close to 0 → odds close to 0 → log-odds $\\to -\\infty$\n- $p$ close to 1 → odds $\\to \\infty$ → log-odds $\\to +\\infty$\n\nNow we can fit a linear model on the log-odds scale.\n\n</details>\n\n## Logistic Regression in Practice\n\nLet's implement logistic regression using the Titanic dataset. Our goal is to predict passenger survival based on characteristics like age, sex, and passenger class.\n\n### Load and Prepare Data\n\n::: {#load-titanic .cell execution_count=2}\n``` {.python .cell-code}\n# load titanic data\nurl = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\ntitanic = pd.read_csv(url)\n\n# select relevant columns and drop missing values\ndf = titanic[['Survived', 'Pclass', 'Sex', 'Age', 'Fare']].dropna()\n\n# convert sex to binary\ndf['Sex'] = (df['Sex'] == 'female').astype(int)\n\nprint(f\"Dataset shape: {df.shape}\")\nprint(\"\\nFirst few rows:\")\nprint(df.head(10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDataset shape: (714, 5)\n\nFirst few rows:\n    Survived  Pclass  Sex   Age     Fare\n0          0       3    0  22.0   7.2500\n1          1       1    1  38.0  71.2833\n2          1       3    1  26.0   7.9250\n3          1       1    1  35.0  53.1000\n4          0       3    0  35.0   8.0500\n6          0       1    0  54.0  51.8625\n7          0       3    0   2.0  21.0750\n8          1       3    1  27.0  11.1333\n9          1       2    1  14.0  30.0708\n10         1       3    1   4.0  16.7000\n```\n:::\n:::\n\n\n### Exploratory Data Analysis\n\nBefore modelling, examine the relationships between predictors and survival.\n\n::: {#eda-survival-rate .cell execution_count=3}\n``` {.python .cell-code}\n# overall survival rate\nsurvival_rate = df['Survived'].mean()\nprint(f\"Overall survival rate: {survival_rate:.1%}\")\n\n# survival by sex\nsurvival_sex = df.groupby('Sex')['Survived'].mean()\nprint(\"\\nSurvival rate by sex:\")\nprint(survival_sex.to_frame().rename(columns={'Survived': 'Survival Rate'}))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOverall survival rate: 40.6%\n\nSurvival rate by sex:\n     Survival Rate\nSex               \n0         0.205298\n1         0.754789\n```\n:::\n:::\n\n\n::: {#cell-eda-plots .cell execution_count=4}\n``` {.python .cell-code}\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# survival by sex\nsurvival_sex = df.groupby('Sex')['Survived'].mean()\naxes[0, 0].bar(['Male', 'Female'], survival_sex.values, color=['#005EB8', '#ED8B00'])\naxes[0, 0].set_ylabel('Survival Rate')\naxes[0, 0].set_title('Survival Rate by Sex')\naxes[0, 0].set_ylim(0, 1)\n\n# survival by class\nsurvival_class = df.groupby('Pclass')['Survived'].mean()\naxes[0, 1].bar(survival_class.index, survival_class.values, color='#005EB8')\naxes[0, 1].set_xlabel('Passenger Class')\naxes[0, 1].set_ylabel('Survival Rate')\naxes[0, 1].set_title('Survival Rate by Class')\naxes[0, 1].set_ylim(0, 1)\n\n# age distribution by survival\nsns.histplot(data=df, x='Age', hue='Survived', bins=30, ax=axes[1, 0], \n             palette={0: '#ED8B00', 1: '#005EB8'}, alpha=0.6)\naxes[1, 0].set_title('Age Distribution by Survival')\naxes[1, 0].set_xlabel('Age')\naxes[1, 0].legend(['Died', 'Survived'])\n\n# fare distribution by survival\nsns.histplot(data=df, x='Fare', hue='Survived', bins=30, ax=axes[1, 1],\n             palette={0: '#ED8B00', 1: '#005EB8'}, alpha=0.6, log_scale=True)\naxes[1, 1].set_title('Fare Distribution by Survival (Log Scale)')\naxes[1, 1].set_xlabel('Fare (£)')\naxes[1, 1].legend(['Died', 'Survived'])\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/eda-plots-output-1.png){#eda-plots}\n:::\n:::\n\n\nWomen, first-class passengers, and children had higher survival rates. Fare correlates with class.\n\n### Fitting a Logistic Regression Model\n\nWe'll fit two models: a simple model with only sex as a predictor, and a multiple model with all predictors.\n\n::: {#fit-logistic .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# prepare data\nX = df[['Sex', 'Age', 'Pclass', 'Fare']]\ny = df['Survived']\n\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# fit simple model (sex only)\nmodel_simple = LogisticRegression(max_iter=1000)\nmodel_simple.fit(X_train[['Sex']], y_train)\n\n# fit multiple model (all predictors)\nmodel_full = LogisticRegression(max_iter=1000)\nmodel_full.fit(X_train, y_train)\n\nprint(\"Models fitted successfully\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModels fitted successfully\n```\n:::\n:::\n\n\n### Interpreting Coefficients\n\n::: {#coefficients .cell execution_count=6}\n``` {.python .cell-code}\nimport statsmodels.api as sm\n\n# use statsmodels for detailed output\nX_train_sm = sm.add_constant(X_train)\nmodel_sm = sm.Logit(y_train, X_train_sm).fit()\n\nprint(model_sm.summary().tables[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.438711\n         Iterations 6\n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.7586      0.613      4.497      0.000       1.556       3.961\nSex            2.6862      0.240     11.179      0.000       2.215       3.157\nAge           -0.0438      0.009     -4.970      0.000      -0.061      -0.026\nPclass        -1.3116      0.187     -7.021      0.000      -1.678      -0.945\nFare          -0.0003      0.003     -0.094      0.925      -0.006       0.005\n==============================================================================\n```\n:::\n:::\n\n\nCoefficients are on the log-odds scale. Positive coefficients increase the probability of survival, negative coefficients decrease it.\n\n- Sex (Female) - Coefficient = 2\\.69. Being female increases log-odds of survival. On the probability scale, women had much higher survival rates.\n- Age - Coefficient = \\-0\\.044. Younger passengers had slightly higher survival rates.\n- Pclass - Coefficient = \\-1\\.31. Higher class numbers (lower class) decrease survival probability.\n- Fare - Coefficient = \\-0\\.000. Higher fares (correlated with class) increase survival.\n\n### Converting Coefficients to Odds Ratios\n\nExponentiate coefficients to get **odds ratios**.\n\n::: {#odds-ratios .cell execution_count=7}\n``` {.python .cell-code}\nodds_ratios = np.exp(model_sm.params)\nconf_int = np.exp(model_sm.conf_int())\n\nresults_df = pd.DataFrame({\n    'Odds Ratio': odds_ratios,\n    '95% CI Lower': conf_int[0],\n    '95% CI Upper': conf_int[1]\n}).round(2)\n\nprint(\"\\nOdds Ratios:\")\nprint(results_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nOdds Ratios:\n        Odds Ratio  95% CI Lower  95% CI Upper\nconst        15.78          4.74         52.50\nSex          14.68          9.16         23.50\nAge           0.96          0.94          0.97\nPclass        0.27          0.19          0.39\nFare          1.00          0.99          1.01\n```\n:::\n:::\n\n\n- Sex - Being female multiplies the odds of survival by ~11. Women were much more likely to survive.\n- Age - Each year of age multiplies odds by ~0.97 (slight decrease).\n- Pclass - Each class decrease (e.g., 2nd to 3rd) multiplies odds by ~0.33.\n\n### Making Predictions\n\n::: {#predictions .cell execution_count=8}\n``` {.python .cell-code}\n# predict probabilities on test set\ny_pred_prob = model_full.predict_proba(X_test)[:, 1]\ny_pred = model_full.predict(X_test)\n\n# model performance\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test set accuracy: {accuracy:.1%}\")\n\n# confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\")\nprint(pd.DataFrame(cm, \n                   columns=['Predicted: Died', 'Predicted: Survived'],\n                   index=['Actual: Died', 'Actual: Survived']))\n\n# classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Died', 'Survived']))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest set accuracy: 75.5%\n\nConfusion Matrix:\n                  Predicted: Died  Predicted: Survived\nActual: Died                   68                   19\nActual: Survived               16                   40\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Died       0.81      0.78      0.80        87\n    Survived       0.68      0.71      0.70        56\n\n    accuracy                           0.76       143\n   macro avg       0.74      0.75      0.75       143\nweighted avg       0.76      0.76      0.76       143\n\n```\n:::\n:::\n\n\nThe model correctly classifies ~76% of passengers. It performs better at predicting deaths than survivals, likely because more passengers died overall.\n\n### Visualising Predictions\n\n::: {#cell-prediction-plot .cell execution_count=9}\n``` {.python .cell-code}\n# create a grid of ages and predict survival probability for male/female passengers\nage_range = np.linspace(0, 80, 100)\n\n# predictions for male, 3rd class passengers\nX_male = pd.DataFrame({\n    'Sex': 0,\n    'Age': age_range,\n    'Pclass': 3,\n    'Fare': df['Fare'].median()\n})\n\n# predictions for female, 3rd class passengers\nX_female = pd.DataFrame({\n    'Sex': 1,\n    'Age': age_range,\n    'Pclass': 3,\n    'Fare': df['Fare'].median()\n})\n\nprob_male = model_full.predict_proba(X_male)[:, 1]\nprob_female = model_full.predict_proba(X_female)[:, 1]\n\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.lineplot(x=age_range, y=prob_male, color='#005EB8', linewidth=2, \n             label='Male, 3rd Class', ax=ax)\nsns.lineplot(x=age_range, y=prob_female, color='#ED8B00', linewidth=2,\n             label='Female, 3rd Class', ax=ax)\nax.set_xlabel('Age')\nax.set_ylabel('Probability of Survival')\nax.set_title('Predicted Survival Probability by Age and Sex (3rd Class)')\nax.set_ylim(0, 1)\nax.legend()\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/prediction-plot-output-1.png){#prediction-plot}\n:::\n:::\n\n\nFemale passengers had much higher predicted survival probabilities across all ages. Younger passengers (especially children) had higher survival rates, consistent with \"women and children first\".\n\n## Other Extensions of Linear Regression\n\nLogistic regression is one example of adapting the linear framework to non-linear data. Here are other common extensions.\n\n### Poisson Regression for Count Data\n\nWhen outcomes are counts (number of events, visits, occurrences), Poisson regression uses a log link.\n\n$$\n\\log(\\text{E}[Y]) = \\beta_0 + \\beta_1X\n$$\n\nTransform back to get expected count: $\\text{E}[Y] = e^{\\beta_0 + \\beta_1X}$. Predictions are always positive, matching count data.\n\n**Example use cases - Number of hospital admissions, customer complaints, goals scored.\n\n### Generalised Additive Models (GAMs)\n\nGAMs fit smooth, flexible curves instead of straight lines.\n\n$$\nY = \\beta_0 + f_1(X_1) + f_2(X_2) + \\epsilon\n$$\n\nEach $f_i(X_i)$ is a smooth function that adapts to the data. GAMs capture complex non-linear patterns (U-shapes, wiggles) while remaining interpretable.\n\nExample use cases - Temperature effects on sales, age-related health trends, non-linear dose-response relationships.\n\nPython implementation uses `pygam`:\n\n```python\nfrom pygam import GAM, s\n\n# fit a GAM with smooth functions\ngam = GAM(s(0) + s(1))\ngam.fit(X, y)\n```\n\n### Multilevel (Mixed-Effects) Models\n\nWhen data has **hierarchical structure** (students in schools, patients in hospitals, repeated measures on individuals), **multilevel models** account for grouping:\n\n$$\nY_{ij} = \\beta_0 + u_j + \\beta_1X_{ij} + \\epsilon_{ij}\n$$\n\n- $u_j$ is a group-specific effect (random intercept)\n- Groups can have different baselines but share information\n\nExample use cases - Educational data (students in schools), clinical trials (patients in sites), longitudinal data (repeated measures on individuals).\n\nPython implementation uses `statsmodels` or `pymer4`:\n\n```python\nimport statsmodels.formula.api as smf\n\n# fit a mixed-effects model\nmodel = smf.mixedlm(\"y ~ x\", data=df, groups=df[\"group_id\"])\nresult = model.fit()\n```\n\n### Survival Analysis\n\nWhen modelling time until an event (death, failure, recovery), use survival models like Cox proportional hazards regression. These account for censoring (observations where the event hasn't occurred yet).\n\nExample use cases - Patient survival times, equipment failure, customer churn.\n\n## Why This Matters\n\nAll these models share the same foundation: linear regression. Once you understand fitting a line to data, you can:\n\n- Use link functions for constrained outcomes (binary, counts, proportions)\n- Fit flexible curves with GAMs\n- Account for hierarchy with multilevel models\n- Model time-to-event data with survival analysis\n\nThe linear framework is incredibly versatile. It's not just for linear relationships.\n\n## Summary\n\n- Real data often violates linear regression assumptions\n- Link functions transform data so linear models work\n- Logistic regression (logit link) handles binary outcomes\n- Other GLMs (Poisson, etc.) handle counts and constrained data\n- Extensions like GAMs and multilevel models add flexibility\n- The linear framework adapts to almost any problem\n\nLinear regression isn't just a starting point. It's a foundation you can build on for almost any modelling task.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}