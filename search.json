[
  {
    "objectID": "sessions/06-data-types/index.html",
    "href": "sessions/06-data-types/index.html",
    "title": "An Introduction to Data Types",
    "section": "",
    "text": "This session is the first in a series of programming fundamentals. We recognise that this content might be a bit more dry and abstract, but it is important background to know when you start to actually use Python in your day to day work.\nIf you’ve used Excel and changed the data format for a cell, you’ve already come across data types! It is important to understand how Python stores values in variables and the pitfalls, gotchas and errors you may come across when working with data. The slide deck below gives a (little) bit of history before giving an overview of how data types work in Python. On the last slides are some links to useful resources on the web, which you may want to make note of for the future. Below the slides is a live notebook that demonstrates this, with some exercises at the end to check your understanding.",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#automatically",
    "href": "sessions/06-data-types/index.html#automatically",
    "title": "An Introduction to Data Types",
    "section": "Automatically",
    "text": "Automatically\nPython automatically assigns a type to a variable based on the value we put into it when we use the = assignment operator.\n\nour_integer = 1\nour_float = 2.2\nour_integer_turned_into_a_float = float(our_integer)\nour_string=\"Hello SCW!\"",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#manually",
    "href": "sessions/06-data-types/index.html#manually",
    "title": "An Introduction to Data Types",
    "section": "Manually",
    "text": "Manually\nIf we need to, we can use a constructor function named after the data type, like int() or str() to force a variable to be the specific type we need it to be.\n\na = str(\"123\") # a will contain the string 123 rather than the numeric value\nb = float(2) # b will contain the decimal value 2.0\nc = int(1.9) # just throws away the .9; not rounded!",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#finding-out-what-type-a-variable-is",
    "href": "sessions/06-data-types/index.html#finding-out-what-type-a-variable-is",
    "title": "An Introduction to Data Types",
    "section": "Finding out what type a variable is",
    "text": "Finding out what type a variable is\n\nprint (type(a)) # output: &lt;class 'str'&gt;\nprint (type(b)) # output: &lt;class 'float'&gt;\nprint (type(c)) # output: &lt;class 'int'&gt;",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#booleans",
    "href": "sessions/06-data-types/index.html#booleans",
    "title": "An Introduction to Data Types",
    "section": "Booleans",
    "text": "Booleans\nBools are often an intermediate - they are an output of evaluations like 1 == 2. Booleans may sound very basic, but they are crucial in understanding control flow, which we’ll be covering in a future session!\n\nz = True            # you'll rarely ever assign a boolean directly like this, but do note they are\n                    # case sensitive; z = true wouldn't have worked here.\nprint(type(z))      # output: &lt;class 'bool'&gt;\nprint (10&gt;9)        # output: True\nprint (1 == 2)      # output: False\n \nprint(bool(123))    # output: True\nprint(bool(\"abc\"))  # output: True\nprint(bool(None))   # output: False\nprint(bool(0))      # output: False",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#numeric-types",
    "href": "sessions/06-data-types/index.html#numeric-types",
    "title": "An Introduction to Data Types",
    "section": "Numeric types",
    "text": "Numeric types\nPython supports different kinds of numbers, including integers (int), floating point numbers (float). You can do basic arithmetic (+, -, *, /), exponentiation (**), and use built-in functions like round(), abs(), and pow().\n\na = 10              # int\nb = 3               # int\nc = 2.5             # float\nd = -2              # int\n\nprint(a+b)          # output: 13, an int\nprint(a+c)          # output: 12.5, a float\nprint(a ** (1/2))   # taking the square root of an int returns a float\n \nprint(float(a))     # output: 10.0\nprint(int(2.88))    # output: 2; just throws away the decimal part\n \nprint(round(2.88))  # output: 3\nprint(round(2.88,1))# output: 2.9",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#strings",
    "href": "sessions/06-data-types/index.html#strings",
    "title": "An Introduction to Data Types",
    "section": "Strings",
    "text": "Strings\nStrings are sequences of characters enclosed in quotes. They support indexing, slicing, and a range of methods like .lower(), .replace(), .split(), and .join().\n\nstr_a = \"Hello\"              # string\nstr_b = \"SCW!\"               # string\n\nstr_ab = str_a + \" \" + str_b # python repurposes the \"+\" to mean string concatenation as well as addition\nprint(str_ab)                # output: Hello SCW!\n \nprint(str_ab.find(\"SCW\"))    # output:6 (the location in the string of the substring \"SCW\". Starts from 0!)\n \nstr_repeated = str_ab * 3 \nprint(str_repeated)          # output: Hello SCW!Hello SCW!Hello SCW!\n \nprint(len(str_a))            # output: 5\nprint(str_a[0])              # output: H\nprint(str_a[0:3])            # output: Hel (give me 3 characters starting at 0)\nprint(str_a[3:])             # output: lo (give me everything starting at 3)\nprint(str_a[:5])             # output: Hello (give me the first 5 characters)",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#lists",
    "href": "sessions/06-data-types/index.html#lists",
    "title": "An Introduction to Data Types",
    "section": "Lists",
    "text": "Lists\nLists are ordered, mutable (changeable) collections. They can hold any type of data and support operations like appending (.append()), removing (.remove()), and slicing (our_list[1:4]).\n\nfruits = [\"banana\", \"lychee\", \"raspberry\", \"apple\"]\nprint(fruits[0])          # output: banana (string)\nprint(fruits[0:2])        # output: ['banana','lychee'] (list!)\nprint(fruits[-1])         # output: apple (string)\n \nfruits.append(\"orange\") \nprint(fruits)             # output: ['banana', 'lychee', 'raspberry', 'apple', 'orange']\n \nprint(\"orange\" in fruits) # output: True\nprint(\"tomato\" in fruits) # output: False\n \nfruits.sort() \nprint(fruits)             # output: ['apple', 'banana', 'lychee', 'orange', 'raspberry']\n\nLists can contain any combination of other data types.\n\nmixed_list = [\"blue\", \"green\", False, 2, 2.55]\nfor item in mixed_list: # we're using a loop here; don't worry if you don't recognise this syntax\n    print(type(item))   # output:&lt;class 'str'&gt; &lt;class 'str'&gt; &lt;class 'bool'&gt; &lt;class 'int'&gt; &lt;class 'float'&gt;",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#dicts",
    "href": "sessions/06-data-types/index.html#dicts",
    "title": "An Introduction to Data Types",
    "section": "Dicts",
    "text": "Dicts\nDictionaries store key-value pairs and are optimized for lookups. Keys must be unique and are immutable, but values are mutable. You can add, update, or delete items using dict[key] = value, dict.get(key), or del dict[key].\n\nSCW_basic_info={\n    \"org_code\"      : \"0DF\",\n    \"short_name\"    : \"SCW CSU\",\n    \"long_name\"     : \"NHS South, Central and West Commissioning Support Unit\",\n    \"year_opened\"   : 2014,\n    \"active\"        : True,\n    \"postcode\"      : \"SO50 5PB\"\n}\n\nprint(type(SCW_basic_info[\"active\"]))       # output: &lt;class 'bool'&gt;\nprint(type(SCW_basic_info[\"year_opened\"]))  # output: &lt;class 'int'&gt;\n \nprint(SCW_basic_info[\"org_code\"])           # output: \"0DF\"\nprint(len(SCW_basic_info))                  # output: 6\n \nSCW_basic_info[\"number_of_staff\"] = 1000    # we can easily add a new key and value at the same time\n \nprint(len(SCW_basic_info))                  # output: 7\n \nSCW_basic_info[\"number_of_staff\"] += 1      # we hired a new member of staff\nprint(SCW_basic_info[\"number_of_staff\"])    # output: 1001",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#what-is-a-notebook",
    "href": "sessions/02-jupyter_notebooks/slides.html#what-is-a-notebook",
    "title": "Jupyter Notebooks",
    "section": "What is a notebook",
    "text": "What is a notebook\n\nThe standard for programming in python is the .py file which can hold a block of code which can contain lines of code that allow you to export the results as visualisations or data files.\nJupyter Notebooks have been developed with the data science and analytical community.\nNotebooks are a collection interactive cells which a user can run as a collection or individually, based on the current state of program.\nCells can be denoted as Code, Markdown or Raw Depending on use case.\n\nCode cells use a process called a kernel to run programme elements in the user selected code base (e.g. Python or R).\nMarkdown cells allow the user to include formatted text and other elements (such as links and images).\nRaw cells have no processing attached and output as plain text."
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#a-brief-history-of-jupyter-notebooks",
    "href": "sessions/02-jupyter_notebooks/slides.html#a-brief-history-of-jupyter-notebooks",
    "title": "Jupyter Notebooks",
    "section": "A brief history of Jupyter notebooks",
    "text": "A brief history of Jupyter notebooks\n\nIn 2001, Fernando Perez started development of the iPython project as a way of incorporating prompts and access to previous output, as he continued development he amalgamated iPython with 2 other projects\nIn 2014, Project Jupyter was born out of the initial iPython project. The key aim was to make the project independent of a programming language and allow different code bases to use notebooks. The Name is a reference to the three initial languages: Julia, Python, and R.\nJupyter Notebooks and more recently Jupiter Labs are more than just the notebook, they are interactive development environments launched from the command line.\nJupyter notebooks are used by many online platforms and service providers including: Kaggle, Microsoft Fabric, and the NHS Federated Data Platform."
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#pros-and-cons-of-using-a-notebook",
    "href": "sessions/02-jupyter_notebooks/slides.html#pros-and-cons-of-using-a-notebook",
    "title": "Jupyter Notebooks",
    "section": "Pros and cons of using a notebook",
    "text": "Pros and cons of using a notebook\nOn the plus side…\n\nNotebooks are highly interactive and allow cells to be run in any order.\nYou can re-run each cell separately, so iterative testing is more granular.\nNotebooks can be used to provide a structured report for an end user regardless of coding knowledge.\n\nHaving said that…\n\nIf you are not careful you can save a notebook in a state that cannot run as intended if changes are not checked.\nIt can be harder to understand complex code interactions."
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#the-toolkit",
    "href": "sessions/02-jupyter_notebooks/slides.html#the-toolkit",
    "title": "Jupyter Notebooks",
    "section": "The Toolkit",
    "text": "The Toolkit\n\nYou will need the following pre-installed:\n\nLanguage: Python\nDependency management: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code (or your preferred IDE)\n\nYou can install all these tools by running the following in PowerShell:\n\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop"
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#walkthrough-and-demonstration",
    "href": "sessions/02-jupyter_notebooks/slides.html#walkthrough-and-demonstration",
    "title": "Jupyter Notebooks",
    "section": "Walkthrough and demonstration",
    "text": "Walkthrough and demonstration\nif reviewing these slides this section is only available in the recording, though the initial steps used should be available on the associated Code Club site page"
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#resources",
    "href": "sessions/02-jupyter_notebooks/slides.html#resources",
    "title": "Jupyter Notebooks",
    "section": "Resources",
    "text": "Resources\n\nCheck out the History of iPython\nYou can find out more about Project Jupyter\nThe demonstration makes use of this markdown cheatsheet\nLikewise this is the Jupyter shortcuts Cheat Sheet"
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html",
    "href": "sessions/05-eda-seaborn/index.html",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "",
    "text": "This session will build on the previous session that introduced the Seaborn library, using it to visualise data and do some exploratory analysis.\nWe are using Australian weather data, taken from Kaggle. This dataset is used to build machine learning models that predict whether it will rain tomorrow, using data about the weather every day from 2007 to 2017. To download the data, click here.\nThe objective from this session is to:\n# import packages\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# control some deprecation warnings in seaborn\nwarnings.filterwarnings(\n    \"ignore\",\n    category=FutureWarning,\n    module=\"seaborn\"\n)\n\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')\nFirst, we will take a subset of the data, using Australia’s five biggest cities. This gives us a more manageable dataset to work with.\n# subset of observations from five biggest cities\nbig_cities = (\n    df.loc[df['Location'].isin(['Adelaide', 'Brisbane', 'Melbourne', 'Perth', 'Sydney'])]\n    .copy()\n)",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html#exploratory-data-analysis",
    "href": "sessions/05-eda-seaborn/index.html#exploratory-data-analysis",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nWhat does exploratory data analysis aim to achieve? What are you looking for when visualising data? Patterns, shapes, signals!\nWhen we describe a variable, or a sample of that variable, we are interested in understanding the characteristics of the observations. The starting point for doing this is describing the value that the data tends to take (central tendency), and how much it tends to deviate from its typical value (spread). Visualising the distribution of a variable can tell us these things (approximately), and can tell us about the shape of the data too.\nThe “central tendency” is the average or most common value that a variable takes. Mean, median, and mode are all descriptions of the central tendency.\n\nMean - Sum of values in a sample divided by the total number of observations.\nMedian - The midpoint value if the sample is ordered from highest to lowest.\nMode - The most common value in the sample1.\n\nThe mean is the most common approach, but the mean, median, and mode choice are context-dependent. Other approaches exist, too, such as the geometric mean2.\n\n# mode rainfall by location\nbig_cities.groupby('Location')['Rainfall'].agg(pd.Series.mode)\n\nLocation\nAdelaide     0.0\nBrisbane     0.0\nMelbourne    0.0\nPerth        0.0\nSydney       0.0\nName: Rainfall, dtype: float64\n\n\n\n# mode location\nbig_cities['Location'].agg(pd.Series.mode)\n\n0    Sydney\nName: Location, dtype: object\n\n\n\n# mode location using value counts\nbig_cities['Location'].value_counts().iloc[0:1]\n\nLocation\nSydney    3344\nName: count, dtype: int64\n\n\n\n# mean rainfall by location\nnp.round(big_cities.groupby('Location')['Rainfall'].mean(), decimals=2)\n\nLocation\nAdelaide     1.57\nBrisbane     3.14\nMelbourne    1.87\nPerth        1.91\nSydney       3.32\nName: Rainfall, dtype: float64\n\n\n\n# median rainfall by location\nbig_cities.groupby('Location')['Rainfall'].median()\n\nLocation\nAdelaide     0.0\nBrisbane     0.0\nMelbourne    0.0\nPerth        0.0\nSydney       0.0\nName: Rainfall, dtype: float64\n\n\n\n# geometric mean max temperature by location\nbig_cities.groupby('Location')['MaxTemp'].apply(lambda x: np.exp(np.log(x).mean()))\n\nLocation\nAdelaide     21.888697\nBrisbane     26.152034\nMelbourne    19.972352\nPerth        24.320203\nSydney       22.570993\nName: MaxTemp, dtype: float64\n\n\nThe values across the different measures of central tendency are not always the same. In this case, the mean and median differs massively.\nQuestions:\n\nWhy is that? Why would the median rainfall be zero for all five cities?\nDoes this matter? How would it change our understanding of the rainfall variable?\n\nDistributions can tell us more. We have simulated three different distributions that have slightly different shapes, to see how their mean and median values differ.\n\n\nPlot Code (Click to Expand)\n# generate distributions\nnp.random.seed(123)\nnormal_dist = np.random.normal(10, 1, 1000)\nright_skewed_dist = np.concatenate([np.random.normal(8, 2, 600), np.random.normal(14, 4, 400)])\nleft_skewed_dist = np.concatenate([np.random.normal(14, 2, 600), np.random.normal(8, 4, 400)])\n\n# set figure size\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# function for calculating summary statistics and plotting distributions\ndef plot_averages(ax, data, title):\n    mean = np.mean(data)\n    median = np.median(data)\n    \n    sns.histplot(data, color=\"#d9dcd6\", bins=30, ax=ax)\n    ax.axvline(mean, color=\"#0081a7\", linewidth=3, linestyle=\"--\", label=f\"Mean: {mean:.2f}\")\n    ax.axvline(median, color=\"#ef233c\", linewidth=3, linestyle=\"--\", label=f\"Median: {median:.2f}\")\n    ax.set_title(title)\n    ax.set_ylabel('')\n    ax.legend()\n\n# plot distributions\nfig, axes = plt.subplots(1, 3, sharey=True)\n\nplot_averages(axes[0], normal_dist, \"Normal Distribution\\n(Mean ≈ Median)\")\nplot_averages(axes[1], right_skewed_dist, \"Right-Skewed Distribution\\n(Mean &gt; Median)\")\nplot_averages(axes[2], left_skewed_dist, \"Left-Skewed Distribution\\n(Mean &lt; Median)\")\n\nplt.suptitle(\"Comparison of Mean & Median Across Distributions\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe mean and median of the normal distribution are identical, while the two skewed distributions have slightly different means and medians.\n\nThe mean is larger than the median when the distribution is right-skewed, and the median is larger than the mean when it is left-skewed.\n\nWhen the distribution is skewed, the median value will be a better description of the central tendency, because the mean value is more sensitive to extreme values (and skewed distributions have longer tails of extreme values).\n\n\nThese differences point to another important factor to consider when summarising data - the spread or deviation of the sample.\n\nHow do we measure how a sample is spread around the central tendency?\n\nStandard deviation and variance quantify spread.\nVariance, the average squared difference between observations and the mean value, measures how spread out a sample is.\nStandard deviation is the square root of the variance. It’s easier to interpret because it’s in the same units as the sample.\n\n\n\n\nPlot Code (Click to Expand)\n# generate distributions\nnp.random.seed(123)\nmean = 10\nstd_devs = [1, 2, 3]\ndistributions = [np.random.normal(mean, std_dev, 1000) for std_dev in std_devs]\n\n# function for calculating summary statistics and plotting distributions\ndef plot_spread(ax, data, std_dev, title):\n    mean = np.mean(data)\n    std_dev = np.std(data)\n\n    sns.histplot(data, color=\"#d9dcd6\", bins=30, ax=ax)\n    ax.axvline(mean, color=\"#0081a7\", linewidth=3, linestyle=\"--\", label=f\"Mean: {mean:.2f}\")\n    ax.axvline(mean + std_dev, color=\"#ee9b00\", linewidth=3, linestyle=\"--\", label=f\"Mean + 1 SD: {mean + std_dev:.2f}\")\n    ax.axvline(mean - std_dev, color=\"#ee9b00\", linewidth=3, linestyle=\"--\", label=f\"Mean - 1 SD: {mean - std_dev:.2f}\")\n    ax.set_title(f\"{title}\")\n    ax.legend()\n\n# plot distributions\nfig, axes = plt.subplots(1, 3, sharey=True, sharex=True)\n\nfor i, std_dev in enumerate(std_devs):\n    plot_spread(axes[i], distributions[i], std_dev, f\"Standard Deviation = {std_dev}\")\n\nplt.suptitle(\"Effect of Standard Deviation on Distribution Shape\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs standard deviation increases, the spread of values around the mean increases.\nWe can compute various summary statistics that describe a sample (mean, median, standard deviation, kurtosis etc. etc.), or we can just visualise it!\nVisualising distributions is a good starting point for understanding a sample. It can quickly and easily tell you a lot about the data.",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html#exploring-australian-weather",
    "href": "sessions/05-eda-seaborn/index.html#exploring-australian-weather",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "Exploring Australian Weather",
    "text": "Exploring Australian Weather\n\nVisualising Single Variables\nWe can start by visualising the distribution of rainfall and sunshine in Australia’s big cities, including dashed lines to show the mean and median values.\n\n# plot distribution of rainfall\nrainfall_mean = np.mean(big_cities['Rainfall'])\nrainfall_median = np.median(big_cities['Rainfall'].dropna())\n\nsns.histplot(data=big_cities, x='Rainfall', binwidth=10, color=\"#d9dcd6\")\nplt.axvline(rainfall_mean, color=\"#0081a7\", linestyle=\"--\", linewidth=2, label=f\"Mean: {rainfall_mean:.2f}\")\nplt.axvline(rainfall_median, color=\"#ef233c\", linestyle=\"--\", linewidth=2, label=f\"Median: {rainfall_median:.2f}\")\n\nplt.title(\"Distribution of Rainfall in Australia's Big Cities\")\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# plot distribution of sunshine\nsunshine_mean = np.mean(big_cities['Sunshine'])\nsunshine_median = np.median(big_cities['Sunshine'].dropna())\n\nsns.histplot(data=big_cities, x='Sunshine', binwidth=1, color=\"#d9dcd6\")\nplt.axvline(sunshine_mean, color=\"#0081a7\", linestyle=\"--\", linewidth=2, label=f\"Mean: {sunshine_mean:.2f}\")\nplt.axvline(sunshine_median, color=\"#ef233c\", linestyle=\"--\", linewidth=2, label=f\"Median: {sunshine_median:.2f}\")\n\nplt.title(\"Distribution of Sunshine in Australia's Big Cities\")\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nRainfall is very skewed, because the vast majority of days have zero rainfall. The distribution of sunshine is a little more evenly spread.\nWhile these two plots require a little more code, we can get most of what we want with a lot less just using sns.histplot() on its own. For example, plotting the distribution of maximum temperature, without all the other bells and whistles, already tells us a lot.\n\nsns.histplot(data=big_cities, x='MaxTemp')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can also look at the distribution of observations split by group, using sns.countplot(). Below, we see the number of observations per city in our subset.\n\nsns.countplot(big_cities, x='Location', color=\"#d9dcd6\", edgecolor='black')\nplt.ylim(3000, 3500)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSimilarly, we can look at the number of days with or without rain the next day.\n\nsns.countplot(big_cities, x='RainTomorrow', color=\"#d9dcd6\", edgecolor='black')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nHowever, it’s worth noting there is a simpler approach to this, just using pd.DataFrame.value_counts().\n\nbig_cities['RainTomorrow'].value_counts()\n\nRainTomorrow\nNo     11673\nYes     3543\nName: count, dtype: int64\n\n\n\n\nVisualising Multiple Variables\nWe will often want to know how values of a given variable change based on the values of another. This may not indicate a relationship, but it helps us better understand our data. There are lots of ways we can do this.\nWe can use sns.barplot() to plot the average hours of sunshine by location.\n\nsns.barplot(big_cities, x='Location', y='Sunshine', color=\"#d9dcd6\", edgecolor='black')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nOr we could use sns.boxplot(), visualising the distribution of maximum temperatures and humidity at 3pm by location.\n\nsns.boxplot(big_cities, x='Location', y='MaxTemp', color=\"#d9dcd6\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.boxplot(data=big_cities, x='RainTomorrow', y='Humidity3pm', color=\"#d9dcd6\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAnother way we can compare the distribution of values by groups is using sns.kdeplot(), which visualises a kernel-density estimation.\n\nsns.kdeplot(data=big_cities, x='Humidity3pm', hue='RainTomorrow')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhile there are lots of different ways we can quickly and easily compare two variables, or compare the value of a variable by groups, the right approach will always be context-dependent. It will all depend on what questions you have about your data, and which variables you are interested in.\nIf you have lots of questions about multiple variables, one way of exploring quickly is sns.pairplot().\n\nbiggest_cities = big_cities.loc[big_cities[\"Location\"].isin(['Sydney', 'Melbourne'])]\nsns.pairplot(\n    biggest_cities,\n    vars=['MinTemp', 'Sunshine', 'Rainfall'],\n    hue='Location'\n    )\n\nplt.show()\n\n\n\n\n\n\n\n\nI tend to struggle to infer much from complex plots like this, so I prefer to create separate plots using sns.scatterplot().\n\nsns.scatterplot(big_cities, x='Sunshine', y='MaxTemp', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nScatterplots can help you visualise how two continuous variables vary together. The above plot shows that sunshine hours are positively associated with maximum temperature, but there is significant noise.\nIf we compare this with a scatterplot visualising the association between two variables that should have a strong relationship, such as humidity at 9am and 3pm, we can see the difference.\n\nsns.scatterplot(big_cities, x='Humidity9am', y='Humidity3pm', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThere is still plenty of noise, but as humidity at 9am increases, it is clear that humidity at 3pm is likely to increase.\nAnother change we might make, to reduce the noise, is adding grouping structures to our scatterplot. Perhaps much of the noise in the sunshine scatterplot is because we are looking at data across many cities.\n\nsns.scatterplot(biggest_cities, x='Sunshine', y='MaxTemp', hue='Location', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhen we compare just two cities, there still appears to be significant noise.\nSometimes you might need to do some more complex operations to transform the data before visualising it, in order to ask more specific questions. For example, you might want to compare how the total rainfall per day has varied over time in the data.\n\n(\n    big_cities\n    # convert date to datetime\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    # create year-month column\n    .assign(Year_Month=lambda x: x['Date'].dt.to_period('M'))\n    # group by year-month and calculate sum of rainfall\n    .groupby('Year_Month')['Rainfall'].sum()\n    # convert year-month index back to column in dataframe\n    .reset_index()\n    # create year-month timestamp for plotting\n    .assign(Year_Month=lambda x: x['Year_Month'].dt.to_timestamp()) \n    # pass df object to seaborn lineplot\n    .pipe(lambda df: sns.lineplot(data=df, x='Year_Month', y='Rainfall', linewidth=2))\n)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe above plot leverages an approach called method chaining, where we call multiple methods one after the other in the same operation3. Method chaining syntax is sometimes a little easier to follow, and you don’t have to create new objects for every operation, which can be a tidier way to work.\nWe can do the same to transform the data and visualise the mean average sunshine per month.\n\n(\n    big_cities\n    # convert date to datetime object\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    # set date column as index\n    .set_index('Date')\n    # resample by month-end for monthly aggregations\n    .resample('ME')\n    # calculate mean sunshine per month\n    .agg({'Sunshine': 'mean'})\n    # convert month index back to column in dataframe\n    .reset_index()\n    # pass df object to seaborn lineplot\n    .pipe(lambda df: sns.lineplot(data=df, x='Date', y='Sunshine', color=\"#1f77b4\", linewidth=2))\n)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFinally, we could combine two plots to look at how the average rainfall and average sunshine both vary by month.\n\nfig, axes = plt.subplots(1, 2)\n\n(\n    big_cities\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    .assign(Month=lambda x: x['Date'].dt.month)\n    .groupby('Month')['Rainfall'].mean()\n    .reset_index()\n    .pipe(lambda df: sns.lineplot(data=df, x='Month', y='Rainfall', color=\"#1f77b4\", linewidth=2, ax=axes[0]))\n)\n\n(\n    big_cities\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    .assign(Month=lambda x: x['Date'].dt.month) \n    .groupby('Month')['Sunshine'].mean() \n    .reset_index()\n    .pipe(lambda df: sns.lineplot(data=df, x='Month', y='Sunshine', color=\"#ff7f0e\", linewidth=2, ax=axes[1]))\n)\n\nxticks = range(1, 13)\nxticklabels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nfor ax in axes:\n    ax.set_xticks(xticks)  # Set ticks\n    ax.set_xticklabels(xticklabels, rotation=45)\n    ax.set_xlabel('')\n    ax.set_ylabel('')\naxes[0].set_title('Average Rainfall by Month', fontsize=16)\naxes[1].set_title('Average Sunshine by Month', fontsize=16)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html#exercises",
    "href": "sessions/05-eda-seaborn/index.html#exercises",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "Exercises",
    "text": "Exercises\nSome of these questions are easily answered by scrolling up and finding the answer in the output of the above code, however, the goal is to find the answer using code. No one actually cares what the answer to any of these questions is, it’s the process that matters!\nRemember, if you don’t know the answer, it’s okay to Google it (or speak to others, including me, for help)!\n\n\nImport Data (to Reset)\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')\n\n\n\nWhat does the distribution of minimum daily temperatures look like in these cities? Are there any unusual patterns?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.histplot(big_cities[\"MinTemp\"].dropna(), kde=True)\nplt.title(\"Distribution of MinTemp\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nDoes the amount of sunshine vary depending on whether it rains the next day? Visualise this.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.boxplot(data=big_cities, x=\"RainTomorrow\", y=\"Sunshine\")\nplt.title(\"Sunshine by Rain Tomorrow\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHow closely related are atmospheric pressure readings in the morning compared to the afternoon?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.scatterplot(data=big_cities, x=\"Pressure9am\", y=\"Pressure3pm\")\nplt.title(\"Pressure at 9am vs 3pm\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHow does humidity in the afternoon vary across the five cities? What can you infer from this?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.violinplot(data=big_cities, x=\"Location\", y=\"Humidity3pm\")\nplt.title(\"Humidity at 3pm by City\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nAre days when rain is expected tomorrow more or less common in this dataset? Show the distribution.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.countplot(data=big_cities, x=\"RainTomorrow\")\nplt.title(\"Rain Tomorrow Counts\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nIs there any relationship between afternoon temperature and humidity? Does this relationship change depending on whether it rains the next day?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.scatterplot(data=big_cities, x=\"Temp3pm\", y=\"Humidity3pm\", hue=\"RainTomorrow\")\nplt.title(\"Temp vs Humidity at 3pm by Rain Tomorrow\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHow strongly are the different continuous variables in this dataset correlated with each other? Create a correlation matrix.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ncorr = big_cities.select_dtypes(include=\"number\").corr().round(2)\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, cmap=\"coolwarm\", center=0)\nplt.title(\"Correlation Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nExplore the relationships between rainfall, sunshine, and afternoon humidity across the cities. What patterns stand out?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.pairplot(big_cities, vars=[\"Rainfall\", \"Sunshine\", \"Humidity3pm\"], hue=\"Location\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHow has the maximum temperature in Brisbane changed over time? Create a time series visualisation.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbrisbane = big_cities.loc[big_cities[\"Location\"] == \"Brisbane\"]\nbrisbane['Date'] = pd.to_datetime(brisbane['Date'])\nbrisbane_daily = brisbane.groupby(\"Date\")[\"MaxTemp\"].mean().reset_index()\n\nplt.figure(figsize=(12, 4))\nsns.lineplot(data=brisbane_daily, x=\"Date\", y=\"MaxTemp\")\nplt.title(\"Daily Max Temp in Brisbane\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does the distribution of daily rainfall amounts look like? Is it skewed or symmetric?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.kdeplot(data=big_cities, x=\"Rainfall\", fill=True)\nplt.title(\"Rainfall Distribution\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHow does the average morning wind speed compare across the five cities?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.barplot(data=big_cities, x=\"Location\", y=\"WindSpeed9am\", ci=None)\nplt.title(\"Average Morning Wind Speed by City\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nIn Perth, is there any visible relationship between the amount of sunshine and rainfall?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nperth = big_cities.loc[big_cities[\"Location\"] == \"Perth\"]\nsns.scatterplot(data=perth, x=\"Sunshine\", y=\"Rainfall\")\nplt.title(\"Sunshine vs Rainfall in Perth\")\nplt.show()",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html#footnotes",
    "href": "sessions/05-eda-seaborn/index.html#footnotes",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe mode value is generally most useful when dealing with categorical variables.↩︎\nThe geometric mean multiplies all values in the sample and takes the \\(n\\)th root of that multiplied value. It can be useful when dealing with skewed data or data with very large ranges, and when dealing with rates, proportions etc. However it can’t handle zeros or negative values.↩︎\nThis may not be something you feel comfortable with yet, but it is something you may come across, and could explore in the future.↩︎",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html",
    "href": "sessions/01-onboarding/index.html",
    "title": "Python Onboarding",
    "section": "",
    "text": "This is the first session of Code Club’s relaunch. It focuses on giving users all the tools they need to get started using Python and demonstrates the setup for a typical Python project.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#session-slides",
    "href": "sessions/01-onboarding/index.html#session-slides",
    "title": "Python Onboarding",
    "section": "Session Slides",
    "text": "Session Slides\nUse the left ⬅️ and right ➡️ arrow keys to navigate through the slides below. To view in a separate tab/window, follow this link.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#the-tools-you-will-need",
    "href": "sessions/01-onboarding/index.html#the-tools-you-will-need",
    "title": "Python Onboarding",
    "section": "The Tools You Will Need",
    "text": "The Tools You Will Need\nWhile this course focuses on Python, we will use several other tools throughout.\n\nLanguage: Python\nDependency Management & Virtual Environments: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code/Jupyter Notebooks (or your preferred IDE)\n\nYou can install all the tools you’ll need by running the following one-liner run in PowerShell:\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop\n\nPython\nPython is an all-purpose programming language that is one of, if not the most popular, in the world1 and is widely used in almost every industry. Its popularity is owed to its flexibility as a language that can be used to achieve nearly any job. It is often referred to as the second-best tool for every job. Specialist languages might be better for specific tasks (for example, R for statistics), but Python is good at everything.\nIt is a strong choice for data science and analytics, being one of the best languages for data wrangling, data visualisation, statistics, and machine learning. It is also well-suited to web development, scientific computing, and automation.\n\n\nDependency Management\nOne of Python’s greatest weaknesses is dependency management. Despite its many strengths, there is no escaping the dependency hell that every Python user faces.\nDependency management refers to the process of tracking and managing all of the packages (dependencies) a project needs to run. It is a consideration in any programming language. It ensures:\n\nThe right packages are installed.\nThe correct versions are used.\nConflicts between packages are avoided.\n\nThere are many reasons that Python handles dependency management so poorly, but there are some tools that make this a little easier on users. We are using uv for dependency management. It is relatively new, but it is quickly becoming the consensus tool for dependency management in Python because it makes the process about as painless as it can be without moving to a different language entirely.\n\nVirtual Environments\nVirtual environments are a component of dependency management. Dependency management becomes much messier when you have many Python projects, each using their own packages, some overlapping and some requiring specific versions, either for compatibility or functionality reasons. Reducing some of this friction by isolating each project in its own virtual environment, like each project is walled off from all other projects, makes dependency management a little easier. Virtual environments allow you to manage dependencies for a specific project without the state of those dependencies affecting other projects or your wider system.\nVirtual environments help by:\n\nKeeping dependencies separate for each project.\nAvoiding version conflicts between projects.\nMaking dependency management more predictable and reproducible.\n\nWe will use uv to manage all dependencies, virtual environments, and even versions of Python.\n\n\n\nVersion Control\nVersion control is the practice of tracking and managing changes to code or files over time, allowing you to:\n\nRevert to earlier versions if needed.\nCollaborate with others on the same project easily.\nMaintain a history of changes.\n\nWe are using Git, a version control system, to host our work and GitHub Desktop to manage version control locally.\nVersion control and Git are topics entirely in their own right, and covering them in detail is out of the scope of this session. We hope to cover version control in a future session, but right now, you just need to be able to access materials for these sessions. You can find the materials in the Code Club repository.\nIf you have downloaded GitHub Desktop, the easiest way to access these materials and keep up-to-date is by cloning the Code Club repository (go to File, then Clone Repository, select URL, and paste the Code Club repository link in the URL field). You can then ensure that the materials you are using are the most current by clicking the Fetch Origin button in GitHub Desktop, which grabs the changes we’ve made from the central repository on GitHub.\n\n\nIDE\nIDEs (Integrated Development Environments) are software that simplifies programming and development by combining many of the most common tasks and helpful features for programming into a single tool. These typically include a code editor, debugging functionality, build tools, and features like syntax highlighting and code completion. When you start your code journey, you might not need all these tools, and fully-featured IDEs can be overwhelming. But as you become more comfortable with programming, all these different features will become very valuable.\nSome common IDEs that are used for Python include:\n\nVS Code\nPyCharm\nVim\nJupyter Notebooks/JupyterLab\nPositron\n\nWe will use VS Code or Jupyter Notebooks (which is not exactly an IDE but is similar).",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#project-setup",
    "href": "sessions/01-onboarding/index.html#project-setup",
    "title": "Python Onboarding",
    "section": "Project Setup",
    "text": "Project Setup\nEvery new Python project should start with using uv to set up a virtual environment for the project to keep everything organised and reduce the risk of finding yourself in dependency hell.\nThe entire project setup process can be handled in the command line. We will use PowerShell for consistency.\nWhen you open a PowerShell window, it should open in your C drive (e.g.,  C:\\Users\\user.name). If it does not, run cd ~, and it should return to your home directory.\nWe will create a new uv project in the home directory2 using the command uv init. The new project will contain everything we need, including a Python installation, a virtual environment, and the necessary project files for tracking and managing any packages installed in the virtual environment. To set up a new project called test-project, use the following command:\nuv init test-project\nHaving created this new directory, navigate to it using cd test-project. You can check the files in a directory using the command ls. If you run this command, you will see three files in the project directory (hello.py, pyproject.toml, and README.md). The project doesn’t yet have a Python installation or a virtual environment, but this will be added when we add external Python packages.\nYou can install Python packages using the command uv add. We can add some common Python packages that we will use in most projects (pandas, numpy, seaborn, and ipykernel3) using the following command:\nuv add pandas numpy seaborn ipykernel\nThe output from this command will reference the Python installation used and the creation of a virtual environment directory .venv. Now, if you run ls, you will see two new items in the directory, uv.lock and .venv.\nYour Python project is now set up, and you are ready to start writing some code. You can open VS Code from your PowerShell window by running code ..\nFor more information about creating and managing projects using uv, check out the uv documentation.\n\nOpening your project in VS Code\nTo open your newly-created uv project in VS Code, launch the application and click File &gt; Open Folder.... You’ll want to make sure you select the root level of your project. Once you’ve opened the folder, the file navigation pane in VS Code should display the files that uv has created, including a main.py example file. Click on this to open it.\nOnce VS Code realises you’ve opened a folder with Python code and a virtual environment, it should do the following:\n\nSuggest you install the Python extension (and, once you’ve created a Jupyter notebook, the Jupyter one) offered by Microsoft - go ahead and do this. If this doesn’t happen, you can install extensions manually from the Extensions pane on the left-hand side.\nSelect the uv-created .venv as the python Environment we’re going to use to actually run our code. If this doesn’t happen, press ctrl-shift-P, type “python environment” to find the Python - Create Environment... option, hit enter, choose “Venv” and proceed to “Use Existing”.\n\nIf all has gone well, you should be able to hit the “play” icon in the top right to execute main.py. The Terminal pane should open up below and display something like Hello from (your-project-name)!.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#footnotes",
    "href": "sessions/01-onboarding/index.html#footnotes",
    "title": "Python Onboarding",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough there are several ways to measure language popularity, the PYPL Index, HackerRank’s Developer Skills Report, and IEEE Spectrum all rank Python as the most popular language in the world, while Stack Overflow’s Developer Survey places Python third behind JavaScript and HTML/CSS.↩︎\nWe recommend using the C drive for all Python projects, especially if using version control. Storing projects like these on One Drive will create many unnecessary issues.↩︎\nStrictly speaking, we should install ipykernel as a development dependency (a dependency that is needed for any development but not when the project is put into production). In this case, we would add it by running uv add --dev ipykernel. However, in this case, it is simpler to just add it as a regular dependency, and it doesn’t harm.↩︎",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/schedule.html",
    "href": "sessions/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This is the schedule for Code Club in FY25/26.\nThe Demonstration, Presentation, and Notebook columns indicate the content to be expected for each session:\n\nDemonstration: A live show-and-tell relating to the discussion topic.\nPresentation: A slide deck covering the discussion topic.\nNotebook: A Jupyter Notebook containing code-along elements or examples for people to work through after the session.\n\nTutorials will be divided into Modules. We recommend that people attend or watch tutorials in the Core module in order to gain a fundamental understanding of coding concepts and resources. Further modules are to be confirmed, but will likely include Automation, Dashboards and Visualisation, and Data Science. People will be able to attend those modules that interest them.\nWe have mapped the contents of the syllabus to competencies in the National Competency Framework for Data Professionals in Health and Care so that you can see which sessions will help you on your way towards them. For full details of the skills in the framework, the self-assessment tool can be found on FutureNHS.\nPlease note that this is a first draft of the mapping of NCF competencies to our syllabus and it is awaiting review.\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  Session Date\n  Module\n  Session Name\n  Description\n  Demonstration\n  Presentation\n  Notebook\n  NCF Competency\n\n\n\n  \n    22/04/2025\n    N/A\n    Nectar Re-Launch\n    Showcasing the re-launch of Code Club\n    🎬\n    💻\n    -\n    -\n  \n  \n    01/05/2025\n    On-boarding\n     Python On-boarding\n    What to install, basic virtual environment management, introduction to VS Code\n    🎬\n    -\n    -\n    SA21 : Python Proficiency\n  \n  \n    15/05/2025\n    On-boarding\n    Jupyter Notebooks\n    Demonstration of Jupyter Notebooks\n    🎬\n    -\n    📖\n    SA21 : Python Proficiency\n  \n  \n    29/05/2025\n    Exploration & Visualisation\n    EDA with Pandas\n    Introduction to exploratory data analysis using pandas\n    🎬\n    -\n    📖\n    SA4 : Descriptive and Explicative Analytics\n  \n  \n    12/06/2025\n    Exploration & Visualisation\n    Visualisation with Seaborn\n    Aesthetically-pleasing visualisations\n    🎬\n    -\n    📖\n    SA1 : Data Visualisation\n  \n  \n    03/07/2025\n    Exploration & Visualisation\n    EDA with Seaborn\n    Using seaborn to visualise and explore data\n    🎬\n    -\n    📖\n    SA4 : Descriptive and Explicative Analytics\n  \n  \n    17/07/2025\n    Core concepts\n    Data Types\n    Introduction to data types\n    -\n    💻\n    📖\n    SA21 : Python Proficiency\n  \n  \n    31/07/2025\n    Core concepts\n    Control Flow\n    Introduction to control flow\n    -\n    💻\n    📖\n    SA21 : Python Proficiency\n  \n  \n    14/08/2025\n    Core concepts\n    Functions & Functional Programming\n    Introduction to functions and functional programming\n    -\n    💻\n    📖\n    SA21 : Python Proficiency\n  \n  \n    28/08/2025\n    Core concepts\n    Object-Oriented Programming\n    Introduction to object-oriented programming\n    -\n    💻\n    📖\n    SA21 : Python Proficiency\n  \n  \n    11/09/2025\n    Exploration & Visualisation\n    Streamlit dashboards\n    How to present data (visualisations) in a Streamlit dashboard\n    🎬\n    -\n    -\n    SA1 : Data Visualisation\n  \n  \n    25/09/2025\n    Data Science\n    Comparing Samples\n    Understanding data distributions and comparisons between samples\n    -\n    💻\n    📖\n    SA4 : Descriptive and Explicative Analytics\n  \n  \n    09/10/2025\n    Data Science\n    Analysing Relationships\n    Quantifying relationships with hypothesis tests and statistical significance\n    -\n    💻\n    📖\n    SA15 : Hypothesis Testing\n  \n  \n    23/10/2025\n    Data Science\n    Introduction to Linear Regression\n    Introduction to regression concepts and the component parts of linear regression\n    -\n    💻\n    📖\n    SA7 : Advanced Statistics\n  \n  \n    06/11/2025\n    Data Science\n    Implementing Linear Regression\n    Building linear models, assessing model fit, and interpreting coefficients\n    🎬\n    💻\n    📖\n    SA7 : Advanced Statistics\n  \n  \n    20/11/2025\n    Data Science\n    Beyond Linearity\n    Introduction to generalised linear regression models\n    🎬\n    💻\n    📖\n    SA7 : Advanced Statistics",
    "crumbs": [
      "Sessions",
      "Session Schedule"
    ]
  },
  {
    "objectID": "resources/glossary.html",
    "href": "resources/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "This page will have a list of definitions for commonly used (and/or commonly misunderstood) terminology and acronyms relating to python or data analysis and manipulation in general.\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n  Term\n  Definition\n\n\n\n  \n    Exploratory Data Analysis (EDA)\n    The process of analysing datasets to summarise their main characteristics, often using visual methods, before formal modeling.\n  \n  \n    Functional programming\n    A programming paradigm that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data.\n  \n  \n    Git\n    A distributed version control system used to track changes in source code during software development.\n  \n  \n    GitHub\n    A cloud-based platform that provides hosting for repositories of files and folders (usually of software code) using git as its backend for version control.\n  \n  \n    Jupyter\n    A software package that allows the creation of python notebooks that can include a mixture of markdown-formatted text and live Python code.\n  \n  \n    Markdown (.md)\n    A simple plain-text markup scheme designed to allow for rapid production of formatted text (with headings, links, etc) within a plaintext file. Used by Quarto (as a Quarto-specific flavour with the .qmd extension).\n  \n  \n    matplotlib\n    The baseline Python library for creating static, animated, and interactive visualisations, offering extensive customisation options for plots and charts. Also used as a framework for more advanced or visually pleasing visualisation packages like seaborn.\n  \n  \n    Object-oriented programming (OOP)\n    A programming paradigm based on the concept of \"objects,\" which can contain data and code to manipulate that data.\n  \n  \n    pandas\n    A python data analysis and manipulation library for working with dataframes (tabular data)\n  \n  \n    Python\n    A general-purpose programming language\n  \n  \n    Regression\n    A method for modeling the relationship between one or more explanatory variables and an outcome. It is used to predict outcomes and understand the impact of changes in predictors (explanatory variables) on the response (outcome).\n  \n  \n    Repository (repo)\n    In git and github, a repository is a self-contained \"project\" of files and folders.\n  \n  \n    Reproducible Analytical Pipelines (RAP)\n    A set of processes and tools designed to ensure that data analysis can be consistently repeated and verified by others.\n  \n  \n    seaborn (sns)\n    A Python data visualisation library based on Matplotlib, providing a high-level interface for drawing attractive and informative statistical graphics.\n  \n  \n    TOML\n    Tom's Obvious Minimal Language - simple, human-readable data serialisation format designed for configuration files, emphasizing readability and ease of use. Used by uv to specify its projects.\n  \n  \n    uv\n    A Python package manager, which can manage python projects (folders) and manage the installation and management of the python environment and libraries within that folder.\n  \n  \n    YAML\n    Yet Another Markup Language - a human-readable data serialisation format often used for configuration files and data exchange between languages."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Code Club!",
    "section": "",
    "text": "Code Club aims to support everyone at SCW in developing technical and analytical skills through interpretive dance code. We believe these skills are indispensable to the NHS today and in the future, enabling the delivery of high-quality insights through data science and advanced analytics, and the automation of day-to-day tasks with programming. We want to foster an environment that welcomes everybody, sparks ideas, and nurtures collaboration.\nThe Code Club syllabus has been designed to help people with little to no coding experience develop their skills in Python and extend their analytical skills through code. Sessions will be an hour long and held once per fortnight at 2:00 PM on Thursdays. To get an idea of what we will be covering and see if it is right for you, go to the Schedule page. We would love for you to join us!"
  },
  {
    "objectID": "sessions/04-seaborn-visualisation/index.html",
    "href": "sessions/04-seaborn-visualisation/index.html",
    "title": "Visualisation with Seaborn",
    "section": "",
    "text": "Python has a rich ecosystem of libraries for data visualisation, each with different strengths. Some popular options include matplotlib for fine control over plots, plotly for interactive visualisations, and bokeh for web-ready dashboards. In this session, we’ll be using seaborn. It’s built on top of matplotlib but offers a simpler, high-level interface and nice looking default styles — it’s therefore a good choice when you who want to quickly create clear and informative plots without needing to tweak every detail.\nWe are using Australian weather data, taken from Kaggle. This dataset is used to build machine learning models that predict whether it will rain tomorrow, using data about the weather every day from 2007 to 2017. To download the data, click here.\nOne final note before we get started - This page is a combination of text and python code. We’ve tried to explain clearly what we’re about to do before we do it, but do also note the # comments within the python code cells themselves that occasionally explain a specific line of code in more detail.",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "4. Visualisations with Seaborn"
    ]
  },
  {
    "objectID": "sessions/04-seaborn-visualisation/index.html#initial-setup",
    "href": "sessions/04-seaborn-visualisation/index.html#initial-setup",
    "title": "Visualisation with Seaborn",
    "section": "Initial setup",
    "text": "Initial setup\nWe’re going to import some python packages. Remember that the plt, np, sns aliases are just for convenience - we could omit this completely or use different aliases if we prefer.\n\n\n\n\n\n\nAside - why sns?\n\n\n\nSeaborn being imported as sns is an odd convention (they are the initials of the fictional character the package was named for) that will make it easier to read or copy/paste online examples.\n\n\n\n# install necessary packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# suppress some annoying warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning) \n\nsns.set_theme(style='darkgrid') # https://seaborn.pydata.org/generated/seaborn.set_theme.html\nsns.set_context(\"notebook\") # set an overall scale. Notebook is the default. In increasing size: paper, notebook, talk, poster.\nplt.rcParams['font.sans-serif'] = ['Calibri','Segoe UI','Arial'] # use a nicer font in matplotlib (if available)\n\nAs before, we need to import our dataset. We’re importing the csv file into an initial dataframe called df to start with.\n\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "4. Visualisations with Seaborn"
    ]
  },
  {
    "objectID": "sessions/04-seaborn-visualisation/index.html#data-manipulation",
    "href": "sessions/04-seaborn-visualisation/index.html#data-manipulation",
    "title": "Visualisation with Seaborn",
    "section": "Data manipulation",
    "text": "Data manipulation\n\nColumn conversions\nBefore we start actually generating some visuals, we need to make sure our Date column contains proper datetimes. We’re also going to drop the years with partial data so that our dataset only has full years. Finally we’re going to change the RainTomorrow field to contain a 0 or a 1 rather than yes/no.\n\n# convert date column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# create a column that just contains the year\ndf['Year'] = df['Date'].dt.year\n\n# drop the partial years (2007,2017)\ndf = df[~df['Year'].isin([2007,2017])]\n\n# convert 'RainTomorrow' to a numeric variable, where 'Yes' = 1 and 'No' = 0.\ndf['RainToday']=df['RainToday'].replace({'Yes': 1, 'No': 0, 'NA':0}).fillna(0).astype(int)\ndf['RainTomorrow']=df['RainTomorrow'].map({'Yes': 1, 'No': 0,'NA': 0}).fillna(0).astype(int); \n\n# little tip: the semicolon suppresses textual output when we don't want it\n\n\n\nSort order and other helper tables\nWe need a month order for our “mmm” months - there is probably an official way of doing this…\n\nmonth_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\nWe also need a sort order for our city names to use as a column order for some of our charts later. We’ll just arrange them alphabetically.\n\ncolOrder_top5Cities=['Adelaide','Brisbane','Melbourne','Perth','Sydney']\n\nTo enhance a chart we’re going to build later, we’re going to dynamically calculate some text describing our data range.\n\n# Calculate the date range dynamically; we're going to use this later...\ndate_min = df['Date'].min().strftime('%Y')\ndate_max = df['Date'].max().strftime('%Y')\ndate_range = f\"{date_min} - {date_max}\"\n\nprint(date_range)\n\n2008 - 2016\n\n\n\n\nPivoting and grouping\nNext, we’re going to create some helper dataframes by filtering, grouping and pivoting the data. These will be used for different types of visuals later. Of course, we could have just created these groupings and pivots inline when we do the actual visualisation, but we’re doing it this way because:\n\nIt’s easier to follow\nIt’s tidier (and probably faster) to create these dataframes once as we’re going to be using them multiple times.\n\n\n# build a month column\ndf['Month'] = df['Date'].dt.strftime('%b') # Add a column that just contains the month in mmm format\ndf['Month'] = pd.Categorical(df['Month'], categories=month_order, ordered=True) # Make it categorical using our custom order so that it appears in the right order\n\n# we're going to filter to top 5 cities from now on\ndf_top5Cities = df[df['Location'].isin(['Perth','Adelaide','Sydney','Melbourne','Brisbane'])]\n\n# a dataframe with the number of rainy days per year and month, and location\ndf_top5Cities_rainyDays = df_top5Cities.groupby(['Location','Year', 'Month'])['RainToday'].sum().reset_index()\n\n# finally, we're going to create some grouped and pivoted dataframes. Picture these as PivotTables in Excel.\ndf_top5Cities_Rainfall_grouped = df_top5Cities.groupby(['Location', 'Month'])['Rainfall'].mean().reset_index()\ndf_top5Cities_Rainfall_pivoted = df_top5Cities_Rainfall_grouped.pivot(index=\"Location\",columns=\"Month\", values=\"Rainfall\")\ndf_top5Cities_monthly_rainyDays_pivoted = df_top5Cities.groupby(['Location', 'Month','Year'])['RainToday'].sum().reset_index().groupby(['Location','Month'])['RainToday'].mean().reset_index().pivot(index=\"Location\",columns=\"Month\", values=\"RainToday\")\n\nLet’s use head() to make sure we understand what each grouped/pivoted DF is for.\n\ndf_top5Cities_Rainfall_grouped.head(2)\n\n\n\n\n\n\n\n\nLocation\nMonth\nRainfall\n\n\n\n\n0\nAdelaide\nJan\n0.672199\n\n\n1\nAdelaide\nFeb\n0.973604\n\n\n\n\n\n\n\n\ndf_top5Cities_Rainfall_pivoted.head(2)\n\n\n\n\n\n\n\nMonth\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\nLocation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelaide\n0.672199\n0.973604\n1.171667\n1.461165\n2.293860\n2.447009\n2.873606\n2.399237\n1.809125\n0.906273\n0.730827\n1.192500\n\n\nBrisbane\n6.415574\n5.325389\n4.442276\n3.165385\n3.126446\n2.516318\n1.000000\n1.273381\n1.314498\n2.419424\n3.347761\n4.551613\n\n\n\n\n\n\n\n\ndf_top5Cities_monthly_rainyDays_pivoted.head(2)\n\n\n\n\n\n\n\nMonth\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\nLocation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelaide\n2.555556\n1.555556\n3.666667\n4.000000\n7.666667\n8.555556\n13.444444\n11.444444\n7.888889\n4.222222\n4.000000\n4.555556\n\n\nBrisbane\n8.000000\n7.111111\n10.000000\n5.333333\n5.555556\n6.222222\n4.111111\n3.555556\n4.111111\n5.888889\n6.888889\n8.777778\n\n\n\n\n\n\n\n\n\n\n\n\n\nAside: why df[df[...?\n\n\n\ndf_top5Cities = df[df['Location'].isin(['Perth','Adelaide','Sydney','Melbourne','Brisbane'])]\n\nThe first (outer) df[ tells pandas that we want to select a subset of rows based on some condition.\nThe second (inner) df[ is going to tell pandas this condition. In this case, we’re using isin to return a dataframe that contains a series of True and False rows corresponding to whether the rows in our original dataframe had the Location column as one of our 5 cities.\nThe final dataframe is then a filtered copy where the inner condition is True.\n\nYes, there are other ways of doing this! For example by using .query() to specify our conditions.",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "4. Visualisations with Seaborn"
    ]
  },
  {
    "objectID": "sessions/04-seaborn-visualisation/index.html#doing-some-actual-plotting",
    "href": "sessions/04-seaborn-visualisation/index.html#doing-some-actual-plotting",
    "title": "Visualisation with Seaborn",
    "section": "Doing some actual plotting",
    "text": "Doing some actual plotting\nThe Seaborn home page has a very good introductory tutorial, reference documentation, and a nice collection of examples. You should familiarise yourself with the documentation; it’ll pay off massively if you actually grasp what each function and argument is for, rather than just copy/pasting examples and tweaking them until they work (without really understanding what they’re doing).\n\nA basic one-line line chart\n\nsns.lineplot(\n  data=df_top5Cities_Rainfall_pivoted.T #.T gives the transpose (flips rows and columns)\n  ) \n\n\n\n\n\n\n\n\n\n\nJust a little bit of customisation - a bar chart\nhttps://seaborn.pydata.org/generated/seaborn.barplot.html\n\nour_barplot = sns.barplot(\n  data=df_top5Cities_Rainfall_grouped \n  ,x=\"Month\"\n  ,y=\"Rainfall\"\n  ,hue=\"Location\" # read \"hue\" as \"series\"\n  ,palette=\"tab10\" # https://matplotlib.org/stable/users/explain/colors/colormaps.html\n  )\n\nour_barplot.set(title='Average daily rainfall by month and city',ylim=(0,8))\n\nsns.move_legend(our_barplot,\"upper left\", title=None, ncol=4) # https://seaborn.pydata.org/generated/seaborn.move_legend.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAside - why did we need to set the title after the sns.barplot call?\n\n\n\nThe barplot function provided by Seaborn doesn’t actually allow setting of a title - it just generates a plot (including its axes) and returns this as a matplotlib Axes object (recall we mentioned earlier that Seaborn is a layer on top of the matplotlib library). By using the .set(...) method on our barplot object, we can modify this returned object to give it a title. We also could have used this to customise our axis labels (the defaults are fine here), set axis limits, or things like tick labels.\n\n\n\n\nHeatmaps\nThe Seaborn heatmap function will easily let us create a two-dimensional heatmap visual with a specific colour theme and custom number formatting.\n\n# We need to use some matplotlib code to set our output size, add a title, and capitalise our x-axis label\nf,ax = plt.subplots(figsize=(10, 5)) # matplotlib subplots are a common way of setting a figure layout\nax.set_title(f\"Average daily rainfall (/mm) each month ({date_range}) for Australia's top 5 cities\", fontsize=16, fontweight=\"bold\", pad=10) # using our previously set date_range variable\n\nsns.heatmap(df_top5Cities_Rainfall_pivoted # Heatmap expects rectangular (pivot-like) data\n            ,annot=True # Put numbers inside the cells\n            ,fmt=\".1f\" # Make the numbers have 1 decimal place\n            ,square=True # Square vs rectangular cells\n            ,cbar=False # Get rid of the colourbar legend on the side\n            ,cmap=\"Blues\" # Seems appropriate for rainfall. Colourmaps reference: https://matplotlib.org/stable/users/explain/colors/colormaps.html \n            ,ax=ax # Tell it to use the matplotlib axes we created earlier\n           )\n\n\n\n\n\n\n\n\n\n\nAnother heatmap with some further tweaks\nWe can make our heatmap look just a little better by apply some tweaks to the subplots object.\n\n# Again setting up matplotlib subplots so that we can make some changes later\nf,ax = plt.subplots(figsize=(10, 5)) \n\nsns.heatmap(df_top5Cities_monthly_rainyDays_pivoted # Heatmap expects rectangular (pivot-like) data\n            ,annot=True # Put numbers inside the cells\n            ,fmt=\".0f\" # Force the number format\n            ,square=True # Square vs rectangular cells\n            ,cbar=False # Get rid of the colourbar legend on the side\n            ,cmap=\"crest\" # Colourmaps reference: https://matplotlib.org/stable/users/explain/colors/colormaps.html \n            ,ax=ax # Tell it to use the matplotlib axes we created earlier\n           )\n\n# We need to use some matplotlib code to set our output size, add a title, and capitalise our x-axis label\nax.tick_params(axis='x', labelsize=11, rotation=45) # I think 45-degree month labels look nicer, but this is a matter of taste.\nax.tick_params(axis='y', labelsize=11)\n\n# Manually changing our axis labels for more control\nax.set_xlabel(\"Month\",fontweight=\"bold\",fontsize=12) \nax.set_ylabel(\"City\",fontweight=\"bold\",fontsize=12)\n\n# Set our title dynamically\nax.set_title(f\"Mean number of rainy days by month between {date_min} and {date_max} for Australia's top 5 cities\", fontsize=16, fontweight=\"bold\", pad=15);\n\n\n\n\n\n\n\n\n\n\nA fancy multi-chart visual\nThis chart uses the boxgrid object to arrange multiple different subcharts. We’re actually generating two sets of different visuals (linegrid and boxgrid) in one output. If you’re not sure what the for [...] in [...] syntax means, don’t worry - this will be covered in a future session.\n\n# Setting up the grid of box plots\n# Box plots are a bit of a rabbit hole and are extremely customisable; we're mostly using defaults here\nboxgrid = sns.FacetGrid(df_top5Cities \n                        ,col=\"Location\" # Defining the different facets\n                        ,col_wrap=5, height=4.5 # Layout and sizing for our facet grid\n                        ,col_order=colOrder_top5Cities  # Using our alphabetical order of city names to arrange our facets\n)\nboxgrid.map(sns.boxplot # This is what tells sns what sort of plots we want in our grid\n            ,\"Month\" # X\n            ,\"MaxTemp\" # Y\n            ,linewidth=1.5\n            ,color=\"skyblue\"\n            ,order=month_order\n            ,fliersize=0 # Seaborn boxplots by default include markers for outliers, which it calls \"fliers\". For this chart we'd like to disable these.\n            ) \n\n# Setting up the grid of line plots\n# Using the default matplotlib plot here\nlinegrid = sns.FacetGrid(df_top5Cities.groupby(['Location', 'Month'])['Rainfall'].mean().reset_index()\n                         ,col=\"Location\" \n                         ,hue=\"Location\",palette=\"Set2\" \n                         ,col_wrap=5, height=4.5 \n                         ,col_order=colOrder_top5Cities\n)\nlinegrid.map(plt.plot, \"Month\", \"Rainfall\",marker=\"o\")\n\n# Formatting axes\nfor ax in boxgrid.axes.flat:\n    ax.tick_params(axis='x', labelsize=9, rotation=45)\n    ax.tick_params(axis='y', labelsize=9)\nfor ax in linegrid.axes.flat:\n    ax.tick_params(axis='x', labelsize=9, rotation=45)\n    ax.tick_params(axis='y', labelsize=9)\n\nlinegrid.set_titles(col_template=\"{col_name}\",fontweight=\"bold\",fontsize=16)\nlinegrid.set_axis_labels(\"Month\",\"Average rainfall /mm\",fontweight=\"bold\",fontsize=10)\nboxgrid.set_titles(col_template=\"{col_name}\",fontweight=\"bold\",fontsize=16)\nboxgrid.set_axis_labels(\"Month\",\"Max temp /$\\\\degree$C\",fontweight=\"bold\",fontsize=10)\n\n# Setting overall titles and spacing\nlinegrid.fig.suptitle(f\"Mean daily rainfall by month for top 5 cities, {date_range}\", fontsize=16, color='black',fontweight='bold') \nboxgrid.fig.suptitle(f\"Max temperature by month for top 5 cities, {date_range}\", fontsize=16, color='black',fontweight='bold') \nlinegrid.fig.subplots_adjust(top=0.85)  \nboxgrid.fig.subplots_adjust(top=0.85);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Small multiples”-type time-series grid\nThis is mostly based on the example given at https://seaborn.pydata.org/examples/timeseries_facets.html.\nFirstly, for this one we need another dataframe that just contains the Sydney data.\n\ndf_Sydney = df[df['Location']==\"Sydney\"].groupby(['Month', 'Location','Year'], as_index=False)['MaxTemp'].max()\n\nWe’re going to use the relplot function to create a grid of plots with a specific set of variables across its rows and columns. For each cell we’re plotting that year’s data with a different colour, and plotting all the other years in grey in the background.\n\n# plot each year's time series in its own facet\ng = sns.relplot(data=df_Sydney\n                ,x=\"Month\"\n                ,y=\"MaxTemp\"\n                ,col=\"Year\"\n                ,hue=\"Year\"\n                ,kind=\"line\"\n                ,palette=\"viridis\"\n                ,linewidth=4\n                ,zorder=5\n                ,col_wrap=3, height=2, aspect=1.5, legend=False,\n)\n\n# iterate over each subplot to customize further\nfor year, ax in g.axes_dict.items():\n\n    # Add the title as an annotation within the plot\n    ax.text(.8\n            ,.85\n            ,year\n            ,transform=ax.transAxes\n            ,fontweight=\"bold\",fontsize=9)\n\n    # Plot every year's time series in the background\n    sns.lineplot(\n        data=df_Sydney, x=\"Month\", y=\"MaxTemp\", units=\"Year\",\n        estimator=None, color=\".7\", linewidth=1, ax=ax\n    )\n\n# reduce the frequency of the x axis ticks\nax.set_xticks(ax.get_xticks()[::2])\n\n# tweak the supporting aspects of the plot\ng.set_titles(\"\")\ng.fig.suptitle(f\"Max temperature by month in Sydney, {date_range}\", fontsize=16, color='black',fontweight='bold') \ng.set_axis_labels(\"\", \"Max Temp /$\\\\degree \\\\mathrm{C}$\");\ng.tight_layout();",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "4. Visualisations with Seaborn"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#what-to-expect",
    "href": "sessions/01-onboarding/slides.html#what-to-expect",
    "title": "Python Onboarding",
    "section": "What to Expect?",
    "text": "What to Expect?\n\nLearning a language is hard. It can be frustrating. Perseverance is key to success.\nThese sessions will introduce you to Python, showing you what is possible and how to achieve some of what might benefit your work.\nBut the real learning comes by doing. You need to run the code yourself, have a play around, and cement what you’ve learned by applying it.\nPractice, repetition, and making mistakes along the way is how real progress is made."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#why-learn-python",
    "href": "sessions/01-onboarding/slides.html#why-learn-python",
    "title": "Python Onboarding",
    "section": "Why Learn Python?",
    "text": "Why Learn Python?\n\nCoding skills, generally, and Python specifically, seem to be a priority in the NHS right now. It’s a clear direction of travel. Learning now sets you up for the future.\nPython and the applied skills taught in these sessions will enable you to use advanced methods and design flexible, scalable solutions.\nPython is very valuable for career development.\nIt is (hopefully) fun!"
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#the-toolkit",
    "href": "sessions/01-onboarding/slides.html#the-toolkit",
    "title": "Python Onboarding",
    "section": "The Toolkit",
    "text": "The Toolkit\n\nWe will be using the following tools throughout this course:\n\nLanguage: Python\nDependency management: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code/Jupyter Notebooks (or your preferred IDE)\n\nYou can install all these tools by running the following in PowerShell:\n\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop"
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#python",
    "href": "sessions/01-onboarding/slides.html#python",
    "title": "Python Onboarding",
    "section": "Python",
    "text": "Python\n\nPython is an all-purpose programming language that is the most popular worldwide and widely used in almost every industry.\nPython’s popularity is owed to its flexibility – it is the second-best tool for every job.\nIt is a strong choice for data science and analytics, being one of the best languages for data wrangling, data visualisation, statistics, and machine learning.\n\nIt is also well-suited to web development, scientific computing, and automation."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#dependency-management",
    "href": "sessions/01-onboarding/slides.html#dependency-management",
    "title": "Python Onboarding",
    "section": "Dependency Management",
    "text": "Dependency Management\n\nDependency management refers to the process of tracking and managing all of the packages (dependencies) a project needs to run. It ensures:\n\nThe right packages are installed.\nThe correct versions are used.\nConflicts between packages are avoided.\n\nWe are using uv for dependency management."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#virtual-environments",
    "href": "sessions/01-onboarding/slides.html#virtual-environments",
    "title": "Python Onboarding",
    "section": "Virtual Environments",
    "text": "Virtual Environments\n\nVirtual environments are isolated Python environments that allow you to manage dependencies for a specific project without the state of those dependencies affecting other projects or your wider system. They help by:\n\nKeeping dependencies separate for each project.\nAvoiding version conflicts between projects.\nMaking dependency management more predictable and reproducible.\n\nVirtual environments are a part of dependency management, and we will use uv to manage both the dependencies and virtual environments."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#version-control",
    "href": "sessions/01-onboarding/slides.html#version-control",
    "title": "Python Onboarding",
    "section": "Version Control",
    "text": "Version Control\n\nVersion control is the practice of tracking and managing changes to code or files over time, allowing you to:\n\nRevert to earlier versions if needed.\nCollaborate with others on the same project easily.\nMaintain a history of changes.\n\nWe are using Git (the version control system) and GitHub (the platform for hosting our work)."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#ide",
    "href": "sessions/01-onboarding/slides.html#ide",
    "title": "Python Onboarding",
    "section": "IDE",
    "text": "IDE\n\nAn IDE (Integrated Development Environment) is fully featured software that provides everything you need to write code as conveniently as possible.\nIt typically includes a code editor, debugger, build tools, and features like syntax highlighting and code completion.\nSome common IDEs used for Python include VS Code, PyCharm, Vim, Jupyter Notebooks/JupyterLab, and Positron.\nWe will use VS Code or Jupyter Notebooks (which is not exactly an IDE but is similar)."
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html",
    "href": "sessions/02-jupyter_notebooks/index.html",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "This is the second session following Code Club’s relaunch. The focus is introducing jupyter notebooks and explaining to users how to get started with a new project and briefly introducing some key concepts.\nWe are also planning some time for Q&A following the first session.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#session-slides",
    "href": "sessions/02-jupyter_notebooks/index.html#session-slides",
    "title": "Jupyter Notebooks",
    "section": "Session Slides",
    "text": "Session Slides\nUse the left ⬅️ and right ➡️ arrow keys to navigate through the slides below. To view in a separate tab/window, follow this link.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#the-tools-you-will-need",
    "href": "sessions/02-jupyter_notebooks/index.html#the-tools-you-will-need",
    "title": "Jupyter Notebooks",
    "section": "The Tools You Will Need",
    "text": "The Tools You Will Need\nThough Jupyter notebooks can be used with a variety of coding languages and in different settings the key tools used in this session are:\n\nLanguage: Python\nDependency Management & Virtual Environments: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code/Jupyter Notebooks (or your preferred IDE)\n\nYou can install all the tools you’ll need by running the following one-liner run in PowerShell:\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop\nYou can find more information on these topics in the Python Onboarding session",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#project-setup",
    "href": "sessions/02-jupyter_notebooks/index.html#project-setup",
    "title": "Jupyter Notebooks",
    "section": "Project Setup",
    "text": "Project Setup\nOur project set-up will follow the same steps as used in the onboarding session, by using uv to set up a new project folder.\nTo get started we will use PowerShell powershell to open a command prompt, it should open in your C drive (e.g., C:\\Users\\user.name). If it does not, run cd ~, and it should return to your home directory. We recommend the use of a single folder to hold your python projects while learning, because we will be using git version control we will call this “Git”. we can use the command mkdir code_club to make this folder and then use cd code_club to relocate to this folder1.\nWe will create a new uv project in this directory using the command uv init. The new project will contain everything we need, including a Python installation, a virtual environment, and the necessary project files for tracking and managing any packages installed in the virtual environment. To set up a new project called test-project, use the following command:\nuv init test_project\nHaving created this new directory, navigate to it using cd test_project.\nFor this session you will need to add 3 Python packages, ipykernel2, pandas and seaborn We can use the following command:\nuv add ipykernel pandas seaborn\nWe are going to create a blank notebook in this file by running the command new-item first_notebook.ipynb if you now run ls you will note this file has been created\nYour Python project is now set up, and you are ready to start writing some code. You can open VS Code from your PowerShell window by running code ..\n\nOpening your project in VS Code\nYou could also do this from within VS Code as most IDEs include a terminal interface which will be demonstrated in session.\nFor now launch VS Code and click File &gt; Open Folder.... You’ll want to make sure you select the root level of your project. Once you’ve opened the folder, the file navigation pane in VS Code should display the files that uv has created, as well as the notebook you created: first_notebook.ipynb. Click on this to open it.\nOnce VS Code realises you’ve opened a folder with Python code and a virtual environment, it should do the following:\n\nSuggest you install the Python extension (and, once you’ve created a Jupyter notebook, the Jupyter one) offered by Microsoft - go ahead and do this. If this doesn’t happen, you can install extensions manually from the Extensions pane on the left-hand side.\nSelect the uv-created .venv as the python Environment we’re going to use to actually run our code. If this doesn’t happen, press ctrl-shift-P, type “python environment” to find the Python - Create Environment... option, hit enter, choose “Venv” and proceed to “Use Existing”.\n\nIf VS Code has found the virtual environment, it may pick up the correct kernel. If not you may need to select this manually this can be done by clicking in the top right where you can see Select Kernel (see below)\n\n\n\nClick ‘Select Kernel’\n\n\nWe can then select the appropriate kernel from python environments and looking for\n\n\n\nclick Python Environments\n\n\n\n\n\nclick venv - recommended\n\n\nOnce the kernel is enabled you are ready to start adding cells to your notebook. these can either be code cells which is where you include your program elements or markdown which enable the addition of headings, analysis and commentary.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#footnotes",
    "href": "sessions/02-jupyter_notebooks/index.html#footnotes",
    "title": "Jupyter Notebooks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe recommend using the C drive for all Python projects, especially if using version control. Storing projects like these on One Drive will create many unnecessary issues. It can be helpful to use a sub-directory to store projects but is not necessary and is not a requirement for code club↩︎\nStrictly speaking, we should install ipykernel as a development dependency (a dependency that is needed for any development but not when the project is put into production). In this case, we would add it by running uv add --dev ipykernel. However, in this case, it is simpler to just add it as a regular dependency, and it doesn’t harm.↩︎",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html",
    "href": "sessions/03-eda-pandas/index.html",
    "title": "Exploring Data Using Pandas",
    "section": "",
    "text": "This is the first of four sessions looking at how to explore data in Python. This session will focus on introducing the Python library, pandas. We will use pandas to import, inspect, summarise, and transform the data, illustrating a typical exploratory data analysis workflow.\nWe are using Australian weather data, taken from Kaggle. This dataset is used to build machine learning models that predict whether it will rain tomorrow, using data about the weather every day from 2007 to 2017. To download the data, click here.\n# install necessary packages\n# !uv add skimpy\n\n# import packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom skimpy import skim\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#setting-the-scene",
    "href": "sessions/03-eda-pandas/index.html#setting-the-scene",
    "title": "Exploring Data Using Pandas",
    "section": "Setting the Scene",
    "text": "Setting the Scene\nBefore we start to explore any dataset, we need to establish what we are looking to do with the data. This should inform our decisions wwith any exploration, and any analysis that follows.\nQuestions:\n\nWhat are we trying to achieve?\nHow do our goals impact our analysis?\nWhat should we take into consideration before we write any code?\nWhat sort of questions might we be interested in with this dataset?\n\n\nWhat Our Data Can Tell Us (And What it Can’t)\nWe also need to consider what the data is and where it came from.\nQuestions:\n\nHow was the data collected?\nWhat is it missing?\nWhat do the variables in our dataset actually mean, and are they a good approximation of the concepts we are interested in?",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#exploring-the-dataset",
    "href": "sessions/03-eda-pandas/index.html#exploring-the-dataset",
    "title": "Exploring Data Using Pandas",
    "section": "Exploring the Dataset",
    "text": "Exploring the Dataset\nFirst, we should start with dataset-wide operations.\nQuestions:\n\nWhat do we want to know about a dataset when we first encounter it?\nHow do we get a quick overview of the data that can help us in our next steps?\nWe need to get a “feel” for the data before we can really make any decisions about how to analyse it. How do we get there with a new dataset?\n\nWe can start by getting a quick glance at the data. The starting point when you have just imported a new dataset is usually the pandas function pd.DataFrame.head(), which shows the top \\(n\\) rows of the dataset (by default it shows the top five rows).\n\n# view the top five rows\ndf.head()\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n0\n2008-12-01\nAlbury\n13.4\n22.9\n0.6\nNaN\nNaN\nW\n44.0\nW\n...\n71.0\n22.0\n1007.7\n1007.1\n8.0\nNaN\n16.9\n21.8\nNo\nNo\n\n\n1\n2008-12-02\nAlbury\n7.4\n25.1\n0.0\nNaN\nNaN\nWNW\n44.0\nNNW\n...\n44.0\n25.0\n1010.6\n1007.8\nNaN\nNaN\n17.2\n24.3\nNo\nNo\n\n\n2\n2008-12-03\nAlbury\n12.9\n25.7\n0.0\nNaN\nNaN\nWSW\n46.0\nW\n...\n38.0\n30.0\n1007.6\n1008.7\nNaN\n2.0\n21.0\n23.2\nNo\nNo\n\n\n3\n2008-12-04\nAlbury\n9.2\n28.0\n0.0\nNaN\nNaN\nNE\n24.0\nSE\n...\n45.0\n16.0\n1017.6\n1012.8\nNaN\nNaN\n18.1\n26.5\nNo\nNo\n\n\n4\n2008-12-05\nAlbury\n17.5\n32.3\n1.0\nNaN\nNaN\nW\n41.0\nENE\n...\n82.0\n33.0\n1010.8\n1006.0\n7.0\n8.0\n17.8\n29.7\nNo\nNo\n\n\n\n\n5 rows × 23 columns\n\n\n\nYou can also look at the bottom rows of the dataset, using pd.DataFrame.tail(). This might be useful if you are dealing with time-series data. Below, we specify that we want to look at the bottom ten rows.\n\n# view the bottom ten rows\ndf.tail(10)\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n145450\n2017-06-16\nUluru\n5.2\n24.3\n0.0\nNaN\nNaN\nE\n24.0\nSE\n...\n53.0\n24.0\n1023.8\n1020.0\nNaN\nNaN\n12.3\n23.3\nNo\nNo\n\n\n145451\n2017-06-17\nUluru\n6.4\n23.4\n0.0\nNaN\nNaN\nESE\n31.0\nS\n...\n53.0\n25.0\n1025.8\n1023.0\nNaN\nNaN\n11.2\n23.1\nNo\nNo\n\n\n145452\n2017-06-18\nUluru\n8.0\n20.7\n0.0\nNaN\nNaN\nESE\n41.0\nSE\n...\n56.0\n32.0\n1028.1\n1024.3\nNaN\n7.0\n11.6\n20.0\nNo\nNo\n\n\n145453\n2017-06-19\nUluru\n7.4\n20.6\n0.0\nNaN\nNaN\nE\n35.0\nESE\n...\n63.0\n33.0\n1027.2\n1023.3\nNaN\nNaN\n11.0\n20.3\nNo\nNo\n\n\n145454\n2017-06-20\nUluru\n3.5\n21.8\n0.0\nNaN\nNaN\nE\n31.0\nESE\n...\n59.0\n27.0\n1024.7\n1021.2\nNaN\nNaN\n9.4\n20.9\nNo\nNo\n\n\n145455\n2017-06-21\nUluru\n2.8\n23.4\n0.0\nNaN\nNaN\nE\n31.0\nSE\n...\n51.0\n24.0\n1024.6\n1020.3\nNaN\nNaN\n10.1\n22.4\nNo\nNo\n\n\n145456\n2017-06-22\nUluru\n3.6\n25.3\n0.0\nNaN\nNaN\nNNW\n22.0\nSE\n...\n56.0\n21.0\n1023.5\n1019.1\nNaN\nNaN\n10.9\n24.5\nNo\nNo\n\n\n145457\n2017-06-23\nUluru\n5.4\n26.9\n0.0\nNaN\nNaN\nN\n37.0\nSE\n...\n53.0\n24.0\n1021.0\n1016.8\nNaN\nNaN\n12.5\n26.1\nNo\nNo\n\n\n145458\n2017-06-24\nUluru\n7.8\n27.0\n0.0\nNaN\nNaN\nSE\n28.0\nSSE\n...\n51.0\n24.0\n1019.4\n1016.5\n3.0\n2.0\n15.1\n26.0\nNo\nNo\n\n\n145459\n2017-06-25\nUluru\n14.9\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nESE\n...\n62.0\n36.0\n1020.2\n1017.9\n8.0\n8.0\n15.0\n20.9\nNo\nNaN\n\n\n\n\n10 rows × 23 columns\n\n\n\nA quick glimpse at the data is useful, but we may also want to get quick descriptions of several aspects of the data. Such as the length of the dataset (len(), which can also be used to get the length of various Python objects), which tells us how many observations we have.\n\n# get the object length\nlen(df)\n\n145460\n\n\nAnother option is pd.DataFrame.shape(), which shows the length (number of rows) and width (number of columns).\n\n# get the object shape (number of rows, number of columns)\ndf.shape\n\n(145460, 23)\n\n\nSpeaking of columns, if we want a quick list of the column names, we can get this using pd.DataFrame.columns().\n\n# get all column names\ndf.columns\n\nIndex(['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation',\n       'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm',\n       'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',\n       'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am',\n       'Temp3pm', 'RainToday', 'RainTomorrow'],\n      dtype='object')\n\n\nA quick and easy way to get some valuable information about the dataset is pd.DataFrame.info(), including the total non-null observations and data type1 of each column.\n\n# get dataframe info (column indices, non-null counts, data types)\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 145460 entries, 0 to 145459\nData columns (total 23 columns):\n #   Column         Non-Null Count   Dtype  \n---  ------         --------------   -----  \n 0   Date           145460 non-null  object \n 1   Location       145460 non-null  object \n 2   MinTemp        143975 non-null  float64\n 3   MaxTemp        144199 non-null  float64\n 4   Rainfall       142199 non-null  float64\n 5   Evaporation    82670 non-null   float64\n 6   Sunshine       75625 non-null   float64\n 7   WindGustDir    135134 non-null  object \n 8   WindGustSpeed  135197 non-null  float64\n 9   WindDir9am     134894 non-null  object \n 10  WindDir3pm     141232 non-null  object \n 11  WindSpeed9am   143693 non-null  float64\n 12  WindSpeed3pm   142398 non-null  float64\n 13  Humidity9am    142806 non-null  float64\n 14  Humidity3pm    140953 non-null  float64\n 15  Pressure9am    130395 non-null  float64\n 16  Pressure3pm    130432 non-null  float64\n 17  Cloud9am       89572 non-null   float64\n 18  Cloud3pm       86102 non-null   float64\n 19  Temp9am        143693 non-null  float64\n 20  Temp3pm        141851 non-null  float64\n 21  RainToday      142199 non-null  object \n 22  RainTomorrow   142193 non-null  object \ndtypes: float64(16), object(7)\nmemory usage: 25.5+ MB\n\n\nIf we wanted to get a better sense of the null values in each column, we could calculate the percentage of null values by capturing whether each row of each column is null (pd.DataFrame.isnull()), summing the total null values in each column (pd.DataFrame.sum()), and then dividing by the length of the dataframe (/len()).\n\n# calculate the percentage of null values in each column\ndf.isnull().sum()/len(df)\n\nDate             0.000000\nLocation         0.000000\nMinTemp          0.010209\nMaxTemp          0.008669\nRainfall         0.022419\nEvaporation      0.431665\nSunshine         0.480098\nWindGustDir      0.070989\nWindGustSpeed    0.070555\nWindDir9am       0.072639\nWindDir3pm       0.029066\nWindSpeed9am     0.012148\nWindSpeed3pm     0.021050\nHumidity9am      0.018246\nHumidity3pm      0.030984\nPressure9am      0.103568\nPressure3pm      0.103314\nCloud9am         0.384216\nCloud3pm         0.408071\nTemp9am          0.012148\nTemp3pm          0.024811\nRainToday        0.022419\nRainTomorrow     0.022460\ndtype: float64\n\n\nIf we want a quick summary of all the numeric columns in the dataset, we can use pd.DataFrame.describe().\n\n# quick summary of numeric variables\ndf.describe()\n\n\n\n\n\n\n\n\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustSpeed\nWindSpeed9am\nWindSpeed3pm\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\n\n\n\n\ncount\n143975.000000\n144199.000000\n142199.000000\n82670.000000\n75625.000000\n135197.000000\n143693.000000\n142398.000000\n142806.000000\n140953.000000\n130395.00000\n130432.000000\n89572.000000\n86102.000000\n143693.000000\n141851.00000\n\n\nmean\n12.194034\n23.221348\n2.360918\n5.468232\n7.611178\n40.035230\n14.043426\n18.662657\n68.880831\n51.539116\n1017.64994\n1015.255889\n4.447461\n4.509930\n16.990631\n21.68339\n\n\nstd\n6.398495\n7.119049\n8.478060\n4.193704\n3.785483\n13.607062\n8.915375\n8.809800\n19.029164\n20.795902\n7.10653\n7.037414\n2.887159\n2.720357\n6.488753\n6.93665\n\n\nmin\n-8.500000\n-4.800000\n0.000000\n0.000000\n0.000000\n6.000000\n0.000000\n0.000000\n0.000000\n0.000000\n980.50000\n977.100000\n0.000000\n0.000000\n-7.200000\n-5.40000\n\n\n25%\n7.600000\n17.900000\n0.000000\n2.600000\n4.800000\n31.000000\n7.000000\n13.000000\n57.000000\n37.000000\n1012.90000\n1010.400000\n1.000000\n2.000000\n12.300000\n16.60000\n\n\n50%\n12.000000\n22.600000\n0.000000\n4.800000\n8.400000\n39.000000\n13.000000\n19.000000\n70.000000\n52.000000\n1017.60000\n1015.200000\n5.000000\n5.000000\n16.700000\n21.10000\n\n\n75%\n16.900000\n28.200000\n0.800000\n7.400000\n10.600000\n48.000000\n19.000000\n24.000000\n83.000000\n66.000000\n1022.40000\n1020.000000\n7.000000\n7.000000\n21.600000\n26.40000\n\n\nmax\n33.900000\n48.100000\n371.000000\n145.000000\n14.500000\n135.000000\n130.000000\n87.000000\n100.000000\n100.000000\n1041.00000\n1039.600000\n9.000000\n9.000000\n40.200000\n46.70000\n\n\n\n\n\n\n\nHowever, I prefer to bring in another package, skimpy, that does all of this very quickly and cleanly. We can get a detailed description of the entire dataset using skim().\n\n# a more informative summary function from the skimpy package\nskim(df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ Dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 145460 │ │ float64     │ 16    │                                                          │\n│ │ Number of columns │ 23     │ │ string      │ 7     │                                                          │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━━━┓  │\n│ ┃ column         ┃ NA     ┃ NA %                ┃ mean  ┃ sd    ┃ p0    ┃ p25  ┃ p50  ┃ p75  ┃ p100 ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━━━┩  │\n│ │ MinTemp        │   1485 │  1.0208992162793895 │ 12.19 │ 6.398 │  -8.5 │  7.6 │   12 │ 16.9 │ 33.9 │  ▃▇▇▃  │  │\n│ │ MaxTemp        │   1261 │  0.8669049910628351 │ 23.22 │ 7.119 │  -4.8 │ 17.9 │ 22.6 │ 28.2 │ 48.1 │  ▁▇▇▃  │  │\n│ │ Rainfall       │   3261 │   2.241853430496356 │ 2.361 │ 8.478 │     0 │    0 │    0 │  0.8 │  371 │   ▇    │  │\n│ │ Evaporation    │  62790 │    43.1665062560154 │ 5.468 │ 4.194 │     0 │  2.6 │  4.8 │  7.4 │  145 │   ▇    │  │\n│ │ Sunshine       │  69835 │   48.00976213391998 │ 7.611 │ 3.785 │     0 │  4.8 │  8.4 │ 10.6 │ 14.5 │ ▃▃▅▆▇▃ │  │\n│ │ WindGustSpeed  │  10263 │   7.055547916953114 │ 40.04 │ 13.61 │     6 │   31 │   39 │   48 │  135 │  ▂▇▂   │  │\n│ │ WindSpeed9am   │   1767 │   1.214766946239516 │ 14.04 │ 8.915 │     0 │    7 │   13 │   19 │  130 │   ▇▂   │  │\n│ │ WindSpeed3pm   │   3062 │   2.105046060772721 │ 18.66 │  8.81 │     0 │   13 │   19 │   24 │   87 │  ▅▇▂   │  │\n│ │ Humidity9am    │   2654 │  1.8245565791282827 │ 68.88 │ 19.03 │     0 │   57 │   70 │   83 │  100 │  ▁▂▇▇▆ │  │\n│ │ Humidity3pm    │   4507 │    3.09844630826344 │ 51.54 │  20.8 │     0 │   37 │   52 │   66 │  100 │ ▁▅▆▇▅▂ │  │\n│ │ Pressure9am    │  15065 │     10.356799120033 │  1018 │ 7.107 │ 980.5 │ 1013 │ 1018 │ 1022 │ 1041 │   ▂▇▅  │  │\n│ │ Pressure3pm    │  15028 │  10.331362573903478 │  1015 │ 7.037 │ 977.1 │ 1010 │ 1015 │ 1020 │ 1040 │   ▂▇▅  │  │\n│ │ Cloud9am       │  55888 │   38.42155919153032 │ 4.447 │ 2.887 │     0 │    1 │    5 │    7 │    9 │ ▇▂▃▂▇▅ │  │\n│ │ Cloud3pm       │  59358 │   40.80709473394748 │  4.51 │  2.72 │     0 │    2 │    5 │    7 │    9 │ ▆▂▃▂▇▃ │  │\n│ │ Temp9am        │   1767 │   1.214766946239516 │ 16.99 │ 6.489 │  -7.2 │ 12.3 │ 16.7 │ 21.6 │ 40.2 │  ▂▇▇▃  │  │\n│ │ Temp3pm        │   3609 │  2.4810944589577892 │ 21.68 │ 6.937 │  -5.4 │ 16.6 │ 21.1 │ 26.4 │ 46.7 │  ▁▇▇▃  │  │\n│ └────────────────┴────────┴─────────────────────┴───────┴───────┴───────┴──────┴──────┴──────┴──────┴────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┓  │\n│ ┃          ┃       ┃          ┃          ┃          ┃          ┃          ┃ chars per ┃ words    ┃ total     ┃  │\n│ ┃ column   ┃ NA    ┃ NA %     ┃ shortest ┃ longest  ┃ min      ┃ max      ┃ row       ┃ per row  ┃ words     ┃  │\n│ ┡━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━┩  │\n│ │ Date     │     0 │        0 │ 2008-12- │ 2008-12- │ 2007-11- │ 2017-06- │        10 │        1 │    145460 │  │\n│ │          │       │          │ 01       │ 01       │ 01       │ 25       │           │          │           │  │\n│ │ Location │     0 │        0 │ Sale     │ Melbourn │ Adelaide │ Woomera  │      8.71 │        1 │    145460 │  │\n│ │          │       │          │          │ eAirport │          │          │           │          │           │  │\n│ │ WindGust │ 10326 │ 7.098858 │ W        │ WNW      │ E        │ WSW      │      2.19 │     0.93 │    135134 │  │\n│ │ Dir      │       │ 79279527 │          │          │          │          │           │          │           │  │\n│ │ WindDir9 │ 10566 │ 7.263852 │ W        │ NNW      │ E        │ WSW      │      2.18 │     0.93 │    134894 │  │\n│ │ am       │       │ 60552729 │          │          │          │          │           │          │           │  │\n│ │          │       │        2 │          │          │          │          │           │          │           │  │\n│ │ WindDir3 │  4228 │ 2.906641 │ E        │ WNW      │ E        │ WSW      │      2.21 │     0.97 │    141232 │  │\n│ │ pm       │       │ 00096246 │          │          │          │          │           │          │           │  │\n│ │          │       │        4 │          │          │          │          │           │          │           │  │\n│ │ RainToda │  3261 │ 2.241853 │ No       │ Yes      │ No       │ Yes      │      2.22 │     0.98 │    142199 │  │\n│ │ y        │       │ 43049635 │          │          │          │          │           │          │           │  │\n│ │          │       │        6 │          │          │          │          │           │          │           │  │\n│ │ RainTomo │  3267 │ 2.245978 │ No       │ Yes      │ No       │ Yes      │      2.22 │     0.98 │    142193 │  │\n│ │ rrow     │       │ 27581465 │          │          │          │          │           │          │           │  │\n│ │          │       │        7 │          │          │          │          │           │          │           │  │\n│ └──────────┴───────┴──────────┴──────────┴──────────┴──────────┴──────────┴───────────┴──────────┴───────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#exploring-variables-columns-observations-rows",
    "href": "sessions/03-eda-pandas/index.html#exploring-variables-columns-observations-rows",
    "title": "Exploring Data Using Pandas",
    "section": "Exploring Variables (Columns) & Observations (Rows)",
    "text": "Exploring Variables (Columns) & Observations (Rows)\nIf we are going to narrow our focus to specific variables or groups of observations, we need to know how to select columns, filter values, and group the data. There are lots of different ways we can slice up the data. We won’t cover all of them here2, but we will try to cover a range that helps illustrate how pandas works and will help you build the intuition for working with data in pandas.\nWe can select columns in a variety of ways, but the “correct” way to select columns in most circumstances is using selection brackets (the square brackets []), also known as the indexing operator.\n\n# selecting a single column by name\ndf['Date']\n\n# alternative ways to select columns\n# df.loc[:, 'Date']\n# df.Date\n\n0         2008-12-01\n1         2008-12-02\n2         2008-12-03\n3         2008-12-04\n4         2008-12-05\n             ...    \n145455    2017-06-21\n145456    2017-06-22\n145457    2017-06-23\n145458    2017-06-24\n145459    2017-06-25\nName: Date, Length: 145460, dtype: object\n\n\nIf we want to select multiple columns, we can use double squared brackets ([[ ]]). This is the same process as before, but the inner brackets define a list, and the outer are the selection brackets.\n\n# selecting multiple columns (and all rows) by name\ndf[['Date', 'Location', 'Rainfall']]\n# df.loc[:, ['Date', 'Location', 'Rainfall']]\n\n\n\n\n\n\n\n\nDate\nLocation\nRainfall\n\n\n\n\n0\n2008-12-01\nAlbury\n0.6\n\n\n1\n2008-12-02\nAlbury\n0.0\n\n\n2\n2008-12-03\nAlbury\n0.0\n\n\n3\n2008-12-04\nAlbury\n0.0\n\n\n4\n2008-12-05\nAlbury\n1.0\n\n\n...\n...\n...\n...\n\n\n145455\n2017-06-21\nUluru\n0.0\n\n\n145456\n2017-06-22\nUluru\n0.0\n\n\n145457\n2017-06-23\nUluru\n0.0\n\n\n145458\n2017-06-24\nUluru\n0.0\n\n\n145459\n2017-06-25\nUluru\n0.0\n\n\n\n\n145460 rows × 3 columns\n\n\n\nWhile selection brackets are a quick and easy solution if we want to grab a subset of variables in the dataset, it is realy only intended to be used for simple operations using only column selection.\nFor row selection, we should use pd.DataFrame.iloc[]. The iloc function is used for “integer position” selection, which means you can select rows or columns using their integer position. For rows 10-15, you can select them using the following:\n\n# slicing by rows\ndf.iloc[10:16]\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n10\n2008-12-11\nAlbury\n13.4\n30.4\n0.0\nNaN\nNaN\nN\n30.0\nSSE\n...\n48.0\n22.0\n1011.8\n1008.7\nNaN\nNaN\n20.4\n28.8\nNo\nYes\n\n\n11\n2008-12-12\nAlbury\n15.9\n21.7\n2.2\nNaN\nNaN\nNNE\n31.0\nNE\n...\n89.0\n91.0\n1010.5\n1004.2\n8.0\n8.0\n15.9\n17.0\nYes\nYes\n\n\n12\n2008-12-13\nAlbury\n15.9\n18.6\n15.6\nNaN\nNaN\nW\n61.0\nNNW\n...\n76.0\n93.0\n994.3\n993.0\n8.0\n8.0\n17.4\n15.8\nYes\nYes\n\n\n13\n2008-12-14\nAlbury\n12.6\n21.0\n3.6\nNaN\nNaN\nSW\n44.0\nW\n...\n65.0\n43.0\n1001.2\n1001.8\nNaN\n7.0\n15.8\n19.8\nYes\nNo\n\n\n14\n2008-12-15\nAlbury\n8.4\n24.6\n0.0\nNaN\nNaN\nNaN\nNaN\nS\n...\n57.0\n32.0\n1009.7\n1008.7\nNaN\nNaN\n15.9\n23.5\nNo\nNaN\n\n\n15\n2008-12-16\nAlbury\n9.8\n27.7\nNaN\nNaN\nNaN\nWNW\n50.0\nNaN\n...\n50.0\n28.0\n1013.4\n1010.3\n0.0\nNaN\n17.3\n26.2\nNaN\nNo\n\n\n\n\n6 rows × 23 columns\n\n\n\nWe can do similar using a column’s integer position, but we have to select all rows (:) first:\n\n# using iloc with columns\ndf.iloc[:, 20]\n\n0         21.8\n1         24.3\n2         23.2\n3         26.5\n4         29.7\n          ... \n145455    22.4\n145456    24.5\n145457    26.1\n145458    26.0\n145459    20.9\nName: Temp3pm, Length: 145460, dtype: float64\n\n\nFinally, we can put both together to take a subset of both rows and columns:\n\n# using iloc with rows and columns\ndf.iloc[10:16, 20]\n\n10    28.8\n11    17.0\n12    15.8\n13    19.8\n14    23.5\n15    26.2\nName: Temp3pm, dtype: float64\n\n\nHowever, selecting by integer position is relatively limited. It is more likely we would want to subset the data based on the values of certain columns. We can filter rows by condition using pd.DataFrame.loc[]. The loc function slices by label, instead of integer position.\nFor example, we might want to look at a subset of the data based on location.\n\n# select all observations in Perth\ndf.loc[df['Location'] == 'Perth']\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n120638\n2008-07-01\nPerth\n2.7\n18.8\n0.0\n0.8\n9.1\nENE\n20.0\nNaN\n...\n97.0\n53.0\n1027.6\n1024.5\n2.0\n3.0\n8.5\n18.1\nNo\nNo\n\n\n120639\n2008-07-02\nPerth\n6.4\n20.7\n0.0\n1.8\n7.0\nNE\n22.0\nESE\n...\n80.0\n39.0\n1024.1\n1019.0\n0.0\n6.0\n11.1\n19.7\nNo\nNo\n\n\n120640\n2008-07-03\nPerth\n6.5\n19.9\n0.4\n2.2\n7.3\nNE\n31.0\nNaN\n...\n84.0\n71.0\n1016.8\n1015.6\n1.0\n3.0\n12.1\n17.7\nNo\nYes\n\n\n120641\n2008-07-04\nPerth\n9.5\n19.2\n1.8\n1.2\n4.7\nW\n26.0\nNNE\n...\n93.0\n73.0\n1019.3\n1018.4\n6.0\n6.0\n13.2\n17.7\nYes\nYes\n\n\n120642\n2008-07-05\nPerth\n9.5\n16.4\n1.8\n1.4\n4.9\nWSW\n44.0\nW\n...\n69.0\n57.0\n1020.4\n1022.1\n7.0\n5.0\n15.9\n16.0\nYes\nYes\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n123826\n2017-06-21\nPerth\n10.3\n19.9\n0.2\n1.8\n7.5\nNW\n37.0\nNNE\n...\n89.0\n60.0\n1017.1\n1013.8\n5.0\n6.0\n13.0\n18.5\nNo\nYes\n\n\n123827\n2017-06-22\nPerth\n13.0\n16.8\n61.2\n3.6\n0.0\nSSW\n46.0\nW\n...\n90.0\n75.0\n1005.6\n1008.9\n7.0\n7.0\n16.4\n15.6\nYes\nNo\n\n\n123828\n2017-06-23\nPerth\n13.3\n18.9\n0.4\n1.8\n6.5\nSE\n37.0\nSE\n...\n85.0\n65.0\n1019.2\n1019.4\n6.0\n6.0\n15.1\n18.0\nNo\nNo\n\n\n123829\n2017-06-24\nPerth\n11.5\n18.2\n0.0\n3.8\n9.3\nSE\n30.0\nESE\n...\n62.0\n47.0\n1025.9\n1023.4\n1.0\n3.0\n14.0\n17.6\nNo\nNo\n\n\n123830\n2017-06-25\nPerth\n6.3\n17.0\n0.0\n1.6\n7.9\nE\n26.0\nSE\n...\n75.0\n49.0\n1028.6\n1026.0\n1.0\n3.0\n11.5\n15.6\nNo\nNo\n\n\n\n\n3193 rows × 23 columns\n\n\n\nWe can also filter by multiple values, such as location and rainfall.\n\ndf.loc[(df['Rainfall'] == 0) & (df['Location'] == 'Perth')]\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n120638\n2008-07-01\nPerth\n2.7\n18.8\n0.0\n0.8\n9.1\nENE\n20.0\nNaN\n...\n97.0\n53.0\n1027.6\n1024.5\n2.0\n3.0\n8.5\n18.1\nNo\nNo\n\n\n120639\n2008-07-02\nPerth\n6.4\n20.7\n0.0\n1.8\n7.0\nNE\n22.0\nESE\n...\n80.0\n39.0\n1024.1\n1019.0\n0.0\n6.0\n11.1\n19.7\nNo\nNo\n\n\n120644\n2008-07-07\nPerth\n0.7\n18.3\n0.0\n0.8\n9.3\nN\n37.0\nNE\n...\n72.0\n36.0\n1028.9\n1024.2\n1.0\n5.0\n8.7\n17.9\nNo\nNo\n\n\n120645\n2008-07-08\nPerth\n3.2\n20.4\n0.0\n1.4\n6.9\nNNW\n24.0\nNE\n...\n58.0\n42.0\n1023.9\n1021.1\n6.0\n5.0\n10.2\n19.3\nNo\nYes\n\n\n120651\n2008-07-14\nPerth\n7.9\n19.7\n0.0\n0.2\n6.5\nNE\n31.0\nNE\n...\n86.0\n41.0\n1026.0\n1021.9\n6.0\n5.0\n11.7\n18.7\nNo\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n123823\n2017-06-18\nPerth\n7.5\n23.4\n0.0\n1.8\n9.2\nNNE\n28.0\nENE\n...\n67.0\n41.0\n1026.9\n1022.9\n0.0\n0.0\n14.2\n22.2\nNo\nNo\n\n\n123824\n2017-06-19\nPerth\n5.5\n23.0\n0.0\n3.0\n9.1\nSW\n19.0\nENE\n...\n84.0\n55.0\n1023.0\n1020.3\n1.0\n2.0\n11.5\n22.0\nNo\nNo\n\n\n123825\n2017-06-20\nPerth\n7.8\n22.5\n0.0\n2.8\n9.1\nNW\n26.0\nW\n...\n98.0\n59.0\n1019.3\n1015.9\n1.0\n1.0\n13.5\n21.6\nNo\nNo\n\n\n123829\n2017-06-24\nPerth\n11.5\n18.2\n0.0\n3.8\n9.3\nSE\n30.0\nESE\n...\n62.0\n47.0\n1025.9\n1023.4\n1.0\n3.0\n14.0\n17.6\nNo\nNo\n\n\n123830\n2017-06-25\nPerth\n6.3\n17.0\n0.0\n1.6\n7.9\nE\n26.0\nSE\n...\n75.0\n49.0\n1028.6\n1026.0\n1.0\n3.0\n11.5\n15.6\nNo\nNo\n\n\n\n\n2293 rows × 23 columns\n\n\n\nFor any complex process for subsetting the data, including multiple conditions, pd.DataFrame.loc[] is the best bet.\n\nSummarising Data\nNow that we know how to select the variables or observations we are interested in, we can start doing some descriptive analysis. The operations we use will depend on the questions we are trying to answer, and the possibilities will be almost endless.\nQuestions:\n\nWhat “functions” might we need to carry out on our data when we are exploring it?\n\nWe know that the weather data includes observations from all over the country, but we might want to check exactly how many different locations there are. We can use pd.DataFrame.nunique() to do this.\n\n# count unique values\ndf['Location'].nunique()\n\n49\n\n\nWe may also be interested in the locations themselves, which may tell us more about the spatial distribution of our data. In this case, we can use pd.DataFrame.unique().\n\n# get unique values\ndf['Location'].unique()\n\narray(['Albury', 'BadgerysCreek', 'Cobar', 'CoffsHarbour', 'Moree',\n       'Newcastle', 'NorahHead', 'NorfolkIsland', 'Penrith', 'Richmond',\n       'Sydney', 'SydneyAirport', 'WaggaWagga', 'Williamtown',\n       'Wollongong', 'Canberra', 'Tuggeranong', 'MountGinini', 'Ballarat',\n       'Bendigo', 'Sale', 'MelbourneAirport', 'Melbourne', 'Mildura',\n       'Nhil', 'Portland', 'Watsonia', 'Dartmoor', 'Brisbane', 'Cairns',\n       'GoldCoast', 'Townsville', 'Adelaide', 'MountGambier', 'Nuriootpa',\n       'Woomera', 'Albany', 'Witchcliffe', 'PearceRAAF', 'PerthAirport',\n       'Perth', 'SalmonGums', 'Walpole', 'Hobart', 'Launceston',\n       'AliceSprings', 'Darwin', 'Katherine', 'Uluru'], dtype=object)\n\n\nAnother common operation we might look to do is calculating the mean value (pd.DataFrame.mean()) of a certain variable. What is the average value of sunshine across the entire dataset?\n\n# calculate variable mean\ndf['Sunshine'].mean()\n\nnp.float64(7.6111775206611565)\n\n\nThis gives us the mean to many decimal places, and we probably don’t need to know the average sunshine hours to this level of precision. We can use the pd.DataFrame.round() function to round to two decimal places.\n\n# round mean value\ndf['Sunshine'].mean().round(2)\n\nnp.float64(7.61)\n\n\nMany operations will return the value with information about the object’s type included. The above values are wrapped in np.float64() because pd.DataFrame.mean() uses numpy to calculate the mean value. However, if you want to strip this information out so you only see the value itself, you can use print().\n\n# print mean value\nprint(df['Sunshine'].mean().round(2))\n\n7.61\n\n\nWhile we are often interested in the mean value when we talk about averages, we might want to know the median instead (pd.DataFrame.median()).\n\n# calculate other summary statistics\nprint(df['Sunshine'].median())\n\n8.4\n\n\nAnother common calculation is summing values (pd.DataFrame.sum()). We can use sum() to see the total hours of sunshine in our dataset, and we can use int() to convert this value to an integer (which also means we don’t need to use print()3).\n\n# calculate sum value and return an integer\nint(df['Sunshine'].sum())\n\n575595\n\n\nWe can also apply these summary operations on multiple variables, using the same selection logic as before (using double squared brackets).\n\nprint(df[['Sunshine', 'Rainfall']].mean())\n\nSunshine    7.611178\nRainfall    2.360918\ndtype: float64\n\n\nAnd we can apply multiple functions, using pd.DataFrame.agg().\n\ndf['Sunshine'].agg(['mean', 'median', 'sum']).round(1)\n\nmean           7.6\nmedian         8.4\nsum       575595.3\nName: Sunshine, dtype: float64\n\n\nThe next step when exploring specific variables will often be group-level summaries. The average amount of sunshine across the whole dataset has limited utility, but the average hours of sunshine in each location allows us to compare between locations and start to understand how different variables are related to each other. If we want to do a group-level operation, we have to use pd.DataFrame.groupby().\n\n# calculate group means\ndf.groupby(by='Location')['Sunshine'].mean().round(1)\n\nLocation\nAdelaide            7.7\nAlbany              6.7\nAlbury              NaN\nAliceSprings        9.6\nBadgerysCreek       NaN\nBallarat            NaN\nBendigo             NaN\nBrisbane            8.1\nCairns              7.6\nCanberra            7.4\nCobar               8.7\nCoffsHarbour        7.4\nDartmoor            6.5\nDarwin              8.5\nGoldCoast           NaN\nHobart              6.6\nKatherine           NaN\nLaunceston          NaN\nMelbourne           6.4\nMelbourneAirport    6.4\nMildura             8.5\nMoree               8.9\nMountGambier        6.5\nMountGinini         NaN\nNewcastle           NaN\nNhil                NaN\nNorahHead           NaN\nNorfolkIsland       7.0\nNuriootpa           7.7\nPearceRAAF          8.8\nPenrith             NaN\nPerth               8.8\nPerthAirport        8.8\nPortland            6.5\nRichmond            NaN\nSale                6.7\nSalmonGums          NaN\nSydney              7.2\nSydneyAirport       7.2\nTownsville          8.5\nTuggeranong         NaN\nUluru               NaN\nWaggaWagga          8.2\nWalpole             NaN\nWatsonia            6.4\nWilliamtown         7.2\nWitchcliffe         NaN\nWollongong          NaN\nWoomera             9.0\nName: Sunshine, dtype: float64\n\n\nThe groupby(by='Location') function tells us the grouping variable (location), then we select the variable we want to summarise by location (sunshine), and then we specify the operation (mean).\nThere are multiple locations that return NaN (Not a Number). This indicates that numpy was unable to calculate a mean value for those locations. This is likely to be because all sunshine values for those locations are null.\nWe can check this using pd.DataFrame.count(), which counts the total non-null values (whereas pd.DataFrame.size() counts the total values).\n\n# group by location and count non-null sunshine values\ndf.groupby('Location')['Sunshine'].count()\n\nLocation\nAdelaide            1769\nAlbany              2520\nAlbury                 0\nAliceSprings        2520\nBadgerysCreek          0\nBallarat               0\nBendigo                0\nBrisbane            3144\nCairns              2564\nCanberra            1521\nCobar                550\nCoffsHarbour        1494\nDartmoor            2566\nDarwin              3189\nGoldCoast              0\nHobart              3179\nKatherine              0\nLaunceston             0\nMelbourne           3192\nMelbourneAirport    3008\nMildura             2876\nMoree               2055\nMountGambier        2597\nMountGinini            0\nNewcastle              0\nNhil                   0\nNorahHead              0\nNorfolkIsland       2570\nNuriootpa           2848\nPearceRAAF          3004\nPenrith                0\nPerth               3188\nPerthAirport        3004\nPortland            2566\nRichmond               0\nSale                1818\nSalmonGums             0\nSydney              3328\nSydneyAirport       2993\nTownsville          2617\nTuggeranong            0\nUluru                  0\nWaggaWagga          2575\nWalpole                0\nWatsonia            3008\nWilliamtown         1355\nWitchcliffe            0\nWollongong             0\nWoomera             2007\nName: Sunshine, dtype: int64\n\n\nThe results show that all the locations that return NaN in our group mean calculation have zero non-null values.",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#transforming-data",
    "href": "sessions/03-eda-pandas/index.html#transforming-data",
    "title": "Exploring Data Using Pandas",
    "section": "Transforming Data",
    "text": "Transforming Data\nDatasets are rarely perfectly clean and tidy. We often need to transform the data before we can get the most out of it.\nQuestions:\n\nWhat sort of transformations would help us get the most out of the analysis of the Australian weather data?\n\nThe first step with any analysis is often converting columns to the correct types. With a longitudinal (time-series) dataset,the date column is a good place to start. We can use pd.DataFrame.dtypes to check the data type, either of a single column (using the selector brackets) or all columns in the dataset.\n\nprint(df.dtypes)\n\nDate              object\nLocation          object\nMinTemp          float64\nMaxTemp          float64\nRainfall         float64\nEvaporation      float64\nSunshine         float64\nWindGustDir       object\nWindGustSpeed    float64\nWindDir9am        object\nWindDir3pm        object\nWindSpeed9am     float64\nWindSpeed3pm     float64\nHumidity9am      float64\nHumidity3pm      float64\nPressure9am      float64\nPressure3pm      float64\nCloud9am         float64\nCloud3pm         float64\nTemp9am          float64\nTemp3pm          float64\nRainToday         object\nRainTomorrow      object\ndtype: object\n\n\nAll columns are either stored as object or float64. The object data type is for generic non-numeric data, but from the columns that are stored as objects, we can tell this is mostly categorical variables where the categories are represented as text. The float64 data type refers to data that is numeric and includes decimals (float64 = 64-bit floating point number).\nThe date column is stored as an object, but pandas can store dates as datetime64. We can convert dates using pd.to_datetime(). When transforming data, if we want to keep those transformations, we have to store those changes, using =. In this case, we want to convert the date column but we don’t want to create an entirely new dataframe to handle this change, so we can overwrite the current date column by using the selection brackets to identify the column we want to apply this change to.\n\n# convert date column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\nThe remaining object columns can be converted to categorical, which makes them easier to work with in subsequent analyses. We can use pd.DataFrame.astype() to convert column data types.\n\n# create a list of all object columns\nobject_cols = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\n\n# convert object columns to category\ndf[object_cols] = df[object_cols].astype('category')\n\nA more efficient, though synactically more complex, way of doing this is using lamda functions. We won’t cover lambda functons in this session (they will be discussed in detail in a future session), but below is how we can use them to convert objects to categories.\n\n# convert object columns to category data type\ndf = df.apply(lambda x: x.astype('category') if x.dtype == 'object' else x)\n\nAnother choice we might make is to remove missing values, using pd.DataFrame.dropna() to filter the null values and keep only the non-null values. We can use this to drop all null values across the entire dataset, or we can apply it to a subset of columns, using the subset argument.\n\n# filter observations where sunshine is NA\ndf.dropna(subset='Sunshine')\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n6049\n2009-01-01\nCobar\n17.9\n35.2\n0.0\n12.0\n12.3\nSSW\n48.0\nENE\n...\n20.0\n13.0\n1006.3\n1004.4\n2.0\n5.0\n26.6\n33.4\nNo\nNo\n\n\n6050\n2009-01-02\nCobar\n18.4\n28.9\n0.0\n14.8\n13.0\nS\n37.0\nSSE\n...\n30.0\n8.0\n1012.9\n1012.1\n1.0\n1.0\n20.3\n27.0\nNo\nNo\n\n\n6051\n2009-01-03\nCobar\n15.5\n34.1\n0.0\n12.6\n13.3\nSE\n30.0\nNaN\n...\nNaN\n7.0\nNaN\n1011.6\nNaN\n1.0\nNaN\n32.7\nNo\nNo\n\n\n6052\n2009-01-04\nCobar\n19.4\n37.6\n0.0\n10.8\n10.6\nNNE\n46.0\nNNE\n...\n42.0\n22.0\n1012.3\n1009.2\n1.0\n6.0\n28.7\n34.9\nNo\nNo\n\n\n6053\n2009-01-05\nCobar\n21.9\n38.4\n0.0\n11.4\n12.2\nWNW\n31.0\nWNW\n...\n37.0\n22.0\n1012.7\n1009.1\n1.0\n5.0\n29.1\n35.6\nNo\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n142298\n2017-06-20\nDarwin\n19.3\n33.4\n0.0\n6.0\n11.0\nENE\n35.0\nSE\n...\n63.0\n32.0\n1013.9\n1010.5\n0.0\n1.0\n24.5\n32.3\nNo\nNo\n\n\n142299\n2017-06-21\nDarwin\n21.2\n32.6\n0.0\n7.6\n8.6\nE\n37.0\nSE\n...\n56.0\n28.0\n1014.6\n1011.2\n7.0\n0.0\n24.8\n32.0\nNo\nNo\n\n\n142300\n2017-06-22\nDarwin\n20.7\n32.8\n0.0\n5.6\n11.0\nE\n33.0\nE\n...\n46.0\n23.0\n1015.3\n1011.8\n0.0\n0.0\n24.8\n32.1\nNo\nNo\n\n\n142301\n2017-06-23\nDarwin\n19.5\n31.8\n0.0\n6.2\n10.6\nESE\n26.0\nSE\n...\n62.0\n58.0\n1014.9\n1010.7\n1.0\n1.0\n24.8\n29.2\nNo\nNo\n\n\n142302\n2017-06-24\nDarwin\n20.2\n31.7\n0.0\n5.6\n10.7\nENE\n30.0\nENE\n...\n73.0\n32.0\n1013.9\n1009.7\n6.0\n5.0\n25.4\n31.0\nNo\nNo\n\n\n\n\n75625 rows × 23 columns\n\n\n\nWe haven’t stored this transformation, because filtering nulls without careful consideration is a bad idea, but it’s useful to know, nonetheless.\nThere are lots of ways we could transform the data, but the final example we will consider here is reshaping the data using pd.DataFrame.pivot(), which transforms the data from long to wide format data, and pd.DataFrame.melt(), which transforms it from wide to long format.\nPerhaps we want to focus on the maximum temperature per day in each location in 2015. We can use pd.Series.dt.year to get the year from the date column, and filter for the year 2015, before reshaping the data.\n\ndf2015 = df.loc[df['Date'].dt.year == 2015]\ndf_wide = df2015.pivot(index='Date', columns='Location', values='MaxTemp')\n\ndf_wide.head()\n\n\n\n\n\n\n\nLocation\nAdelaide\nAlbany\nAlbury\nAliceSprings\nBadgerysCreek\nBallarat\nBendigo\nBrisbane\nCairns\nCanberra\n...\nTownsville\nTuggeranong\nUluru\nWaggaWagga\nWalpole\nWatsonia\nWilliamtown\nWitchcliffe\nWollongong\nWoomera\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2015-01-01\n37.0\n21.9\n33.5\n40.3\n34.7\n27.4\n31.2\n31.3\n33.7\n32.6\n...\n32.6\n32.1\n42.0\n35.2\n23.6\n28.3\n33.7\n25.0\n25.3\n39.2\n\n\n2015-01-02\n44.1\n21.2\n39.6\n41.4\n30.5\n38.2\n39.8\n30.5\n33.7\n35.2\n...\n33.0\n34.1\n42.4\n38.9\n21.1\n40.6\n29.3\n23.6\n24.6\n43.3\n\n\n2015-01-03\n38.2\n21.5\n38.3\n36.4\n34.3\n37.5\n40.3\n28.9\n33.6\n34.7\n...\n28.1\n33.7\n39.8\n37.5\n21.8\n39.5\n32.8\n23.0\n25.7\n44.7\n\n\n2015-01-04\n30.5\n23.3\n33.1\n29.0\n34.8\n23.5\n29.0\n30.2\n29.4\n32.5\n...\n31.6\n32.8\n36.1\n33.8\n24.4\n25.1\n34.5\n29.8\n25.3\n37.6\n\n\n2015-01-05\n34.9\n24.9\n35.2\n27.1\n27.2\n26.6\n33.6\n28.1\n31.4\n29.6\n...\n31.6\n28.9\n38.8\n34.9\n29.5\n25.7\n27.0\n31.7\n23.1\n38.3\n\n\n\n\n5 rows × 49 columns\n\n\n\nPerhaps we want to look at the maximum and minimum temperatures in each location, together. We can reshape the data to support this4.\n\ndf_long = df2015.melt(\n    id_vars=['Date', 'Location'],\n    value_vars=['MaxTemp', 'MinTemp'],\n    var_name='Variable',\n    value_name='Value'\n)\n\ndf_long.head()\n\n\n\n\n\n\n\n\nDate\nLocation\nVariable\nValue\n\n\n\n\n0\n2015-01-01\nAlbury\nMaxTemp\n33.5\n\n\n1\n2015-01-02\nAlbury\nMaxTemp\n39.6\n\n\n2\n2015-01-03\nAlbury\nMaxTemp\n38.3\n\n\n3\n2015-01-04\nAlbury\nMaxTemp\n33.1\n\n\n4\n2015-01-05\nAlbury\nMaxTemp\n35.2",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#exercises",
    "href": "sessions/03-eda-pandas/index.html#exercises",
    "title": "Exploring Data Using Pandas",
    "section": "Exercises",
    "text": "Exercises\nSome of these questions are easily answered by scrolling up and finding the answer in the output of the above code, however, the goal is to find the answer using code. No one actually cares what the answer to any of these questions is, it’s the process that matters!\nRemember, if you don’t know the answer, it’s okay to Google it (or speak to others, including me, for help)!\n\n\nImport Data (to Reset)\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')\n\n\n\nWhat is the ‘Sunshine’ column’s data type?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# What is the 'Sunshine' column's data type?\nprint(df['Sunshine'].dtypes)\n\nfloat64\n\n\n\n\n\n\nIdentify all the columns that are of dtype ‘object’.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Identify all the columns that are of dtype 'object'\nprint(list(df.select_dtypes(include=['object'])))\n\n['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\n\n\n\n\n\n\nHow many of the dataframe’s columns are of dtype ‘object’?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# How many of the dataframe's columns are of dtype 'object'?\nlen(list(df.select_dtypes(include=['object'])))\n\n7\n\n\n\n\n\n\nHow many of the ‘Rainfall’ column values are NAs?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# How many of the 'Rainfall' column values are NAs?\nprint(df['Rainfall'].isna().sum())\n\n3261\n\n\n\n\n\n\nCreate a new dataframe which only includes the ‘Date’, ‘Location, ’Sunshine’, ‘Rainfall’, and ‘RainTomorrow’ columns.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nnew_df = df[['Date', 'Location', 'Sunshine', 'Rainfall', 'RainTomorrow']]\nnew_df.head()\n\n\n\n\n\n\n\n\nDate\nLocation\nSunshine\nRainfall\nRainTomorrow\n\n\n\n\n0\n2008-12-01\nAlbury\nNaN\n0.6\nNo\n\n\n1\n2008-12-02\nAlbury\nNaN\n0.0\nNo\n\n\n2\n2008-12-03\nAlbury\nNaN\n0.0\nNo\n\n\n3\n2008-12-04\nAlbury\nNaN\n0.0\nNo\n\n\n4\n2008-12-05\nAlbury\nNaN\n1.0\nNo\n\n\n\n\n\n\n\n\n\n\n\nConvert ‘RainTomorrow’ to a numeric variable, where ‘Yes’ = 1 and ‘No’ = 0.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# df['Location'].astype('category').cat.codes\n# df['RainTomorrow'].astype('category').cat.codes\ndf['RainTomorrow'].map({'Yes': 1, 'No': 0})\n\n0         0.0\n1         0.0\n2         0.0\n3         0.0\n4         0.0\n         ... \n145455    0.0\n145456    0.0\n145457    0.0\n145458    0.0\n145459    NaN\nName: RainTomorrow, Length: 145460, dtype: float64\n\n\n\n\n\n\nWhat is the average amount of rainfall for each location?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# average rainfall by location, sorted by value\ndf.groupby('Location')['Rainfall'].mean().sort_values(ascending=False)\n\nLocation\nCairns              5.742035\nDarwin              5.092452\nCoffsHarbour        5.061497\nGoldCoast           3.769396\nWollongong          3.594903\nWilliamtown         3.591108\nTownsville          3.485592\nNorahHead           3.387299\nSydney              3.324543\nMountGinini         3.292260\nKatherine           3.201090\nNewcastle           3.183892\nBrisbane            3.144891\nNorfolkIsland       3.127665\nSydneyAirport       3.009917\nWalpole             2.906846\nWitchcliffe         2.895664\nPortland            2.530374\nAlbany              2.263859\nBadgerysCreek       2.193101\nPenrith             2.175304\nTuggeranong         2.164043\nDartmoor            2.146567\nRichmond            2.138462\nMountGambier        2.087562\nLaunceston          2.011988\nAlbury              1.914115\nPerth               1.906295\nMelbourne           1.870062\nWatsonia            1.860820\nPerthAirport        1.761648\nCanberra            1.741720\nBallarat            1.740026\nWaggaWagga          1.709946\nPearceRAAF          1.669080\nMoree               1.630203\nBendigo             1.619380\nHobart              1.601819\nAdelaide            1.566354\nSale                1.510167\nMelbourneAirport    1.451977\nNuriootpa           1.390343\nCobar               1.127309\nSalmonGums          1.034382\nMildura             0.945062\nNhil                0.934863\nAliceSprings        0.882850\nUluru               0.784363\nWoomera             0.490405\nName: Rainfall, dtype: float64\n\n\n\n\n\n\nWhat is the average amount of rainfall for days that it will rain tomorrow?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# average rainfall depending on whether it will rain tomorrow or not\ndf.groupby('RainTomorrow')['Rainfall'].mean()\n\nRainTomorrow\nNo     1.270290\nYes    6.142104\nName: Rainfall, dtype: float64\n\n\n\n\n\n\nWhat is the average amount of sunshine in Perth when it will not rain tomorrow?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# average sunshine in Perth when it won't rain tomorrow\ndf.loc[(df['Location'] == 'Perth') & (df['RainTomorrow'] == 'No'), 'Sunshine'].mean()\n# df[(df['Location']=='Perth') & (df['RainTomorrow']=='No')]['Sunshine'].mean()\n\nnp.float64(9.705306603773584)\n\n\n\n\n\n\nWe want to understand the role that time plays in the dataset. Using the original dataframe, carry the following tasks and answer the corresponding questions:\n\nCreate columns representing the year and month from the ‘Date’ column. How many years of data are in the dataset?\nExamine the distribution of the ‘Sunshine’ NAs over time. Is time a component in the ‘Sunshine’ data quality issues?\nCalculate the average rainfall and sunshine by month. How do rainfall and sunshine vary through the year?\nCalculate the average rainfall and sunshine by year. How have rainfall and sunshine changed over time?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# get year and month columns\ndf = (\n    df.assign(Date=pd.to_datetime(df['Date']))\n    .assign(\n        Year=lambda x: x['Date'].dt.year,\n        Month=lambda x: x['Date'].dt.month\n    )\n)\n\n# count unique years\ndf['Year'].nunique()\n\n11\n\n\n\n# lambda function counting nulls by year\ndf.groupby('Year')['Sunshine'].apply(lambda x: x.isna().sum())\n\nYear\n2007        0\n2008      323\n2009     6146\n2010     6220\n2011     6053\n2012     6539\n2013     7570\n2014     9157\n2015     9441\n2016    11994\n2017     6392\nName: Sunshine, dtype: int64\n\n\n\n# rainfall and sunshine by month\ndf.groupby('Month')[['Rainfall', 'Sunshine']].mean().round(1)\n\n\n\n\n\n\n\n\nRainfall\nSunshine\n\n\nMonth\n\n\n\n\n\n\n1\n2.7\n9.2\n\n\n2\n3.2\n8.6\n\n\n3\n2.8\n7.6\n\n\n4\n2.3\n7.1\n\n\n5\n2.0\n6.3\n\n\n6\n2.8\n5.6\n\n\n7\n2.2\n6.1\n\n\n8\n2.0\n7.1\n\n\n9\n1.9\n7.7\n\n\n10\n1.6\n8.5\n\n\n11\n2.3\n8.7\n\n\n12\n2.5\n9.0\n\n\n\n\n\n\n\n\n# rainfall and sunshine by year\ndf.groupby('Year')[['Rainfall', 'Sunshine']].mean().round(1)\n\n\n\n\n\n\n\n\nRainfall\nSunshine\n\n\nYear\n\n\n\n\n\n\n2007\n3.2\n8.1\n\n\n2008\n2.3\n7.8\n\n\n2009\n2.2\n7.9\n\n\n2010\n2.7\n7.3\n\n\n2011\n2.8\n7.3\n\n\n2012\n2.4\n7.6\n\n\n2013\n2.3\n7.7\n\n\n2014\n2.0\n7.8\n\n\n2015\n2.2\n7.7\n\n\n2016\n2.4\n7.6\n\n\n2017\n2.5\n7.7",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#footnotes",
    "href": "sessions/03-eda-pandas/index.html#footnotes",
    "title": "Exploring Data Using Pandas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor more information about pandas data types, check out the pandas documentation on dtypes.↩︎\nFor more information, I’d recommend the pandas documentation, and this pandas tutorial on subsetting data.↩︎\nSome functions should be wrapped in print() in order to return a value that is easy to read, but others won’t. There will be an internal logic for which is which, but it’s not of huge importance to us. You are better off just testing functions out and wrapping them in print() if necessary.↩︎\nThis is often very useful when we need to visualise data, for example plotting the max and min temp for each location, is easier if the values are organised in the same column and differentiated using another column.↩︎",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/06-data-types/slides.html#a-brief-history-of-data-types",
    "href": "sessions/06-data-types/slides.html#a-brief-history-of-data-types",
    "title": "An Introduction To Data Types",
    "section": "A brief history of data types",
    "text": "A brief history of data types\n\n\nAll1 computers store data in binary (1s and 0s) – example shown on the right, represented as hexadecimal\nVariables add a level of convenience and abstraction by letting us name specific buckets to put data in, and data types give structure to these buckets.\nIn the early days of computing data was stored as raw binary\nThe need for specific data types came from the emergence of structured programming from the 1950s onward\nLanguages like FORTRAN and COBOL introduced the segregation of numeric datatypes and character types\nObject-oriented languages like C++ and Java further expanded on this with user-defined data types\nSpecifying the type of data allows the machine to allocate an appropriate amount of memory to it (was very important in the early days of computing, but still relevant)\nAllows us to prevent errors; setting the expectation on the exact type of data that a specific variable will contain.\n\n\n\n\n\nRaw data in hex format (ASCII representation on right).\n\n\n\n\n\nCore rope memory. More on this on the next slide. (Konstantin Lanzet, Wikimedia Commons)"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#history-lesson---core-rope-memory",
    "href": "sessions/06-data-types/slides.html#history-lesson---core-rope-memory",
    "title": "An Introduction To Data Types",
    "section": "History lesson - core rope memory",
    "text": "History lesson - core rope memory\n\nThe Apollo Guidance Computer for the Apollo programme which eventually landed the first person on the moon made use of core rope memory. The program code and fixed data (such as important physical and astronomical constant) were literally woven into a grid of magnetic round cores using a needle, with the sequence the wire took through the cores deciding the pattern of 0s and 1s. This highly technical work was done in bulk in factories by almost exclusively female workers.\n\n\n\nA closeup of a few cores in a core rope memory module, showing the hundreds of times the sense wire goes through each core2.\n\n\n\n\n\n\nA factory employee working on a core rope module3\n\n\n\n\n\nOne of the memory trays of the AGC - each rectangular module contains a self-contained core rope grid4"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#a-quick-note-on-type-systems",
    "href": "sessions/06-data-types/slides.html#a-quick-note-on-type-systems",
    "title": "An Introduction To Data Types",
    "section": "A quick note on type systems",
    "text": "A quick note on type systems\n\nProgramming languages have different philosophies. They are often referred as being “strong” or “weak” and “static” or “dynamic”.\nStrongly but dynamically-typed languages (e.g. Python)\n\nPython features dynamic typing. There is no need to explicitly declare variables as being a specific data type, and it does allow limited implicit conversions, but not as extensively as e.g. JavaScript.\n\nStatically-typed languages (C++, Rust, SQL)\n\nThe programmer has to specify the data type for a variable or object in the code itself and they are checked at compile time. Safer (catches errors early) and possibly more performant, but more tedious and less flexible\n\nWeakly-typed languages (e.g. Javascript)\n\nAllows extensive type coercion; mixing-and-matching of datatypes freely e.g. 5+“2”=“52”\n\n\n\n\n\nhttps://remotescout24.com/en/blog/806-typed-vs-untyped-programming-languages\n\n\n\n\n\nC++. This code generates a type error; we tried to assign a string value to an int\n\n\n\n\n\nJavaScript. This is valid JS code and ends with z being a string with the content “52”"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#overview",
    "href": "sessions/06-data-types/slides.html#overview",
    "title": "An Introduction To Data Types",
    "section": "Overview",
    "text": "Overview\n\n\n\nA logical overview of the basic data types in python. From https://pynative.com/python-data-types/"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#booleans",
    "href": "sessions/06-data-types/slides.html#booleans",
    "title": "An Introduction To Data Types",
    "section": "Booleans",
    "text": "Booleans\n\nLike most programming languages, python has a bool datatype. In some ways, this is the simplest type available. A Boolean can only be True or False, and is returned when evaluating an expression. For example:\nour_result = 10&gt;9 print(our_result)\nReturns True - we’re asking Python for the result of the comparison 10&gt;9, and to store this result in a variable called our_result. The data type of a true-false comparison result like that is bool, so our variable will also be of this type.\nBooleans will become highly relevant when we talk about conditionals and program flow.\n\n\n\n\nGeorge Boole (1815-1864) - the originator of Boolean logic"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#numeric-types",
    "href": "sessions/06-data-types/slides.html#numeric-types",
    "title": "An Introduction To Data Types",
    "section": "Numeric types",
    "text": "Numeric types\nNumeric types are for variables that will only contain numbers. Other programming languages often have many different numeric types, but Python (mercifully) only has two:\nint can contain any5 whole (no fraction or decimals) number; negative, positive or zero. E.g.\n\na = -4\nb = 3\nc = 9087358292578\n\nfloat can contain any number with a decimal point, to arbitrary6 precision. E,g,\n\nx = -2.2\ny = 3.0\nz = 2452.259259999999999\n\nIf you’re manually assigning a number to a variable, python will always choose an int or float depending on whether you’ve used a decimal point or not - so 2 and 2.0 are not equivalent in this context."
  },
  {
    "objectID": "sessions/06-data-types/slides.html#data-structures",
    "href": "sessions/06-data-types/slides.html#data-structures",
    "title": "An Introduction To Data Types",
    "section": "Data structures",
    "text": "Data structures\nWith data structures, we can address an element or elements by using square bracket notation - more on this below.\nStrings (str)\nThese are similar to a VARCHAR in SQL. They are ordered sequences (strings) of characters7. Enclosed by quotation marks8. E.g.\n\nour_string = \"Hello world\"\n\nLists (list)\nAn ordered sequence of objects, where each object can be another data type (int, float, string, bool, etc). Enclosed by square brackets, and the items separated by commas. E.g.\n\nour_list = [1, 2.3, \"abc\"]\n\nDictionaries (dict)\nDictionaries are key-value pairs, where each entry is a pair of entries. Enclosed by curly braces, the keys and values separated by a colon and each pair separated by a comma. E.g.\n\nour_dict = {\"org_code\":\"0DF\",\"name\":\"SCW CSU\",\"year\": 2013}"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#other-data-types",
    "href": "sessions/06-data-types/slides.html#other-data-types",
    "title": "An Introduction To Data Types",
    "section": "Other data types",
    "text": "Other data types\nBuilt-in\n\nWe’ve skipped over complex numbers and tuples, the latter being like a dict but non-changeable.\n\nOther packages\n\nYou may have heard of other data types such as arrays (which are kind like lists but multi-dimensional).\nArrays are not a built-in Python type but are offered by the numpy package.\npandas also offers additional data types such as timestamp (similar to SQL’s datetime).\ndataframes (from pandas) are an example of a higher-order class that makes use of datatypes within it; remember from previous sessions that a dataframe can contain strings, integers, timestamps etc."
  },
  {
    "objectID": "sessions/06-data-types/slides.html#final-thoughts",
    "href": "sessions/06-data-types/slides.html#final-thoughts",
    "title": "An Introduction To Data Types",
    "section": "Final thoughts",
    "text": "Final thoughts\nDon’t worry about memorising any of this! If you take one thing away from this session, make it the fact that data types exist, that being aware of them will help you understand problems with your code, and that resources and documentation are readily available online.\n\nFurther reading\n\nhttps://docs.python.org/3/tutorial/datastructures.html\nhttps://docs.python.org/3/library/stdtypes.html\nhttps://www.geeksforgeeks.org/python-data-types/\nhttps://www.w3schools.com/python/python_datatypes.asp"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#footnotes",
    "href": "sessions/06-data-types/slides.html#footnotes",
    "title": "An Introduction To Data Types",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nexperimental ternary computers and quantum computing are firmly out of scope of this presentation\nfrom https://www.righto.com/2019/07/software-woven-into-wire-core-rope-and.html\nfrom https://www.righto.com/2019/07/software-woven-into-wire-core-rope-and.html\nfrom https://www.righto.com/2019/07/software-woven-into-wire-core-rope-and.html\nthere is no clearly-defined maximum number for an integer in python; certainly not one you’re likely to ever encounter\nagain, limits exist but aren’t relevant here\nletters, numbers, symbols, etc. - any valid UTF-8 symbols\nin most instances either double quotes (\") or single quotes (') are fine - but it’s a good idea to pick one style and be consistent."
  }
]