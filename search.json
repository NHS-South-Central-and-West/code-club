[
  {
    "objectID": "resources/glossary.html",
    "href": "resources/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "This page will have a list of definitions for commonly used (and/or commonly misunderstood) terminology and acronyms relating to python or data analysis and manipulation in general.\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n  Term\n  Definition\n\n\n\n  \n    Dependency\n    A package or library that is required by a Python program (or by another package) in order to function. For example, if your software interacts with a SQL server then it might have sqlalchemy as one of its dependencies.\n  \n  \n    Exploratory Data Analysis (EDA)\n    The process of analysing datasets to summarise their main characteristics, often using visual methods, before formal modeling.\n  \n  \n    Functional programming\n    A programming paradigm that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data.\n  \n  \n    Git\n    A distributed version control system used to track changes in source code during software development.\n  \n  \n    GitHub\n    A cloud-based platform that provides hosting for repositories of files and folders (usually of software code) using git as its backend for version control.\n  \n  \n    Integrated Development Environment (IDE)\n    A software application that provides tools for coding, such as a code editor, debugger, terminal, and build automation, within a single interface. It helps streamline and simplify the software development process. A popular example is VS Code.\n  \n  \n    Jupyter\n    A software package that allows the creation of python notebooks that can include a mixture of markdown-formatted text and live Python code.\n  \n  \n    Markdown (.md)\n    A simple plain-text markup scheme designed to allow for rapid production of formatted text (with headings, links, etc) within a plaintext file. Used by Quarto (as a Quarto-specific flavour with the .qmd extension).\n  \n  \n    matplotlib\n    The baseline Python library for creating static, animated, and interactive visualisations, offering extensive customisation options for plots and charts. Also used as a framework for more advanced or visually pleasing visualisation packages like seaborn.\n  \n  \n    numpy\n    A python library for working with numbers and doing science.\n  \n  \n    Object-oriented programming (OOP)\n    A programming paradigm based on the concept of \"objects,\" which can contain data and code to manipulate that data.\n  \n  \n    pandas\n    A python data analysis and manipulation library for working with dataframes (tabular data).\n  \n  \n    Python\n    A general-purpose programming language.\n  \n  \n    Regression\n    A method for modeling the relationship between one or more explanatory variables and an outcome. It is used to predict outcomes and understand the impact of changes in predictors (explanatory variables) on the response (outcome).\n  \n  \n    Repository (repo)\n    In git and github, a repository is a self-contained \"project\" of files and folders.\n  \n  \n    Reproducible Analytical Pipelines (RAP)\n    A set of processes and tools designed to ensure that data analysis can be consistently repeated and verified by others.\n  \n  \n    seaborn (sns)\n    A Python data visualisation library based on Matplotlib, providing a high-level interface for drawing attractive and informative statistical graphics.\n  \n  \n    skimpy\n    A python library for creating summary statistics from dataframes\n  \n  \n    sqlalchemy\n    A python library for doing SQL queries.\n  \n  \n    TOML\n    Tom's Obvious Minimal Language - simple, human-readable data serialisation format designed for configuration files, emphasizing readability and ease of use. Used by uv to specify its projects.\n  \n  \n    uv\n    A Python package manager, which can manage python projects (folders) and manage the installation and management of the python environment and libraries within that folder.\n  \n  \n    Virtual Environment (venv)\n    An isolated space where you (or an environment manager like uv) can install and manage Python packages in a self-contained way without affecting the system-wide Python setup.\n  \n  \n    YAML\n    Yet Another Markup Language - a human-readable data serialisation format often used for configuration files and data exchange between languages."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Code Club!",
    "section": "",
    "text": "Code Club aims to support everyone at SCW in developing technical and analytical skills through interpretive dance code. We believe these skills are indispensable to the NHS today and in the future, enabling the delivery of high-quality insights through data science and advanced analytics, and the automation of day-to-day tasks with programming. We want to foster an environment that welcomes everybody, sparks ideas, and nurtures collaboration.\nThe Code Club syllabus has been designed to help people with little to no coding experience develop their skills in Python and extend their analytical skills through code. Sessions will be an hour long and held once per fortnight at 2:00 PM on Thursdays. To get an idea of what we will be covering and see if it is right for you, go to the Schedule page. We would love for you to join us!"
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#what-is-a-notebook",
    "href": "sessions/02-jupyter_notebooks/slides.html#what-is-a-notebook",
    "title": "Jupyter Notebooks",
    "section": "What is a notebook",
    "text": "What is a notebook\n\nThe standard for programming in python is the .py file which can hold a block of code which can contain lines of code that allow you to export the results as visualisations or data files.\nJupyter Notebooks have been developed with the data science and analytical community.\nNotebooks are a collection interactive cells which a user can run as a collection or individually, based on the current state of program.\nCells can be denoted as Code, Markdown or Raw Depending on use case.\n\nCode cells use a process called a kernel to run programme elements in the user selected code base (e.g. Python or R).\nMarkdown cells allow the user to include formatted text and other elements (such as links and images).\nRaw cells have no processing attached and output as plain text."
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#a-brief-history-of-jupyter-notebooks",
    "href": "sessions/02-jupyter_notebooks/slides.html#a-brief-history-of-jupyter-notebooks",
    "title": "Jupyter Notebooks",
    "section": "A brief history of Jupyter notebooks",
    "text": "A brief history of Jupyter notebooks\n\nIn 2001, Fernando Perez started development of the iPython project as a way of incorporating prompts and access to previous output, as he continued development he amalgamated iPython with 2 other projects\nIn 2014, Project Jupyter was born out of the initial iPython project. The key aim was to make the project independent of a programming language and allow different code bases to use notebooks. The Name is a reference to the three initial languages: Julia, Python, and R.\nJupyter Notebooks and more recently Jupiter Labs are more than just the notebook, they are interactive development environments launched from the command line.\nJupyter notebooks are used by many online platforms and service providers including: Kaggle, Microsoft Fabric, and the NHS Federated Data Platform."
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#pros-and-cons-of-using-a-notebook",
    "href": "sessions/02-jupyter_notebooks/slides.html#pros-and-cons-of-using-a-notebook",
    "title": "Jupyter Notebooks",
    "section": "Pros and cons of using a notebook",
    "text": "Pros and cons of using a notebook\nOn the plus side…\n\nNotebooks are highly interactive and allow cells to be run in any order.\nYou can re-run each cell separately, so iterative testing is more granular.\nNotebooks can be used to provide a structured report for an end user regardless of coding knowledge.\n\nHaving said that…\n\nIf you are not careful you can save a notebook in a state that cannot run as intended if changes are not checked.\nIt can be harder to understand complex code interactions."
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#the-toolkit",
    "href": "sessions/02-jupyter_notebooks/slides.html#the-toolkit",
    "title": "Jupyter Notebooks",
    "section": "The Toolkit",
    "text": "The Toolkit\n\nYou will need the following pre-installed:\n\nLanguage: Python\nDependency management: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code (or your preferred IDE)\n\nYou can install all these tools by running the following in PowerShell:\n\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop"
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#walkthrough-and-demonstration",
    "href": "sessions/02-jupyter_notebooks/slides.html#walkthrough-and-demonstration",
    "title": "Jupyter Notebooks",
    "section": "Walkthrough and demonstration",
    "text": "Walkthrough and demonstration\nif reviewing these slides this section is only available in the recording, though the initial steps used should be available on the associated Code Club site page"
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#resources",
    "href": "sessions/02-jupyter_notebooks/slides.html#resources",
    "title": "Jupyter Notebooks",
    "section": "Resources",
    "text": "Resources\n\nCheck out the History of iPython\nYou can find out more about Project Jupyter\nThe demonstration makes use of this markdown cheatsheet\nLikewise this is the Jupyter shortcuts Cheat Sheet"
  },
  {
    "objectID": "sessions/04-seaborn-visualisation/index.html",
    "href": "sessions/04-seaborn-visualisation/index.html",
    "title": "Visualisation with Seaborn",
    "section": "",
    "text": "Python has a rich ecosystem of libraries for data visualisation, each with different strengths. Some popular options include matplotlib for fine control over plots, plotly for interactive visualisations, and bokeh for web-ready dashboards. In this session, we’ll be using seaborn. It’s built on top of matplotlib but offers a simpler, high-level interface and nice looking default styles — it’s therefore a good choice when you who want to quickly create clear and informative plots without needing to tweak every detail.\nWe are using Australian weather data, taken from Kaggle. This dataset is used to build machine learning models that predict whether it will rain tomorrow, using data about the weather every day from 2007 to 2017. To download the data, click here.\nOne final note before we get started - This page is a combination of text and python code. We’ve tried to explain clearly what we’re about to do before we do it, but do also note the # comments within the python code cells themselves that occasionally explain a specific line of code in more detail.",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "4. Visualisations with Seaborn"
    ]
  },
  {
    "objectID": "sessions/04-seaborn-visualisation/index.html#initial-setup",
    "href": "sessions/04-seaborn-visualisation/index.html#initial-setup",
    "title": "Visualisation with Seaborn",
    "section": "Initial setup",
    "text": "Initial setup\nWe’re going to import some python packages. Remember that the plt, np, sns aliases are just for convenience - we could omit this completely or use different aliases if we prefer.\n\n\n\n\n\n\nAside - why sns?\n\n\n\nSeaborn being imported as sns is an odd convention (they are the initials of the fictional character the package was named for) that will make it easier to read or copy/paste online examples.\n\n\n\n# install necessary packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# suppress some annoying warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning) \n\nsns.set_theme(style='darkgrid') # https://seaborn.pydata.org/generated/seaborn.set_theme.html\nsns.set_context(\"notebook\") # set an overall scale. Notebook is the default. In increasing size: paper, notebook, talk, poster.\nplt.rcParams['font.sans-serif'] = ['Calibri','Segoe UI','Arial'] # use a nicer font in matplotlib (if available)\n\nAs before, we need to import our dataset. We’re importing the csv file into an initial dataframe called df to start with.\n\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "4. Visualisations with Seaborn"
    ]
  },
  {
    "objectID": "sessions/04-seaborn-visualisation/index.html#data-manipulation",
    "href": "sessions/04-seaborn-visualisation/index.html#data-manipulation",
    "title": "Visualisation with Seaborn",
    "section": "Data manipulation",
    "text": "Data manipulation\n\nColumn conversions\nBefore we start actually generating some visuals, we need to make sure our Date column contains proper datetimes. We’re also going to drop the years with partial data so that our dataset only has full years. Finally we’re going to change the RainTomorrow field to contain a 0 or a 1 rather than yes/no.\n\n# convert date column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# create a column that just contains the year\ndf['Year'] = df['Date'].dt.year\n\n# drop the partial years (2007,2017)\ndf = df[~df['Year'].isin([2007,2017])]\n\n# convert 'RainTomorrow' to a numeric variable, where 'Yes' = 1 and 'No' = 0.\ndf['RainToday']=df['RainToday'].replace({'Yes': 1, 'No': 0, 'NA':0}).fillna(0).astype(int)\ndf['RainTomorrow']=df['RainTomorrow'].map({'Yes': 1, 'No': 0,'NA': 0}).fillna(0).astype(int); \n\n# little tip: the semicolon suppresses textual output when we don't want it\n\n\n\nSort order and other helper tables\nWe need a month order for our “mmm” months - there is probably an official way of doing this…\n\nmonth_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\nWe also need a sort order for our city names to use as a column order for some of our charts later. We’ll just arrange them alphabetically.\n\ncolOrder_top5Cities=['Adelaide','Brisbane','Melbourne','Perth','Sydney']\n\nTo enhance a chart we’re going to build later, we’re going to dynamically calculate some text describing our data range.\n\n# Calculate the date range dynamically; we're going to use this later...\ndate_min = df['Date'].min().strftime('%Y')\ndate_max = df['Date'].max().strftime('%Y')\ndate_range = f\"{date_min} - {date_max}\"\n\nprint(date_range)\n\n2008 - 2016\n\n\n\n\nPivoting and grouping\nNext, we’re going to create some helper dataframes by filtering, grouping and pivoting the data. These will be used for different types of visuals later. Of course, we could have just created these groupings and pivots inline when we do the actual visualisation, but we’re doing it this way because:\n\nIt’s easier to follow\nIt’s tidier (and probably faster) to create these dataframes once as we’re going to be using them multiple times.\n\n\n# build a month column\ndf['Month'] = df['Date'].dt.strftime('%b') # Add a column that just contains the month in mmm format\ndf['Month'] = pd.Categorical(df['Month'], categories=month_order, ordered=True) # Make it categorical using our custom order so that it appears in the right order\n\n# we're going to filter to top 5 cities from now on\ndf_top5Cities = df[df['Location'].isin(['Perth','Adelaide','Sydney','Melbourne','Brisbane'])]\n\n# a dataframe with the number of rainy days per year and month, and location\ndf_top5Cities_rainyDays = df_top5Cities.groupby(['Location','Year', 'Month'])['RainToday'].sum().reset_index()\n\n# finally, we're going to create some grouped and pivoted dataframes. Picture these as PivotTables in Excel.\ndf_top5Cities_Rainfall_grouped = df_top5Cities.groupby(['Location', 'Month'])['Rainfall'].mean().reset_index()\ndf_top5Cities_Rainfall_pivoted = df_top5Cities_Rainfall_grouped.pivot(index=\"Location\",columns=\"Month\", values=\"Rainfall\")\ndf_top5Cities_monthly_rainyDays_pivoted = df_top5Cities.groupby(['Location', 'Month','Year'])['RainToday'].sum().reset_index().groupby(['Location','Month'])['RainToday'].mean().reset_index().pivot(index=\"Location\",columns=\"Month\", values=\"RainToday\")\n\nLet’s use head() to make sure we understand what each grouped/pivoted DF is for.\n\ndf_top5Cities_Rainfall_grouped.head(2)\n\n\n\n\n\n\n\n\nLocation\nMonth\nRainfall\n\n\n\n\n0\nAdelaide\nJan\n0.672199\n\n\n1\nAdelaide\nFeb\n0.973604\n\n\n\n\n\n\n\n\ndf_top5Cities_Rainfall_pivoted.head(2)\n\n\n\n\n\n\n\nMonth\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\nLocation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelaide\n0.672199\n0.973604\n1.171667\n1.461165\n2.293860\n2.447009\n2.873606\n2.399237\n1.809125\n0.906273\n0.730827\n1.192500\n\n\nBrisbane\n6.415574\n5.325389\n4.442276\n3.165385\n3.126446\n2.516318\n1.000000\n1.273381\n1.314498\n2.419424\n3.347761\n4.551613\n\n\n\n\n\n\n\n\ndf_top5Cities_monthly_rainyDays_pivoted.head(2)\n\n\n\n\n\n\n\nMonth\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\nLocation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelaide\n2.555556\n1.555556\n3.666667\n4.000000\n7.666667\n8.555556\n13.444444\n11.444444\n7.888889\n4.222222\n4.000000\n4.555556\n\n\nBrisbane\n8.000000\n7.111111\n10.000000\n5.333333\n5.555556\n6.222222\n4.111111\n3.555556\n4.111111\n5.888889\n6.888889\n8.777778\n\n\n\n\n\n\n\n\n\n\n\n\n\nAside: why df[df[...?\n\n\n\ndf_top5Cities = df[df['Location'].isin(['Perth','Adelaide','Sydney','Melbourne','Brisbane'])]\n\nThe first (outer) df[ tells pandas that we want to select a subset of rows based on some condition.\nThe second (inner) df[ is going to tell pandas this condition. In this case, we’re using isin to return a dataframe that contains a series of True and False rows corresponding to whether the rows in our original dataframe had the Location column as one of our 5 cities.\nThe final dataframe is then a filtered copy where the inner condition is True.\n\nYes, there are other ways of doing this! For example by using .query() to specify our conditions.",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "4. Visualisations with Seaborn"
    ]
  },
  {
    "objectID": "sessions/04-seaborn-visualisation/index.html#doing-some-actual-plotting",
    "href": "sessions/04-seaborn-visualisation/index.html#doing-some-actual-plotting",
    "title": "Visualisation with Seaborn",
    "section": "Doing some actual plotting",
    "text": "Doing some actual plotting\nThe Seaborn home page has a very good introductory tutorial, reference documentation, and a nice collection of examples. You should familiarise yourself with the documentation; it’ll pay off massively if you actually grasp what each function and argument is for, rather than just copy/pasting examples and tweaking them until they work (without really understanding what they’re doing).\n\nA basic one-line line chart\n\nsns.lineplot(\n  data=df_top5Cities_Rainfall_pivoted.T #.T gives the transpose (flips rows and columns)\n  ) \n\n\n\n\n\n\n\n\n\n\nJust a little bit of customisation - a bar chart\nhttps://seaborn.pydata.org/generated/seaborn.barplot.html\n\nour_barplot = sns.barplot(\n  data=df_top5Cities_Rainfall_grouped \n  ,x=\"Month\"\n  ,y=\"Rainfall\"\n  ,hue=\"Location\" # read \"hue\" as \"series\"\n  ,palette=\"tab10\" # https://matplotlib.org/stable/users/explain/colors/colormaps.html\n  )\n\nour_barplot.set(title='Average daily rainfall by month and city',ylim=(0,8))\n\nsns.move_legend(our_barplot,\"upper left\", title=None, ncol=4) # https://seaborn.pydata.org/generated/seaborn.move_legend.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAside - why did we need to set the title after the sns.barplot call?\n\n\n\nThe barplot function provided by Seaborn doesn’t actually allow setting of a title - it just generates a plot (including its axes) and returns this as a matplotlib Axes object (recall we mentioned earlier that Seaborn is a layer on top of the matplotlib library). By using the .set(...) method on our barplot object, we can modify this returned object to give it a title. We also could have used this to customise our axis labels (the defaults are fine here), set axis limits, or things like tick labels.\n\n\n\n\nHeatmaps\nThe Seaborn heatmap function will easily let us create a two-dimensional heatmap visual with a specific colour theme and custom number formatting.\n\n# We need to use some matplotlib code to set our output size, add a title, and capitalise our x-axis label\nf,ax = plt.subplots(figsize=(10, 5)) # matplotlib subplots are a common way of setting a figure layout\nax.set_title(f\"Average daily rainfall (/mm) each month ({date_range}) for Australia's top 5 cities\", fontsize=16, fontweight=\"bold\", pad=10) # using our previously set date_range variable\n\nsns.heatmap(df_top5Cities_Rainfall_pivoted # Heatmap expects rectangular (pivot-like) data\n            ,annot=True # Put numbers inside the cells\n            ,fmt=\".1f\" # Make the numbers have 1 decimal place\n            ,square=True # Square vs rectangular cells\n            ,cbar=False # Get rid of the colourbar legend on the side\n            ,cmap=\"Blues\" # Seems appropriate for rainfall. Colourmaps reference: https://matplotlib.org/stable/users/explain/colors/colormaps.html \n            ,ax=ax # Tell it to use the matplotlib axes we created earlier\n           )\n\n\n\n\n\n\n\n\n\n\nAnother heatmap with some further tweaks\nWe can make our heatmap look just a little better by apply some tweaks to the subplots object.\n\n# Again setting up matplotlib subplots so that we can make some changes later\nf,ax = plt.subplots(figsize=(10, 5)) \n\nsns.heatmap(df_top5Cities_monthly_rainyDays_pivoted # Heatmap expects rectangular (pivot-like) data\n            ,annot=True # Put numbers inside the cells\n            ,fmt=\".0f\" # Force the number format\n            ,square=True # Square vs rectangular cells\n            ,cbar=False # Get rid of the colourbar legend on the side\n            ,cmap=\"crest\" # Colourmaps reference: https://matplotlib.org/stable/users/explain/colors/colormaps.html \n            ,ax=ax # Tell it to use the matplotlib axes we created earlier\n           )\n\n# We need to use some matplotlib code to set our output size, add a title, and capitalise our x-axis label\nax.tick_params(axis='x', labelsize=11, rotation=45) # I think 45-degree month labels look nicer, but this is a matter of taste.\nax.tick_params(axis='y', labelsize=11)\n\n# Manually changing our axis labels for more control\nax.set_xlabel(\"Month\",fontweight=\"bold\",fontsize=12) \nax.set_ylabel(\"City\",fontweight=\"bold\",fontsize=12)\n\n# Set our title dynamically\nax.set_title(f\"Mean number of rainy days by month between {date_min} and {date_max} for Australia's top 5 cities\", fontsize=16, fontweight=\"bold\", pad=15);\n\n\n\n\n\n\n\n\n\n\nA fancy multi-chart visual\nThis chart uses the boxgrid object to arrange multiple different subcharts. We’re actually generating two sets of different visuals (linegrid and boxgrid) in one output. If you’re not sure what the for [...] in [...] syntax means, don’t worry - this will be covered in a future session.\n\n# Setting up the grid of box plots\n# Box plots are a bit of a rabbit hole and are extremely customisable; we're mostly using defaults here\nboxgrid = sns.FacetGrid(df_top5Cities \n                        ,col=\"Location\" # Defining the different facets\n                        ,col_wrap=5, height=4.5 # Layout and sizing for our facet grid\n                        ,col_order=colOrder_top5Cities  # Using our alphabetical order of city names to arrange our facets\n)\nboxgrid.map(sns.boxplot # This is what tells sns what sort of plots we want in our grid\n            ,\"Month\" # X\n            ,\"MaxTemp\" # Y\n            ,linewidth=1.5\n            ,color=\"skyblue\"\n            ,order=month_order\n            ,fliersize=0 # Seaborn boxplots by default include markers for outliers, which it calls \"fliers\". For this chart we'd like to disable these.\n            ) \n\n# Setting up the grid of line plots\n# Using the default matplotlib plot here\nlinegrid = sns.FacetGrid(df_top5Cities.groupby(['Location', 'Month'])['Rainfall'].mean().reset_index()\n                         ,col=\"Location\" \n                         ,hue=\"Location\",palette=\"Set2\" \n                         ,col_wrap=5, height=4.5 \n                         ,col_order=colOrder_top5Cities\n)\nlinegrid.map(plt.plot, \"Month\", \"Rainfall\",marker=\"o\")\n\n# Formatting axes\nfor ax in boxgrid.axes.flat:\n    ax.tick_params(axis='x', labelsize=9, rotation=45)\n    ax.tick_params(axis='y', labelsize=9)\nfor ax in linegrid.axes.flat:\n    ax.tick_params(axis='x', labelsize=9, rotation=45)\n    ax.tick_params(axis='y', labelsize=9)\n\nlinegrid.set_titles(col_template=\"{col_name}\",fontweight=\"bold\",fontsize=16)\nlinegrid.set_axis_labels(\"Month\",\"Average rainfall /mm\",fontweight=\"bold\",fontsize=10)\nboxgrid.set_titles(col_template=\"{col_name}\",fontweight=\"bold\",fontsize=16)\nboxgrid.set_axis_labels(\"Month\",\"Max temp /$\\\\degree$C\",fontweight=\"bold\",fontsize=10)\n\n# Setting overall titles and spacing\nlinegrid.fig.suptitle(f\"Mean daily rainfall by month for top 5 cities, {date_range}\", fontsize=16, color='black',fontweight='bold') \nboxgrid.fig.suptitle(f\"Max temperature by month for top 5 cities, {date_range}\", fontsize=16, color='black',fontweight='bold') \nlinegrid.fig.subplots_adjust(top=0.85)  \nboxgrid.fig.subplots_adjust(top=0.85);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Small multiples”-type time-series grid\nThis is mostly based on the example given at https://seaborn.pydata.org/examples/timeseries_facets.html.\nFirstly, for this one we need another dataframe that just contains the Sydney data.\n\ndf_Sydney = df[df['Location']==\"Sydney\"].groupby(['Month', 'Location','Year'], as_index=False)['MaxTemp'].max()\n\nWe’re going to use the relplot function to create a grid of plots with a specific set of variables across its rows and columns. For each cell we’re plotting that year’s data with a different colour, and plotting all the other years in grey in the background.\n\n# plot each year's time series in its own facet\ng = sns.relplot(data=df_Sydney\n                ,x=\"Month\"\n                ,y=\"MaxTemp\"\n                ,col=\"Year\"\n                ,hue=\"Year\"\n                ,kind=\"line\"\n                ,palette=\"viridis\"\n                ,linewidth=4\n                ,zorder=5\n                ,col_wrap=3, height=2, aspect=1.5, legend=False,\n)\n\n# iterate over each subplot to customize further\nfor year, ax in g.axes_dict.items():\n\n    # Add the title as an annotation within the plot\n    ax.text(.8\n            ,.85\n            ,year\n            ,transform=ax.transAxes\n            ,fontweight=\"bold\",fontsize=9)\n\n    # Plot every year's time series in the background\n    sns.lineplot(\n        data=df_Sydney, x=\"Month\", y=\"MaxTemp\", units=\"Year\",\n        estimator=None, color=\".7\", linewidth=1, ax=ax\n    )\n\n# reduce the frequency of the x axis ticks\nax.set_xticks(ax.get_xticks()[::2])\n\n# tweak the supporting aspects of the plot\ng.set_titles(\"\")\ng.fig.suptitle(f\"Max temperature by month in Sydney, {date_range}\", fontsize=16, color='black',fontweight='bold') \ng.set_axis_labels(\"\", \"Max Temp /$\\\\degree \\\\mathrm{C}$\");\ng.tight_layout();",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "4. Visualisations with Seaborn"
    ]
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#learning-objectives",
    "href": "sessions/09-object-oriented-programming/slides.html#learning-objectives",
    "title": "Introduction to Object-Oriented Programming",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand core concepts of object-oriented programming\nUnderstand the benefits of object-oriented programming\nLearn how to create your own object classes (see the accompanying notebook)"
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#object-oriented-programming-what-is-this-tech-geekery-and-why-should-i-care",
    "href": "sessions/09-object-oriented-programming/slides.html#object-oriented-programming-what-is-this-tech-geekery-and-why-should-i-care",
    "title": "Introduction to Object-Oriented Programming",
    "section": "Object-Oriented Programming? What is this tech-geekery and why should I care?",
    "text": "Object-Oriented Programming? What is this tech-geekery and why should I care?\n\nPython is an object-oriented language. Every entity is treated as an object; even single integers are objects of the “int” class.\nAn understanding of object-oriented programming will help give you a better understanding of how the packages you use function.\nYou can use this understanding to create your own programs that harness the strengths of objected-oriented programming:\n\nConvenience\nFlexibility\nExtensibility\nSimpler interfacing"
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#history",
    "href": "sessions/09-object-oriented-programming/slides.html#history",
    "title": "Introduction to Object-Oriented Programming",
    "section": "History",
    "text": "History\n\n\nIn the early days of programming, variables could only be the “primitive” data types containing a single value\n\nInteger, Float, Boolean, Char\n\nLater came Structures (“Structs”), which can contain multiple values of different types.\nStructs were the precursor to objects, but they couldn’t yet contain associated functions within them.\nObjects first appear in the Simula programming language in the 1960s for modelling physical phenomena.\nThose objects influenced Alan Kay, who coined the term “object-oriented programming” to describe architecture where objects pass information to one another.\n\n\n\n\n\nMy. Kay looking very pleased with his coinage."
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#classes-and-objects",
    "href": "sessions/09-object-oriented-programming/slides.html#classes-and-objects",
    "title": "Introduction to Object-Oriented Programming",
    "section": "Classes and Objects",
    "text": "Classes and Objects\n\n\nClasses act as templates for objects\nObjects are referred to as “instances” of classes\n\nWe talk of objects being “instantiated” from a class\nThink of each object as being a copy created using the class template\n\nObjects represent entities with their own data (attributes) and behaviours (methods)\nWe can create lots of instances of an object with their own attribute values and call methods on them separately yet consistently\nObjects are self-contained units that can interact with objects both of the same and of other classes\n\n\n\n\n\nConsistency is the key."
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#inheritance",
    "href": "sessions/09-object-oriented-programming/slides.html#inheritance",
    "title": "Introduction to Object-Oriented Programming",
    "section": "Inheritance",
    "text": "Inheritance\n\n\nChild classes inherit attributes and methods from parent classes\nChild classes can modify / override and add to what they have inherited\nReduces code duplication; increases re-usability\nImproves extensibility: i.e. new classes with the same core behaviours, but new features, can be based on existing classes\n\n\n\n\n\nInheritance is a much less contentious issue in Python."
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#encapsulation-for-convenience",
    "href": "sessions/09-object-oriented-programming/slides.html#encapsulation-for-convenience",
    "title": "Introduction to Object-Oriented Programming",
    "section": "Encapsulation for convenience",
    "text": "Encapsulation for convenience\n\nBundling data (attributes) with functions (methods)\nMethods are tailor-made to work with the data contained in the object\nSaves on having to pass data between multiple functions, which is particularly useful in machine learning models\nPandas DataFrames demonstrate encapsulation. They contain data, but also have methods associated with them\n\ndf = pd.DataFrame(data) &lt;– Instantiating a dataframe object\ndf.head(), df.describe(), df.drop() &lt;– calling methods"
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#polymorphism-for-flexibility",
    "href": "sessions/09-object-oriented-programming/slides.html#polymorphism-for-flexibility",
    "title": "Introduction to Object-Oriented Programming",
    "section": "Polymorphism for flexibility",
    "text": "Polymorphism for flexibility\n\nObjects of different types can be treated in the same way, even if the behaviour differs\n\nWith Pandas DataFrames, .head() will work on both a DataFrame and a Series1\n\n“Duck typing”: If the behaviour of a thing matches that of another thing, they are considered the same. In OOP terms, the presence of certain methods is more important than which class an object comes from2\n\nThe scikit-learn library’s allows the same code to work for different models\n\n\nSeries.head() with return the first few values, while DataFrame.head() returns the first few rows of all columnsThe concept of “duck typing”, found in Python and other languages, comes from the phrase “if it walks like a duck, quacks like a duck and swims like a duck, then it’s a duck.”"
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#abstraction-for-simpler-interfacing",
    "href": "sessions/09-object-oriented-programming/slides.html#abstraction-for-simpler-interfacing",
    "title": "Introduction to Object-Oriented Programming",
    "section": "Abstraction for simpler interfacing",
    "text": "Abstraction for simpler interfacing\n\nSeparating the implementation code from the functionality that users (i.e. other programmers) interact with\nCreates a simple interface for parts of a program pass information between each other\nExamples:\n\nWhen working with machine learning models, users only need to apply simple methods to train the model and make a prediction.\nEntities interacting with each other within a simulation model."
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#when-to-use-oop",
    "href": "sessions/09-object-oriented-programming/slides.html#when-to-use-oop",
    "title": "Introduction to Object-Oriented Programming",
    "section": "When to use OOP",
    "text": "When to use OOP\n\nWhen you want to easily re-use code, to avoid repetition and to extend functionality\nDiscrete Event Simulations for modelling queueing / capacity problems\nCreating custom, branded visualisation packages, for example an NHS-branded SPC chart\n\nCreating a package that can be used to import the latest data from a website without users having to understand API calls or the website’s structure\n\nWhen you want to model real-world entities\n\nDiscrete Event Simulations for modelling queueing / capacity problems\n\nWhen you want to make code modular and easy for others to work with\nWhen you want to simplify end-users’ interaction with Python, fostering a self-service approach to analytics\n\nCreating custom, branded visualisation packages, for example an NHS-branded SPC chart\n\nLess appropriate for: When you want to be certain of the state of your data at each step of a process, for example when cleansing data"
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#resources",
    "href": "sessions/09-object-oriented-programming/slides.html#resources",
    "title": "Introduction to Object-Oriented Programming",
    "section": "Resources",
    "text": "Resources\nRealPython: Object-Oriented Programming (OOP) in Python\n\nOOP produces code that is easy to read, extend and maintain\n\nHSMA’s Guide to Object-Oriented Programming\n\nHSMA’s Discrete Event Simulation Module"
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#things-that-flow",
    "href": "sessions/07-control-flow/slides.html#things-that-flow",
    "title": "An Introduction to Control Flow",
    "section": "Things That Flow",
    "text": "Things That Flow\n\n\n\n\nA stream flowing towards the sea.\n\n\n\n\n\n\nTraffic flow on a busy road.\n\n\n\n\n\n\nConversation flowing between people."
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#code-flows-too",
    "href": "sessions/07-control-flow/slides.html#code-flows-too",
    "title": "An Introduction to Control Flow",
    "section": "Code Flows Too",
    "text": "Code Flows Too\n\nThe flow of code and the steps that impact that flow are called “control flow”.\nIn general code flows in the way same as a book (with some exceptions).\n\nEach function is completed before considering the next.\n\nFunctions can be nested in other functions - the inner most function is completed first.\nControl structures can be used to change the course of a program.\nRepetition structures can be used to repeat a section of code."
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#sequential-structure",
    "href": "sessions/07-control-flow/slides.html#sequential-structure",
    "title": "An Introduction to Control Flow",
    "section": "Sequential Structure",
    "text": "Sequential Structure\n\nIn general code flows like a book reads:\n\nStatements (like lines of code) run top to bottom line,\nLeft to right in statement,\nEach statement being completed before moving to the next.\n\n\n\nv = \"2\"         #stores \"2\" as a string in 'v'\ni = int(v)      #stores v as an integer 'i'\nt = type(i)     #stores the type of 'i' as 't'\nprint(t)        #prints 't'"
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#nesting-functions-and-operations",
    "href": "sessions/07-control-flow/slides.html#nesting-functions-and-operations",
    "title": "An Introduction to Control Flow",
    "section": "Nesting Functions and Operations",
    "text": "Nesting Functions and Operations\n\nWe are not limited to a single function or operation per row.\nThe previous example could be re-written as:\n\nprint(type(int(\"2\")))\n\nNesting functions can be useful, however care should be taken and it may be easier to separate functions over multiple rows."
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#control-structures",
    "href": "sessions/07-control-flow/slides.html#control-structures",
    "title": "An Introduction to Control Flow",
    "section": "Control Structures",
    "text": "Control Structures\n\nControl structures (also known as decision structures) allow the flow to respond to varying situations.\nA decision is made based on one or more conditions.\nThese control structures are very similar to the IF function in Excel and the CASE statement in SQL (but remember that indentation matters in Python).\n\n\n\n\nPython - Control\nSQL - CASE\nExcel - IF\n\n\n\n\nif x = 2:\nCASE WHEN x &gt; 2\nIF(x &gt; 2,\n\n\ny = 1\nTHEN 1\n1,\n\n\nelse:\nELSE\n\n\n\ny = 0\n0 END\n0)"
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#the-importance-of-being-boolean",
    "href": "sessions/07-control-flow/slides.html#the-importance-of-being-boolean",
    "title": "An Introduction to Control Flow",
    "section": "The Importance of Being Boole(an)",
    "text": "The Importance of Being Boole(an)\n\n\n\n\nGeorge Boole was an English Mathmetician and logician whose work on binary logic has resulted in binary conditions bearing his name\nAny statement that can be evaluated as only either True (1) or False (0) is Boolean."
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#repetition-structures",
    "href": "sessions/07-control-flow/slides.html#repetition-structures",
    "title": "An Introduction to Control Flow",
    "section": "Repetition Structures",
    "text": "Repetition Structures\n\nRepetition structures (commonly referred to as “loops”) allow for us to recycle chunks of code to perform the same or similar operation a specified number of times or until a condition changes.\nFor loops cycle through a series of iterations, until they reach the end performing each series of statements for each iteration.\n\nThis can be used to cycle through a list, dictionary or other iterable as well as working through ranges of numbers\n\nWhile loops continue until a predetermined condition changes from True to False.\n\nThis can be useful for testing conditions but comes with a warning:\n\nMake sure your condition will change at some point or else your loop cannot end."
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#what-is-iterability",
    "href": "sessions/07-control-flow/slides.html#what-is-iterability",
    "title": "An Introduction to Control Flow",
    "section": "What is Iterability?",
    "text": "What is Iterability?\n\nIterability is the ability to split an object into discrete items. The item may be ordered or unordered, each item will be extracted, processed ad set aside.\nIn general if an object can be split into multiple items it can be iterated (integers and floats are not iterable).\nIterable objects include:\n\nStrings (the word “strings” contains 7 iterable items).\nLists eg [1, 2, 3, 4, 4, 4, 5, 6]\nTuples eg (12, ‘Red’, ‘Apples’)\nSets eg {1, 2, 3, 4, 5, 6}\nDict eg {ICB_Code: ‘QSL’, Metric_Code: E.M.10}"
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#sequential-structure-example",
    "href": "sessions/07-control-flow/slides.html#sequential-structure-example",
    "title": "An Introduction to Control Flow",
    "section": "Sequential Structure Example",
    "text": "Sequential Structure Example\n\nThe following sequential code will create a variable called ‘var’ which is a string, it converts this string to an integer and conducts a series of mathematical operators before printing the result:\n\n\nvar = '22' # set \"var\" to '22'\nvar = int(var) # convert \"var\" to int\nvar = var  / 2 - 3 # apply math operators\nprint (var) # print result"
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#control-structure-example",
    "href": "sessions/07-control-flow/slides.html#control-structure-example",
    "title": "An Introduction to Control Flow",
    "section": "Control Structure Example",
    "text": "Control Structure Example\n\nThis code checks if a variable called ‘provider’ in this list is equal to a selection of values and prints out an associated string.\n\n\nif provider == ‘ryr’:\n  print(‘SUSSEX’)\nelif provider == ‘rhu’:\n  print(‘HIOW’)\nelif provider == ‘rxq’:\n  print(‘BOB’)\nelse:\n  print(‘Unknown’)"
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#control-structure-example-1",
    "href": "sessions/07-control-flow/slides.html#control-structure-example-1",
    "title": "An Introduction to Control Flow",
    "section": "Control Structure Example",
    "text": "Control Structure Example\nFor comparison, this is the equivalent SQL CASE statement.\nCASE\n  WHEN provider = 'ryr'\n    THEN 'Sussex'\n  WHEN provider = 'rhu'\n    THEN 'HIOW'\n  WHEN provider = 'rhu'\n    THEN 'BOB'\n  ELSE 'Unknown'\n  END"
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#watch-out-for-unintended-consequences",
    "href": "sessions/07-control-flow/slides.html#watch-out-for-unintended-consequences",
    "title": "An Introduction to Control Flow",
    "section": "Watch Out for Unintended Consequences",
    "text": "Watch Out for Unintended Consequences\n\n\nNot taking care over your coding can cause big issues. Consider the corner cases and unintended consequences?\nEmpty variables, incorrect data types, and misunderstood flow in the structure can affect your program.\nClose the loop! Make sure you know how your loops are being switched off and that it’s possible.\nA cautionary tale: The Virtual Plague That Nearly Wiped Out The World of Warcraft\n\n\n\n\n\nThe “Corrupted Blood” Incident - a fairly famous coding error."
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#final-thoughts",
    "href": "sessions/07-control-flow/slides.html#final-thoughts",
    "title": "An Introduction to Control Flow",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n\nDon’t worry about memorising any of this!\nThe aim of this session is to give a basic understanding of the logic needed to implement control flow in your program."
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#further-reading",
    "href": "sessions/07-control-flow/slides.html#further-reading",
    "title": "An Introduction to Control Flow",
    "section": "Further Reading",
    "text": "Further Reading\n\n\nPython Tutorial - Control Flow\nGeeks for Geeks - Control Structures\nW3 Schools - if/elif/else Logic\nList Comprehension in Python"
  },
  {
    "objectID": "sessions/06-data-types/index.html",
    "href": "sessions/06-data-types/index.html",
    "title": "An Introduction to Data Types",
    "section": "",
    "text": "This session is the first in a series of programming fundamentals. We recognise that this content might be a bit more dry and abstract, but it is important background to know when you start to actually use Python in your day to day work.\nIf you’ve used Excel and changed the data format for a cell, you’ve already come across data types! It is important to understand how Python stores values in variables and the pitfalls, gotchas and errors you may come across when working with data. The slide deck below gives a (little) bit of history before giving an overview of how data types work in Python. On the last slides are some links to useful resources on the web, which you may want to make note of for the future. Below the slides is a live notebook that demonstrates this, with some exercises at the end to check your understanding.",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#automatically",
    "href": "sessions/06-data-types/index.html#automatically",
    "title": "An Introduction to Data Types",
    "section": "Automatically",
    "text": "Automatically\nPython automatically assigns a type to a variable based on the value we put into it when we use the = assignment operator.\n\nour_integer = 1\nour_float = 2.2\nour_integer_turned_into_a_float = float(our_integer)\nour_string=\"Hello SCW!\"",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#manually",
    "href": "sessions/06-data-types/index.html#manually",
    "title": "An Introduction to Data Types",
    "section": "Manually",
    "text": "Manually\nIf we need to, we can use a constructor function named after the data type, like int() or str() to force a variable to be the specific type we need it to be.\n\na = str(\"123\") # a will contain the string 123 rather than the numeric value\nb = float(2) # b will contain the decimal value 2.0\nc = int(1.9) # just throws away the .9; not rounded!",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#finding-out-what-type-a-variable-is",
    "href": "sessions/06-data-types/index.html#finding-out-what-type-a-variable-is",
    "title": "An Introduction to Data Types",
    "section": "Finding out what type a variable is",
    "text": "Finding out what type a variable is\n\nprint (type(a)) # output: &lt;class 'str'&gt;\nprint (type(b)) # output: &lt;class 'float'&gt;\nprint (type(c)) # output: &lt;class 'int'&gt;",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#booleans",
    "href": "sessions/06-data-types/index.html#booleans",
    "title": "An Introduction to Data Types",
    "section": "Booleans",
    "text": "Booleans\nBools are often an intermediate - they are an output of evaluations like 1 == 2. Booleans may sound very basic, but they are crucial in understanding control flow, which we’ll be covering in a future session!\n\nz = True            # you'll rarely ever assign a boolean directly like this, but do note they are\n                    # case sensitive; z = true wouldn't have worked here.\nprint(type(z))      # output: &lt;class 'bool'&gt;\nprint (10&gt;9)        # output: True\nprint (1 == 2)      # output: False\n \nprint(bool(123))    # output: True\nprint(bool(\"abc\"))  # output: True\nprint(bool(None))   # output: False\nprint(bool(0))      # output: False",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#numeric-types",
    "href": "sessions/06-data-types/index.html#numeric-types",
    "title": "An Introduction to Data Types",
    "section": "Numeric types",
    "text": "Numeric types\nPython supports different kinds of numbers, including integers (int), floating point numbers (float). You can do basic arithmetic (+, -, *, /), exponentiation (**), and use built-in functions like round(), abs(), and pow().\n\na = 10              # int\nb = 3               # int\nc = 2.5             # float\nd = -2              # int\n\nprint(a+b)          # output: 13, an int\nprint(a+c)          # output: 12.5, a float\nprint(a ** (1/2))   # taking the square root of an int returns a float\n \nprint(float(a))     # output: 10.0\nprint(int(2.88))    # output: 2; just throws away the decimal part\n \nprint(round(2.88))  # output: 3\nprint(round(2.88,1))# output: 2.9",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#strings",
    "href": "sessions/06-data-types/index.html#strings",
    "title": "An Introduction to Data Types",
    "section": "Strings",
    "text": "Strings\nStrings are sequences of characters enclosed in quotes. They support indexing, slicing, and a range of methods like .lower(), .replace(), .split(), and .join().\n\nstr_a = \"Hello\"              # string\nstr_b = \"SCW!\"               # string\n\nstr_ab = str_a + \" \" + str_b # python repurposes the \"+\" to mean string concatenation as well as addition\nprint(str_ab)                # output: Hello SCW!\n \nprint(str_ab.find(\"SCW\"))    # output:6 (the location in the string of the substring \"SCW\". Starts from 0!)\n \nstr_repeated = str_ab * 3 \nprint(str_repeated)          # output: Hello SCW!Hello SCW!Hello SCW!\n \nprint(len(str_a))            # output: 5\nprint(str_a[0])              # output: H\nprint(str_a[0:3])            # output: Hel (give me 3 characters starting at 0)\nprint(str_a[3:])             # output: lo (give me everything starting at 3)\nprint(str_a[:5])             # output: Hello (give me the first 5 characters)",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#lists",
    "href": "sessions/06-data-types/index.html#lists",
    "title": "An Introduction to Data Types",
    "section": "Lists",
    "text": "Lists\nLists are ordered, mutable (changeable) collections. They can hold any type of data and support operations like appending (.append()), removing (.remove()), and slicing (our_list[1:4]).\n\nfruits = [\"banana\", \"lychee\", \"raspberry\", \"apple\"]\nprint(fruits[0])          # output: banana (string)\nprint(fruits[0:2])        # output: ['banana','lychee'] (list!)\nprint(fruits[-1])         # output: apple (string)\n \nfruits.append(\"orange\") \nprint(fruits)             # output: ['banana', 'lychee', 'raspberry', 'apple', 'orange']\n \nprint(\"orange\" in fruits) # output: True\nprint(\"tomato\" in fruits) # output: False\n \nfruits.sort() \nprint(fruits)             # output: ['apple', 'banana', 'lychee', 'orange', 'raspberry']\n\nLists can contain any combination of other data types.\n\nmixed_list = [\"blue\", \"green\", False, 2, 2.55]\nfor item in mixed_list: # we're using a loop here; don't worry if you don't recognise this syntax\n    print(type(item))   # output:&lt;class 'str'&gt; &lt;class 'str'&gt; &lt;class 'bool'&gt; &lt;class 'int'&gt; &lt;class 'float'&gt;",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#dicts",
    "href": "sessions/06-data-types/index.html#dicts",
    "title": "An Introduction to Data Types",
    "section": "Dicts",
    "text": "Dicts\nDictionaries store key-value pairs and are optimized for lookups. Keys must be unique and are immutable, but values are mutable. You can add, update, or delete items using dict[key] = value, dict.get(key), or del dict[key].\n\nSCW_basic_info={\n    \"org_code\"      : \"0DF\",\n    \"short_name\"    : \"SCW CSU\",\n    \"long_name\"     : \"NHS South, Central and West Commissioning Support Unit\",\n    \"year_opened\"   : 2014,\n    \"active\"        : True,\n    \"postcode\"      : \"SO50 5PB\"\n}\n\nprint(type(SCW_basic_info[\"active\"]))       # output: &lt;class 'bool'&gt;\nprint(type(SCW_basic_info[\"year_opened\"]))  # output: &lt;class 'int'&gt;\n \nprint(SCW_basic_info[\"org_code\"])           # output: \"0DF\"\nprint(len(SCW_basic_info))                  # output: 6\n \nSCW_basic_info[\"number_of_staff\"] = 1000    # we can easily add a new key and value at the same time\n \nprint(len(SCW_basic_info))                  # output: 7\n \nSCW_basic_info[\"number_of_staff\"] += 1      # we hired a new member of staff\nprint(SCW_basic_info[\"number_of_staff\"])    # output: 1001",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/08-functions/index.html",
    "href": "sessions/08-functions/index.html",
    "title": "Functions & Functional Programming",
    "section": "",
    "text": "This session is the third in a series of programming fundamentals. The concepts here can feel abstract at first, but they are a big part of how Python code is structured in real projects. By the end, you’ll see how functions make code shorter, cleaner, and easier to re-use.\nThe below slides aim to provide an introduction to these concepts and the way we can use them.",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "8. Functions & Functional Programming"
    ]
  },
  {
    "objectID": "sessions/08-functions/index.html#slides",
    "href": "sessions/08-functions/index.html#slides",
    "title": "Functions & Functional Programming",
    "section": "Slides",
    "text": "Slides\nUse the left ⬅️ and right ➡️ arrow keys to navigate through the slides below. To view in a separate tab/window, follow this link.",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "8. Functions & Functional Programming"
    ]
  },
  {
    "objectID": "sessions/08-functions/index.html#what-are-functions",
    "href": "sessions/08-functions/index.html#what-are-functions",
    "title": "Functions & Functional Programming",
    "section": "What are Functions?",
    "text": "What are Functions?\nA function is just a reusable set of instructions that takes input, does something with it, and gives you a result.\nIf you’ve used Excel, you already use functions all the time. For example, SUM(A1:A10) or VLOOKUP(…). You give them arguments (the input), they process it, and they return an output. If you’ve used SQL, it’s the same idea. COUNT(*), ROUND(price, 2), or UPPER(name) are functions. They save you from writing the same logic over and over, and they keep code tidy.\nIn Python, functions work the same way, but you can also write your own custom ones, so instead of just using what is built-in, you can create tools that do exactly what you need.",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "8. Functions & Functional Programming"
    ]
  },
  {
    "objectID": "sessions/08-functions/index.html#simple-built-in-user-defined-functions",
    "href": "sessions/08-functions/index.html#simple-built-in-user-defined-functions",
    "title": "Functions & Functional Programming",
    "section": "Simple Built-In & User-Defined Functions",
    "text": "Simple Built-In & User-Defined Functions\nPython already has many built-in functions which makes the language more functional.\n\nPrint Statements\nThe print() function sends output to the screen. It’s often the first Python function you use.\n\nprint(\"Hello, World!\")\n\nHello, World!\n\n\n\n\nComparing Operations\nWe’ll compare how to do things “manually” with loops vs. using Python’s built-in (or imported) functions. This shows how functions save time and reduce code.\n\nLength\nWe can count the number of items in a list using a for loop.\n\nvalues = [10, 20, 30, 40, 50]\n\nlength_manual = 0\nfor _ in values:\n    length_manual += 1\nprint(\"Length:\", length_manual)\n\nLength: 5\n\n\nHowever, it is much faster to just use len() instead.\n\nprint(\"Length:\", len(values))\n\nLength: 5\n\n\n\n\nSum\nWe can also sum the value of all the numbers in our values object.\n\ntotal_manual = 0\nfor val in values:\n    total_manual += val\nprint(\"Sum:\", total_manual)\n\nSum: 150\n\n\nOr we can use sum().\n\nprint(\"Sum:\", sum(values))\n\nSum: 150\n\n\n\n\nMean\nFinally, we can manually calculate the mean of our list of values by summing them and then dividing by the length of the list.\n\ntotal_for_mean = 0\ntotal_length = 0\n\nfor val in values:\n    total_for_mean += val\n\nfor val in values:\n    total_length += 1\n\nmean_manual = total_for_mean / total_length\nprint(\"Mean:\", mean_manual)\n\nMean: 30.0\n\n\nOr we can import numpy and use np.mean().\n\nimport numpy as np\n\nvalues = [10, 20, 30, 40, 50]\nprint(\"Mean:\", np.mean(values))\n\nMean: 30.0\n\n\n\n\n\nCombining Operations\nWe can create our own functions to group multiple calculations. The function below takes two numbers and returns a sentence describing their sum, difference, and product.\n\ndef summarise_numbers(a, b):\n\n    total = a + b\n    difference = a - b\n    product = a * b\n    return (\n        f\"The sum of {a} and {b} is {total}, \"\n        f\"the difference is {difference}, \"\n        f\"and their product is {product}.\"\n    )\n\nsummarise_numbers(10, 5)\n\n'The sum of 10 and 5 is 15, the difference is 5, and their product is 50.'\n\n\nTo illustrate how functions work, we can break them down step-by-step. def summarise_numbers(a, b) is the function header. def states that you are defining a function, summarise_numbers is the function name, and (a, b) is the input parameter (the numbers we are summarising). The function body (the indented code below the header) defines the steps the function should take, and the return statement declares the output from the function.",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "8. Functions & Functional Programming"
    ]
  },
  {
    "objectID": "sessions/08-functions/index.html#exploring-data-with-functions",
    "href": "sessions/08-functions/index.html#exploring-data-with-functions",
    "title": "Functions & Functional Programming",
    "section": "Exploring Data with Functions",
    "text": "Exploring Data with Functions\nWe can use functions to explore an entire dataset quickly and efficiently, where a manual process would require a lot of repetition.\n\nSetup\nFirst, we will import all of the packages we need.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\nfrom sklearn.datasets import fetch_california_housing\n\nsns.set_theme(style=\"whitegrid\")\n\n\n\nImport Data\nWe can then import the California housing dataset and store it in housing_raw, before previewing the housing_raw object.\n\nhousing_raw = fetch_california_housing(as_frame=True).frame\nhousing_raw.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n\n\n\n\n\n\n\nPreprocess Data\nWe’ll make a helper function to convert text to snake_case (lowercase with underscores). This is a common style for column names.\n\ndef to_snake_case(s: str) -&gt; str:\n    \"\"\"\n    Convert a given string to snake_case.\n    \"\"\"\n    s = s.strip()  # remove leading/trailing spaces\n    s = re.sub(r'[\\s-]+', '_', s)  # replace spaces and hyphens with underscores\n    s = re.sub(r'(?&lt;=[a-z])(?=[A-Z])', '_', s)  # add underscore before capital letters\n    s = re.sub(r'[^a-zA-Z0-9_]', '', s)  # remove anything not letter, number, or underscore\n    return s.lower()  # make everything lowercase\n\nThis function has the same basic structure as the function we defined earlier, but with some additional information that is good practice for writing reproducible code. In the function header, the input (s: str) includes the input parameter s and a type-hint starting that s should be a string. The -&gt; str immediately after states that the function will return a string. The triple-quoted text just below the function header describes what the function does. You can also include what the function expects and what it returns.\nNext, we can create a function that cleans our dataset, including the to_snake_case function as a step in the process. The other step is to drop all NAs and duplicates.\n\ndef preprocess_data(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Preprocess a dataframe by cleaning and standardizing column names.\n    \"\"\"\n    df = df.dropna().drop_duplicates().copy()  # remove missing rows and duplicates\n    df.columns = [to_snake_case(col) for col in df.columns]  # rename columns to snake_case\n    return df  # return cleaned dataframe\n\nWe can then apply this to our dataset.\n\ndf = preprocess_data(housing_raw)\ndf.head()\n\n\n\n\n\n\n\n\nmed_inc\nhouse_age\nave_rooms\nave_bedrms\npopulation\nave_occup\nlatitude\nlongitude\nmed_house_val\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n\n\n\n\n\n\n\nVisualise Distributions\nA great way to use functions for exploratory data analysis is for visualing multiple columns at once. If we visualise every column manually, this would require a lot of code. However, we can write a single function that returns a plot for every relevant column in a single figure.\nBelow is a function for plotting a histogram for each numeric column in a single figure.\n\ndef plot_numeric_columns(df: pd.DataFrame) -&gt; None:\n\n    \"\"\"\n    plot histograms for all numeric columns in one figure with subplots.\n    \"\"\"\n    \n    numeric_cols = df.select_dtypes(include=[np.number]).columns  # get numeric column names\n    n = len(numeric_cols)  # count how many numeric columns there are\n    if n == 0: # if there are no numeric columns  \n        print(\"no numeric columns found\") # tell the user\n        return # and stop the function\n\n    # determine how many plots per row (max 3)\n    ncols = min(n, 3)  # number of columns in subplot grid\n    nrows = (n + ncols - 1) // ncols  # number of rows in subplot grid (ceiling division)\n    fig, axes = plt.subplots(nrows, ncols, figsize=(6 * ncols, 4 * nrows))  # create figure and axes\n    if n == 1:  # if only one numeric column\n        axes = [axes]  # put single axis in a list for consistency\n    else:\n        axes = axes.flatten()  # flatten 2d array of axes into 1d list\n\n    for ax, col in zip(axes, numeric_cols):  # loop through axes and column names\n        ax.hist(df[col], bins=20, edgecolor=\"black\")  # draw histogram for column\n        ax.set_xlabel(col)  # set x-axis label\n        ax.set_ylabel(\"\")  # remove y-axis label\n\n    # remove any extra empty plots\n    for ax in axes[len(numeric_cols):]:  # loop over unused axes\n        fig.delaxes(ax)  # delete unused subplot\n\n    plt.tight_layout()  # adjust layout so plots don't overlap\n    plt.show()  # display the plots\n\nAnd then we can run this function on our California housing dataset.\n\nplot_numeric_columns(df)\n\n\n\n\n\n\n\n\nWe can do the same for categorical columns, using bar charts.\n\ndef plot_categorical_columns(df: pd.DataFrame) -&gt; None:\n    \n    \"\"\"\n    plot bar charts for all categorical columns in one figure with subplots.\n    \"\"\"\n    \n    cat_cols = df.select_dtypes(exclude=[np.number]).columns  # get non-numeric column names\n    n = len(cat_cols)  # count how many categorical columns there are\n    if n == 0:  # if there are no categorical columns\n        print(\"no categorical columns found\")  # tell the user\n        return  # and stop the function\n\n    # determine how many plots per row (max 3)\n    ncols = min(n, 3)  # number of columns in subplot grid\n    nrows = (n + ncols - 1) // ncols  # number of rows in subplot grid (ceiling division)\n    fig, axes = plt.subplots(nrows, ncols, figsize=(6 * ncols, 4 * nrows))  # create figure and axes\n    if n == 1:  # if only one categorical column\n        axes = [axes]  # put single axis in a list for consistency\n    else:\n        axes = axes.flatten()  # flatten 2d array of axes into 1d list\n\n    for ax, col in zip(axes, cat_cols):  # loop through axes and column names\n        df[col].value_counts().plot(kind=\"bar\", ax=ax, edgecolor=\"black\")  # draw bar chart\n        ax.set_xlabel(col)  # set x-axis label\n        ax.set_ylabel(\"\")  # remove y-axis label\n\n    # remove any extra empty plots\n    for ax in axes[len(cat_cols):]:  # loop over unused axes\n        fig.delaxes(ax)  # delete unused subplot\n\n    plt.tight_layout()  # adjust layout so plots don't overlap\n    plt.show()  # display the plots\n\nHowever, there are no categorical columns in this dataset1.\n\nplot_categorical_columns(df)\n\nno categorical columns found",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "8. Functions & Functional Programming"
    ]
  },
  {
    "objectID": "sessions/08-functions/index.html#summary",
    "href": "sessions/08-functions/index.html#summary",
    "title": "Functions & Functional Programming",
    "section": "Summary",
    "text": "Summary\nFunctions let you package steps into reusable, predictable tools. You will have used functions before in other settings, and when writing Python code you will regularly encounter built-in functions and functions imported from packages. The more you work in Python, the more you’ll see yourself building small helper functions to avoid repeating code.",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "8. Functions & Functional Programming"
    ]
  },
  {
    "objectID": "sessions/08-functions/index.html#exercises",
    "href": "sessions/08-functions/index.html#exercises",
    "title": "Functions & Functional Programming",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a function that returns the maximum and minimum values in a list.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef min_max(lst):\n    return min(lst), max(lst)\n\nmin_max([4, 1, 9])\n\n(1, 9)\n\n\n\n\n\n\nModify summarise_numbers to also return the division result (a / b).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef summarise_numbers(a, b):\n    total = a + b\n    difference = a - b\n    product = a * b\n    division = a / b\n    return total, difference, product, division\n\nsummarise_numbers(5, 10)\n\n(15, -5, 50, 0.5)\n\n\n\n\n\n\nWrite a function that counts how many even numbers are in a list.\n\nHint: This requires using a ‘modulo operator’2.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef count_evens(lst):\n    return sum(1 for x in lst if x % 2 == 0)\n\nvalues = [1, 2, 3, 4, 5]\ncount_evens(values)\n\n2\n\n\n\n\n\nFor the next three questions, you can use this sample dataset:\n\nsample_df = pd.DataFrame({\n    \"Name\": np.random.choice([\"Alice\", \"Bob\", \"Charlie\", \"John\"], size=20),\n    \"Department\": np.random.choice([\"HR\", \"IT\", \"Finance\"], size=20),\n    \"Age\": np.random.randint(21, 60, size=20),\n    \"Salary\": np.random.randint(30000, 80000, size=20),\n    \"Years_at_Company\": np.random.randint(1, 20, size=20)\n})\n\n\nCreate a function that takes a dataframe and returns only columns with numeric data.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef select_numeric(df):\n    return df.select_dtypes(include=[np.number])\n\nselect_numeric(sample_df)\n\n\n\n\n\n\n\n\nAge\nSalary\nYears_at_Company\n\n\n\n\n0\n40\n56878\n4\n\n\n1\n54\n77552\n16\n\n\n2\n49\n36998\n14\n\n\n3\n52\n37384\n7\n\n\n4\n55\n72077\n3\n\n\n5\n29\n38431\n15\n\n\n6\n38\n58165\n4\n\n\n7\n58\n37899\n8\n\n\n8\n28\n43677\n7\n\n\n9\n51\n38795\n8\n\n\n10\n28\n57380\n5\n\n\n11\n32\n30778\n15\n\n\n12\n49\n42577\n8\n\n\n13\n25\n65975\n5\n\n\n14\n45\n52071\n1\n\n\n15\n44\n36087\n15\n\n\n16\n58\n52010\n16\n\n\n17\n53\n60327\n15\n\n\n18\n29\n49389\n1\n\n\n19\n36\n42061\n13\n\n\n\n\n\n\n\n\n\n\n\nRewrite plot_numeric_columns so it uses seaborn’s histplot instead of matplotlib’s hist.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef plot_numeric_columns(df):\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    for col in numeric_cols:\n        sns.histplot(df[col], bins=20)\n        plt.show()\n\nplot_numeric_columns(sample_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite a function that converts all string columns in a dataframe to lowercase.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef lowercase_strings(df):\n    for col in df.select_dtypes(include=['object']):\n        df[col] = df[col].str.lower()\n    return df\n\nlowercase_strings(sample_df)\n\n\n\n\n\n\n\n\nName\nDepartment\nAge\nSalary\nYears_at_Company\n\n\n\n\n0\ncharlie\nit\n40\n56878\n4\n\n\n1\njohn\nit\n54\n77552\n16\n\n\n2\nbob\nit\n49\n36998\n14\n\n\n3\nalice\nfinance\n52\n37384\n7\n\n\n4\ncharlie\nit\n55\n72077\n3\n\n\n5\ncharlie\nfinance\n29\n38431\n15\n\n\n6\ncharlie\nit\n38\n58165\n4\n\n\n7\nalice\nit\n58\n37899\n8\n\n\n8\nbob\nfinance\n28\n43677\n7\n\n\n9\ncharlie\nit\n51\n38795\n8\n\n\n10\ncharlie\nfinance\n28\n57380\n5\n\n\n11\ncharlie\nfinance\n32\n30778\n15\n\n\n12\ncharlie\nhr\n49\n42577\n8\n\n\n13\njohn\nhr\n25\n65975\n5\n\n\n14\ncharlie\nfinance\n45\n52071\n1\n\n\n15\nalice\nhr\n44\n36087\n15\n\n\n16\ncharlie\nhr\n58\n52010\n16\n\n\n17\njohn\nfinance\n53\n60327\n15\n\n\n18\ncharlie\nit\n29\n49389\n1\n\n\n19\nalice\nhr\n36\n42061\n13",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "8. Functions & Functional Programming"
    ]
  },
  {
    "objectID": "sessions/08-functions/index.html#footnotes",
    "href": "sessions/08-functions/index.html#footnotes",
    "title": "Functions & Functional Programming",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is just here as a demonstration of how you would do this with categorical columns, and to show what it would look like if the function cannot find relevant columns and stops early.↩︎\nIf you don’t know what a modulo operator is (totally understandable), you can search this online and it will likely help you find the answer to this question. It is always okay (encouraged, even) to search for answers to code questions online.↩︎",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "8. Functions & Functional Programming"
    ]
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html",
    "href": "sessions/05-eda-seaborn/index.html",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "",
    "text": "This session will build on the previous session that introduced the Seaborn library, using it to visualise data and do some exploratory analysis.\nWe are using Australian weather data, taken from Kaggle. This dataset is used to build machine learning models that predict whether it will rain tomorrow, using data about the weather every day from 2007 to 2017. To download the data, click here.\nThe objective from this session is to:\n# import packages\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# control some deprecation warnings in seaborn\nwarnings.filterwarnings(\n    \"ignore\",\n    category=FutureWarning,\n    module=\"seaborn\"\n)\n\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')\nFirst, we will take a subset of the data, using Australia’s five biggest cities. This gives us a more manageable dataset to work with.\n# subset of observations from five biggest cities\nbig_cities = (\n    df.loc[df['Location'].isin(['Adelaide', 'Brisbane', 'Melbourne', 'Perth', 'Sydney'])]\n    .copy()\n)",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html#exploratory-data-analysis",
    "href": "sessions/05-eda-seaborn/index.html#exploratory-data-analysis",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nWhat does exploratory data analysis aim to achieve? What are you looking for when visualising data? Patterns, shapes, signals!\nWhen we describe a variable, or a sample of that variable, we are interested in understanding the characteristics of the observations. The starting point for doing this is describing the value that the data tends to take (central tendency), and how much it tends to deviate from its typical value (spread). Visualising the distribution of a variable can tell us these things (approximately), and can tell us about the shape of the data too.\nThe “central tendency” is the average or most common value that a variable takes. Mean, median, and mode are all descriptions of the central tendency.\n\nMean - Sum of values in a sample divided by the total number of observations.\nMedian - The midpoint value if the sample is ordered from highest to lowest.\nMode - The most common value in the sample1.\n\nThe mean is the most common approach, but the mean, median, and mode choice are context-dependent. Other approaches exist, too, such as the geometric mean2.\n\n# mode rainfall by location\nbig_cities.groupby('Location')['Rainfall'].agg(pd.Series.mode)\n\nLocation\nAdelaide     0.0\nBrisbane     0.0\nMelbourne    0.0\nPerth        0.0\nSydney       0.0\nName: Rainfall, dtype: float64\n\n\n\n# mode location\nbig_cities['Location'].agg(pd.Series.mode)\n\n0    Sydney\nName: Location, dtype: object\n\n\n\n# mode location using value counts\nbig_cities['Location'].value_counts().iloc[0:1]\n\nLocation\nSydney    3344\nName: count, dtype: int64\n\n\n\n# mean rainfall by location\nnp.round(big_cities.groupby('Location')['Rainfall'].mean(), decimals=2)\n\nLocation\nAdelaide     1.57\nBrisbane     3.14\nMelbourne    1.87\nPerth        1.91\nSydney       3.32\nName: Rainfall, dtype: float64\n\n\n\n# median rainfall by location\nbig_cities.groupby('Location')['Rainfall'].median()\n\nLocation\nAdelaide     0.0\nBrisbane     0.0\nMelbourne    0.0\nPerth        0.0\nSydney       0.0\nName: Rainfall, dtype: float64\n\n\n\n# geometric mean max temperature by location\nbig_cities.groupby('Location')['MaxTemp'].apply(lambda x: np.exp(np.log(x).mean()))\n\nLocation\nAdelaide     21.888697\nBrisbane     26.152034\nMelbourne    19.972352\nPerth        24.320203\nSydney       22.570993\nName: MaxTemp, dtype: float64\n\n\nThe values across the different measures of central tendency are not always the same. In this case, the mean and median differs massively.\nQuestions:\n\nWhy is that? Why would the median rainfall be zero for all five cities?\nDoes this matter? How would it change our understanding of the rainfall variable?\n\nDistributions can tell us more. We have simulated three different distributions that have slightly different shapes, to see how their mean and median values differ.\n\n\nPlot Code (Click to Expand)\n# generate distributions\nnp.random.seed(123)\nnormal_dist = np.random.normal(10, 1, 1000)\nright_skewed_dist = np.concatenate([np.random.normal(8, 2, 600), np.random.normal(14, 4, 400)])\nleft_skewed_dist = np.concatenate([np.random.normal(14, 2, 600), np.random.normal(8, 4, 400)])\n\n# set figure size\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# function for calculating summary statistics and plotting distributions\ndef plot_averages(ax, data, title):\n    mean = np.mean(data)\n    median = np.median(data)\n    \n    sns.histplot(data, color=\"#d9dcd6\", bins=30, ax=ax)\n    ax.axvline(mean, color=\"#0081a7\", linewidth=3, linestyle=\"--\", label=f\"Mean: {mean:.2f}\")\n    ax.axvline(median, color=\"#ef233c\", linewidth=3, linestyle=\"--\", label=f\"Median: {median:.2f}\")\n    ax.set_title(title)\n    ax.set_ylabel('')\n    ax.legend()\n\n# plot distributions\nfig, axes = plt.subplots(1, 3, sharey=True)\n\nplot_averages(axes[0], normal_dist, \"Normal Distribution\\n(Mean ≈ Median)\")\nplot_averages(axes[1], right_skewed_dist, \"Right-Skewed Distribution\\n(Mean &gt; Median)\")\nplot_averages(axes[2], left_skewed_dist, \"Left-Skewed Distribution\\n(Mean &lt; Median)\")\n\nplt.suptitle(\"Comparison of Mean & Median Across Distributions\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe mean and median of the normal distribution are identical, while the two skewed distributions have slightly different means and medians.\n\nThe mean is larger than the median when the distribution is right-skewed, and the median is larger than the mean when it is left-skewed.\n\nWhen the distribution is skewed, the median value will be a better description of the central tendency, because the mean value is more sensitive to extreme values (and skewed distributions have longer tails of extreme values).\n\n\nThese differences point to another important factor to consider when summarising data - the spread or deviation of the sample.\n\nHow do we measure how a sample is spread around the central tendency?\n\nStandard deviation and variance quantify spread.\nVariance, the average squared difference between observations and the mean value, measures how spread out a sample is.\nStandard deviation is the square root of the variance. It’s easier to interpret because it’s in the same units as the sample.\n\n\n\n\nPlot Code (Click to Expand)\n# generate distributions\nnp.random.seed(123)\nmean = 10\nstd_devs = [1, 2, 3]\ndistributions = [np.random.normal(mean, std_dev, 1000) for std_dev in std_devs]\n\n# function for calculating summary statistics and plotting distributions\ndef plot_spread(ax, data, std_dev, title):\n    mean = np.mean(data)\n    std_dev = np.std(data)\n\n    sns.histplot(data, color=\"#d9dcd6\", bins=30, ax=ax)\n    ax.axvline(mean, color=\"#0081a7\", linewidth=3, linestyle=\"--\", label=f\"Mean: {mean:.2f}\")\n    ax.axvline(mean + std_dev, color=\"#ee9b00\", linewidth=3, linestyle=\"--\", label=f\"Mean + 1 SD: {mean + std_dev:.2f}\")\n    ax.axvline(mean - std_dev, color=\"#ee9b00\", linewidth=3, linestyle=\"--\", label=f\"Mean - 1 SD: {mean - std_dev:.2f}\")\n    ax.set_title(f\"{title}\")\n    ax.legend()\n\n# plot distributions\nfig, axes = plt.subplots(1, 3, sharey=True, sharex=True)\n\nfor i, std_dev in enumerate(std_devs):\n    plot_spread(axes[i], distributions[i], std_dev, f\"Standard Deviation = {std_dev}\")\n\nplt.suptitle(\"Effect of Standard Deviation on Distribution Shape\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs standard deviation increases, the spread of values around the mean increases.\nWe can compute various summary statistics that describe a sample (mean, median, standard deviation, kurtosis etc. etc.), or we can just visualise it!\nVisualising distributions is a good starting point for understanding a sample. It can quickly and easily tell you a lot about the data.",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html#exploring-australian-weather",
    "href": "sessions/05-eda-seaborn/index.html#exploring-australian-weather",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "Exploring Australian Weather",
    "text": "Exploring Australian Weather\n\nVisualising Single Variables\nWe can start by visualising the distribution of rainfall and sunshine in Australia’s big cities, including dashed lines to show the mean and median values.\n\n# plot distribution of rainfall\nrainfall_mean = np.mean(big_cities['Rainfall'])\nrainfall_median = np.median(big_cities['Rainfall'].dropna())\n\nsns.histplot(data=big_cities, x='Rainfall', binwidth=10, color=\"#d9dcd6\")\nplt.axvline(rainfall_mean, color=\"#0081a7\", linestyle=\"--\", linewidth=2, label=f\"Mean: {rainfall_mean:.2f}\")\nplt.axvline(rainfall_median, color=\"#ef233c\", linestyle=\"--\", linewidth=2, label=f\"Median: {rainfall_median:.2f}\")\n\nplt.title(\"Distribution of Rainfall in Australia's Big Cities\")\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# plot distribution of sunshine\nsunshine_mean = np.mean(big_cities['Sunshine'])\nsunshine_median = np.median(big_cities['Sunshine'].dropna())\n\nsns.histplot(data=big_cities, x='Sunshine', binwidth=1, color=\"#d9dcd6\")\nplt.axvline(sunshine_mean, color=\"#0081a7\", linestyle=\"--\", linewidth=2, label=f\"Mean: {sunshine_mean:.2f}\")\nplt.axvline(sunshine_median, color=\"#ef233c\", linestyle=\"--\", linewidth=2, label=f\"Median: {sunshine_median:.2f}\")\n\nplt.title(\"Distribution of Sunshine in Australia's Big Cities\")\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nRainfall is very skewed, because the vast majority of days have zero rainfall. The distribution of sunshine is a little more evenly spread.\nWhile these two plots require a little more code, we can get most of what we want with a lot less just using sns.histplot() on its own. For example, plotting the distribution of maximum temperature, without all the other bells and whistles, already tells us a lot.\n\nsns.histplot(data=big_cities, x='MaxTemp')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can also look at the distribution of observations split by group, using sns.countplot(). Below, we see the number of observations per city in our subset.\n\nsns.countplot(big_cities, x='Location', color=\"#d9dcd6\", edgecolor='black')\nplt.ylim(3000, 3500)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSimilarly, we can look at the number of days with or without rain the next day.\n\nsns.countplot(big_cities, x='RainTomorrow', color=\"#d9dcd6\", edgecolor='black')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nHowever, it’s worth noting there is a simpler approach to this, just using pd.DataFrame.value_counts().\n\nbig_cities['RainTomorrow'].value_counts()\n\nRainTomorrow\nNo     11673\nYes     3543\nName: count, dtype: int64\n\n\n\n\nVisualising Multiple Variables\nWe will often want to know how values of a given variable change based on the values of another. This may not indicate a relationship, but it helps us better understand our data. There are lots of ways we can do this.\nWe can use sns.barplot() to plot the average hours of sunshine by location.\n\nsns.barplot(big_cities, x='Location', y='Sunshine', color=\"#d9dcd6\", edgecolor='black')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nOr we could use sns.boxplot(), visualising the distribution of maximum temperatures and humidity at 3pm by location.\n\nsns.boxplot(big_cities, x='Location', y='MaxTemp', color=\"#d9dcd6\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.boxplot(data=big_cities, x='RainTomorrow', y='Humidity3pm', color=\"#d9dcd6\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAnother way we can compare the distribution of values by groups is using sns.kdeplot(), which visualises a kernel-density estimation.\n\nsns.kdeplot(data=big_cities, x='Humidity3pm', hue='RainTomorrow')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhile there are lots of different ways we can quickly and easily compare two variables, or compare the value of a variable by groups, the right approach will always be context-dependent. It will all depend on what questions you have about your data, and which variables you are interested in.\nIf you have lots of questions about multiple variables, one way of exploring quickly is sns.pairplot().\n\nbiggest_cities = big_cities.loc[big_cities[\"Location\"].isin(['Sydney', 'Melbourne'])]\nsns.pairplot(\n    biggest_cities,\n    vars=['MinTemp', 'Sunshine', 'Rainfall'],\n    hue='Location'\n    )\n\nplt.show()\n\n\n\n\n\n\n\n\nI tend to struggle to infer much from complex plots like this, so I prefer to create separate plots using sns.scatterplot().\n\nsns.scatterplot(big_cities, x='Sunshine', y='MaxTemp', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nScatterplots can help you visualise how two continuous variables vary together. The above plot shows that sunshine hours are positively associated with maximum temperature, but there is significant noise.\nIf we compare this with a scatterplot visualising the association between two variables that should have a strong relationship, such as humidity at 9am and 3pm, we can see the difference.\n\nsns.scatterplot(big_cities, x='Humidity9am', y='Humidity3pm', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThere is still plenty of noise, but as humidity at 9am increases, it is clear that humidity at 3pm is likely to increase.\nAnother change we might make, to reduce the noise, is adding grouping structures to our scatterplot. Perhaps much of the noise in the sunshine scatterplot is because we are looking at data across many cities.\n\nsns.scatterplot(biggest_cities, x='Sunshine', y='MaxTemp', hue='Location', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhen we compare just two cities, there still appears to be significant noise.\nSometimes you might need to do some more complex operations to transform the data before visualising it, in order to ask more specific questions. For example, you might want to compare how the total rainfall per day has varied over time in the data.\n\n(\n    big_cities\n    # convert date to datetime\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    # create year-month column\n    .assign(Year_Month=lambda x: x['Date'].dt.to_period('M'))\n    # group by year-month and calculate sum of rainfall\n    .groupby('Year_Month')['Rainfall'].sum()\n    # convert year-month index back to column in dataframe\n    .reset_index()\n    # create year-month timestamp for plotting\n    .assign(Year_Month=lambda x: x['Year_Month'].dt.to_timestamp()) \n    # pass df object to seaborn lineplot\n    .pipe(lambda df: sns.lineplot(data=df, x='Year_Month', y='Rainfall', linewidth=2))\n)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe above plot leverages an approach called method chaining, where we call multiple methods one after the other in the same operation3. Method chaining syntax is sometimes a little easier to follow, and you don’t have to create new objects for every operation, which can be a tidier way to work.\nWe can do the same to transform the data and visualise the mean average sunshine per month.\n\n(\n    big_cities\n    # convert date to datetime object\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    # set date column as index\n    .set_index('Date')\n    # resample by month-end for monthly aggregations\n    .resample('ME')\n    # calculate mean sunshine per month\n    .agg({'Sunshine': 'mean'})\n    # convert month index back to column in dataframe\n    .reset_index()\n    # pass df object to seaborn lineplot\n    .pipe(lambda df: sns.lineplot(data=df, x='Date', y='Sunshine', color=\"#1f77b4\", linewidth=2))\n)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFinally, we could combine two plots to look at how the average rainfall and average sunshine both vary by month.\n\nfig, axes = plt.subplots(1, 2)\n\n(\n    big_cities\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    .assign(Month=lambda x: x['Date'].dt.month)\n    .groupby('Month')['Rainfall'].mean()\n    .reset_index()\n    .pipe(lambda df: sns.lineplot(data=df, x='Month', y='Rainfall', color=\"#1f77b4\", linewidth=2, ax=axes[0]))\n)\n\n(\n    big_cities\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    .assign(Month=lambda x: x['Date'].dt.month) \n    .groupby('Month')['Sunshine'].mean() \n    .reset_index()\n    .pipe(lambda df: sns.lineplot(data=df, x='Month', y='Sunshine', color=\"#ff7f0e\", linewidth=2, ax=axes[1]))\n)\n\nxticks = range(1, 13)\nxticklabels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nfor ax in axes:\n    ax.set_xticks(xticks)  # Set ticks\n    ax.set_xticklabels(xticklabels, rotation=45)\n    ax.set_xlabel('')\n    ax.set_ylabel('')\naxes[0].set_title('Average Rainfall by Month', fontsize=16)\naxes[1].set_title('Average Sunshine by Month', fontsize=16)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html#exercises",
    "href": "sessions/05-eda-seaborn/index.html#exercises",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "Exercises",
    "text": "Exercises\nSome of these questions are easily answered by scrolling up and finding the answer in the output of the above code, however, the goal is to find the answer using code. No one actually cares what the answer to any of these questions is, it’s the process that matters!\nRemember, if you don’t know the answer, it’s okay to Google it (or speak to others, including me, for help)!\n\n\nImport Data (to Reset)\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')\n\n\n\nWhat does the distribution of minimum daily temperatures look like in these cities? Are there any unusual patterns?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.histplot(big_cities[\"MinTemp\"].dropna(), kde=True)\nplt.title(\"Distribution of MinTemp\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nDoes the amount of sunshine vary depending on whether it rains the next day? Visualise this.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.boxplot(data=big_cities, x=\"RainTomorrow\", y=\"Sunshine\")\nplt.title(\"Sunshine by Rain Tomorrow\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHow closely related are atmospheric pressure readings in the morning compared to the afternoon?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.scatterplot(data=big_cities, x=\"Pressure9am\", y=\"Pressure3pm\")\nplt.title(\"Pressure at 9am vs 3pm\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHow does humidity in the afternoon vary across the five cities? What can you infer from this?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.violinplot(data=big_cities, x=\"Location\", y=\"Humidity3pm\")\nplt.title(\"Humidity at 3pm by City\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nAre days when rain is expected tomorrow more or less common in this dataset? Show the distribution.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.countplot(data=big_cities, x=\"RainTomorrow\")\nplt.title(\"Rain Tomorrow Counts\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nIs there any relationship between afternoon temperature and humidity? Does this relationship change depending on whether it rains the next day?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.scatterplot(data=big_cities, x=\"Temp3pm\", y=\"Humidity3pm\", hue=\"RainTomorrow\")\nplt.title(\"Temp vs Humidity at 3pm by Rain Tomorrow\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHow strongly are the different continuous variables in this dataset correlated with each other? Create a correlation matrix.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ncorr = big_cities.select_dtypes(include=\"number\").corr().round(2)\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, cmap=\"coolwarm\", center=0)\nplt.title(\"Correlation Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nExplore the relationships between rainfall, sunshine, and afternoon humidity across the cities. What patterns stand out?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.pairplot(big_cities, vars=[\"Rainfall\", \"Sunshine\", \"Humidity3pm\"], hue=\"Location\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHow has the maximum temperature in Brisbane changed over time? Create a time series visualisation.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbrisbane = big_cities.loc[big_cities[\"Location\"] == \"Brisbane\"]\nbrisbane['Date'] = pd.to_datetime(brisbane['Date'])\nbrisbane_daily = brisbane.groupby(\"Date\")[\"MaxTemp\"].mean().reset_index()\n\nplt.figure(figsize=(12, 4))\nsns.lineplot(data=brisbane_daily, x=\"Date\", y=\"MaxTemp\")\nplt.title(\"Daily Max Temp in Brisbane\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does the distribution of daily rainfall amounts look like? Is it skewed or symmetric?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.kdeplot(data=big_cities, x=\"Rainfall\", fill=True)\nplt.title(\"Rainfall Distribution\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHow does the average morning wind speed compare across the five cities?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.barplot(data=big_cities, x=\"Location\", y=\"WindSpeed9am\", ci=None)\nplt.title(\"Average Morning Wind Speed by City\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nIn Perth, is there any visible relationship between the amount of sunshine and rainfall?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nperth = big_cities.loc[big_cities[\"Location\"] == \"Perth\"]\nsns.scatterplot(data=perth, x=\"Sunshine\", y=\"Rainfall\")\nplt.title(\"Sunshine vs Rainfall in Perth\")\nplt.show()",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html#footnotes",
    "href": "sessions/05-eda-seaborn/index.html#footnotes",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe mode value is generally most useful when dealing with categorical variables.↩︎\nThe geometric mean multiplies all values in the sample and takes the \\(n\\)th root of that multiplied value. It can be useful when dealing with skewed data or data with very large ranges, and when dealing with rates, proportions etc. However it can’t handle zeros or negative values.↩︎\nThis may not be something you feel comfortable with yet, but it is something you may come across, and could explore in the future.↩︎",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#what-to-expect",
    "href": "sessions/01-onboarding/slides.html#what-to-expect",
    "title": "Python Onboarding",
    "section": "What to Expect?",
    "text": "What to Expect?\n\nLearning a language is hard. It can be frustrating. Perseverance is key to success.\nThese sessions will introduce you to Python, showing you what is possible and how to achieve some of what might benefit your work.\nBut the real learning comes by doing. You need to run the code yourself, have a play around, and cement what you’ve learned by applying it.\nPractice, repetition, and making mistakes along the way is how real progress is made."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#why-learn-python",
    "href": "sessions/01-onboarding/slides.html#why-learn-python",
    "title": "Python Onboarding",
    "section": "Why Learn Python?",
    "text": "Why Learn Python?\n\nCoding skills, generally, and Python specifically, seem to be a priority in the NHS right now. It’s a clear direction of travel. Learning now sets you up for the future.\nPython and the applied skills taught in these sessions will enable you to use advanced methods and design flexible, scalable solutions.\nPython is very valuable for career development.\nIt is (hopefully) fun!"
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#the-toolkit",
    "href": "sessions/01-onboarding/slides.html#the-toolkit",
    "title": "Python Onboarding",
    "section": "The Toolkit",
    "text": "The Toolkit\n\nWe will be using the following tools throughout this course:\n\nLanguage: Python\nDependency management: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code/Jupyter Notebooks (or your preferred IDE)\n\nYou can install all these tools by running the following in PowerShell:\n\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop"
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#python",
    "href": "sessions/01-onboarding/slides.html#python",
    "title": "Python Onboarding",
    "section": "Python",
    "text": "Python\n\nPython is an all-purpose programming language that is the most popular worldwide and widely used in almost every industry.\nPython’s popularity is owed to its flexibility – it is the second-best tool for every job.\nIt is a strong choice for data science and analytics, being one of the best languages for data wrangling, data visualisation, statistics, and machine learning.\n\nIt is also well-suited to web development, scientific computing, and automation."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#dependency-management",
    "href": "sessions/01-onboarding/slides.html#dependency-management",
    "title": "Python Onboarding",
    "section": "Dependency Management",
    "text": "Dependency Management\n\nDependency management refers to the process of tracking and managing all of the packages (dependencies) a project needs to run. It ensures:\n\nThe right packages are installed.\nThe correct versions are used.\nConflicts between packages are avoided.\n\nWe are using uv for dependency management."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#virtual-environments",
    "href": "sessions/01-onboarding/slides.html#virtual-environments",
    "title": "Python Onboarding",
    "section": "Virtual Environments",
    "text": "Virtual Environments\n\nVirtual environments are isolated Python environments that allow you to manage dependencies for a specific project without the state of those dependencies affecting other projects or your wider system. They help by:\n\nKeeping dependencies separate for each project.\nAvoiding version conflicts between projects.\nMaking dependency management more predictable and reproducible.\n\nVirtual environments are a part of dependency management, and we will use uv to manage both the dependencies and virtual environments."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#version-control",
    "href": "sessions/01-onboarding/slides.html#version-control",
    "title": "Python Onboarding",
    "section": "Version Control",
    "text": "Version Control\n\nVersion control is the practice of tracking and managing changes to code or files over time, allowing you to:\n\nRevert to earlier versions if needed.\nCollaborate with others on the same project easily.\nMaintain a history of changes.\n\nWe are using Git (the version control system) and GitHub (the platform for hosting our work)."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#ide",
    "href": "sessions/01-onboarding/slides.html#ide",
    "title": "Python Onboarding",
    "section": "IDE",
    "text": "IDE\n\nAn IDE (Integrated Development Environment) is fully featured software that provides everything you need to write code as conveniently as possible.\nIt typically includes a code editor, debugger, build tools, and features like syntax highlighting and code completion.\nSome common IDEs used for Python include VS Code, PyCharm, Vim, Jupyter Notebooks/JupyterLab, and Positron.\nWe will use VS Code or Jupyter Notebooks (which is not exactly an IDE but is similar)."
  },
  {
    "objectID": "sessions/10-streamlit/index.html",
    "href": "sessions/10-streamlit/index.html",
    "title": "Streamlit",
    "section": "",
    "text": "It is a Python library and a web platform that enables coders like you to create interactive, web-based dashboards and apps entirely1 using Python; no need to learn a language like Javascript.\nTo get an overview of its capabilities, you can visit the official website here.\nYou can see some examples of apps and dashboards created by the Streamlit community here, or you could catch up with the recording of the Code Club session recording when we showcased our own!\n\n\n\n\nTo view the dashboard in a separate browser tab, click here. The inspiration for this dashboard was a piece of work that required Providers to send data in Excel or .csv files to a team inbox for it to be processed, uploaded to the warehouse, imported back into Excel to be visualised, only for it to be sent back out to Providers. The demonstration dashboard illustrates how it would be possible to create a web-based app where Providers could upload their own data, explore and visualise it, and finally apply some kind of pre-built model to the data. In this case, we have applied a couple of forecasting models to the data: one traditional statistical model and one modern machine learning-based model. If you would like to pinch learn from the code we used to create our dashboard, you can find it here. The app.py script is the one that is run to pull all of the pages together (see more in the “How to get started” section). The “Pages” folder contains the scripts for the individual pages.\n\n\n\n\n\n\nDemo data\n\n\n\nIf you would like to make use of the data used in the demonstration, please contact your Code Club hosts via the Teams channel or write to the Specialist Analytics Team",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "10. Streamlit Dashboards"
    ]
  },
  {
    "objectID": "sessions/10-streamlit/index.html#what-is-streamlit",
    "href": "sessions/10-streamlit/index.html#what-is-streamlit",
    "title": "Streamlit",
    "section": "",
    "text": "It is a Python library and a web platform that enables coders like you to create interactive, web-based dashboards and apps entirely1 using Python; no need to learn a language like Javascript.\nTo get an overview of its capabilities, you can visit the official website here.\nYou can see some examples of apps and dashboards created by the Streamlit community here, or you could catch up with the recording of the Code Club session recording when we showcased our own!\n\n\n\n\nTo view the dashboard in a separate browser tab, click here. The inspiration for this dashboard was a piece of work that required Providers to send data in Excel or .csv files to a team inbox for it to be processed, uploaded to the warehouse, imported back into Excel to be visualised, only for it to be sent back out to Providers. The demonstration dashboard illustrates how it would be possible to create a web-based app where Providers could upload their own data, explore and visualise it, and finally apply some kind of pre-built model to the data. In this case, we have applied a couple of forecasting models to the data: one traditional statistical model and one modern machine learning-based model. If you would like to pinch learn from the code we used to create our dashboard, you can find it here. The app.py script is the one that is run to pull all of the pages together (see more in the “How to get started” section). The “Pages” folder contains the scripts for the individual pages.\n\n\n\n\n\n\nDemo data\n\n\n\nIf you would like to make use of the data used in the demonstration, please contact your Code Club hosts via the Teams channel or write to the Specialist Analytics Team",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "10. Streamlit Dashboards"
    ]
  },
  {
    "objectID": "sessions/10-streamlit/index.html#how-can-it-benefit-me-as-an-analyst",
    "href": "sessions/10-streamlit/index.html#how-can-it-benefit-me-as-an-analyst",
    "title": "Streamlit",
    "section": "How can it benefit me as an analyst?",
    "text": "How can it benefit me as an analyst?\n\nSince everything is built using Python scripts, it is easy to integrate data processing, modelling and visualisation into one app, rather than requiring separate applications to handle different elements.\nEnables the flexibility of making use of Python’s diverse range of libraries. They can be used to create finely customised visualisations and greater levels of interactivity. Streamlit itself gets updated all the time with new functionality that often just requires one line of code to implement!\nIt is an excellent way to practice your Python coding since it brings lots of elements together:\n\nProcessing data with dataframes.\nVisualisations.\nMore advanced control flow that takes user interactions into account.\nInteracting with GitHub (when you want to have your dashboard/app hosted online).\nWriting Python as complete scripts (as opposed to working cell by cell in Jupyter), which is much more like the reality of developing real-life applications.\n\nDeveloping an app or dashboard using pure code makes version control much easier than trying to do so using proprietary software. Such software can often be over-engineered with lots of things that you don’t really need, and active settings can often be hidden in layer upon layer of menus📎. It is much easier to keep on top of which settings have been applied when you have everything in front of you in a well-commented script.\nIt is free, open-source software, so you do not need to be assigned a potentially expensive licence before you can get started and start experimenting.\nStreamlit can be used as a way to bring complex analytical models to life:\n\nThe PenCHORD team at the University of Exeter, who run the Health Service Modelling Associates course, have produced a web app that allows users to interact with a Discrete Event Simulation model in order to learn how they work. You could build something similar to this to allow stakeholders to interact with your data modelling, enabling them to test different scenarios. For inspiration, you can find a list of real-life healthcare projects undertaken by HSMA alumni that have a Streamlit interface here.",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "10. Streamlit Dashboards"
    ]
  },
  {
    "objectID": "sessions/10-streamlit/index.html#how-to-get-started",
    "href": "sessions/10-streamlit/index.html#how-to-get-started",
    "title": "Streamlit",
    "section": "How to get started",
    "text": "How to get started\n\nInstallation\n\nStart a new project as outlined in the Project Setup section of the “Python Onboarding” materials.\nYou will need to install the Streamlit package with the Powershell command uv add streamlit. You will probably also want to install a package for handling dataframes such as pandas or polars, and a visualisation package such as seaborn or altair2.\n\n\n\nYour first app\nStreamlit apps are written in standard .py Python files. It is convention to use the alias “st” when importing the package, i.e. import streamlit as st.\nYour first app can be as simple as the examples in the animation on the Streamlit homepage. Streamlit contains a very handy method called .write(), which can be used to render all sorts of things, not just text. You can also use the .markdown() method to render text using markdown notation.\nIf you want to see a very simple example and have a play around, have a look at Streamlit’s Playground\nOnce you have written your script and want to run it locally in a browser window:\n\nActivate your Python environment by entering the following command in Powershell: ./.venv/Scripts/activate.ps1.\n\nYou many need to swap “venv” with the specific name of your virtual environment, if you have given it a name.\n\nLaunch the app by entering this command in Powershell: python -m streamlit run app.py, where “app.py” is the name of the Python file containing your script.\n\nYour app will be launched in a browser window on a localhost.\n\n\n\n\n\n\nBefore your very eyes…\n\n\n\nThe great thing about working with Streamlit is that you can see the effects of changes to your script more-or-less immediately. Save changes to your .py file and refresh the browser to render the changes.",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "10. Streamlit Dashboards"
    ]
  },
  {
    "objectID": "sessions/10-streamlit/index.html#deployment",
    "href": "sessions/10-streamlit/index.html#deployment",
    "title": "Streamlit",
    "section": "Deployment",
    "text": "Deployment\nIf you would like to have your app / dashboard hosted online, the recommended options are to get a free account on one of the following:\n\nStreamlit Community Cloud\nPosit Cloud\n\nNote that you will also need a free GitHub account. This is where you will upload your code and your environment requirements. Streamlit / Posit will use that information when rendering the app. You will need to set your repository to “Public” visibility.\nIf you need help pushing the necessary files to GitHub, please do feel free to contact your hosts at Code Club. If you want to get an idea of how your repository should look, have another look at the repository for the demonstration dashboard.\n\n\n\n\n\n\nIG Alert!\n\n\n\nIf you are going to be sharing your app / dashboard online, make sure that you are following all Information Governance requirements for sharing data publicly. Your masterpiece and its repository will be freely accessible to the public. It is advised that you stick to using data that has already been published, such as Fingertips data, or to create a proof-of-concept using dummy or synthetic data. If you are creating an app where end-users upload their own data, make sure that you include any test data in your .gitignore so that it does not get uploaded to GitHub.\n\n\n\n\n\n\n\n\nProduction servers\n\n\n\nPlease be aware that SCW does not currently have a production server licences, so if you have an idea for something that contains patient-level or commissioning data that you want to be able to share with customers, please contact the Specialist Analytics Team.",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "10. Streamlit Dashboards"
    ]
  },
  {
    "objectID": "sessions/10-streamlit/index.html#footnotes",
    "href": "sessions/10-streamlit/index.html#footnotes",
    "title": "Streamlit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWell, almost entirely. You may find you need to tweak the layout of the pages using some HTML, but snippets of code for this purpose are easy to come by. It is normally something like fixing the width of the sidebar. The important thing is that all of the elements that do something, i.e. process data, produce charts and enable interactivity, are programmed in Python.↩︎\nStreamlit has some methods that are designed specifcally for rendering outputs from certain packages, such as .altair_chart() for Altair charts and .pyplot() for rendering matplotlib.pyplot charts. They can often be used to tweak the default rendering: By default, rendering a Pandas dataframe will include the index. Using .dataframe() to render the dataframe allows you to hide the index, e.g. st.dataframe(df, hide_index= True).↩︎",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "10. Streamlit Dashboards"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html",
    "href": "sessions/01-onboarding/index.html",
    "title": "Python Onboarding",
    "section": "",
    "text": "This is the first session of Code Club’s relaunch. It focuses on giving users all the tools they need to get started using Python and demonstrates the setup for a typical Python project.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#session-slides",
    "href": "sessions/01-onboarding/index.html#session-slides",
    "title": "Python Onboarding",
    "section": "Session Slides",
    "text": "Session Slides\nUse the left ⬅️ and right ➡️ arrow keys to navigate through the slides below. To view in a separate tab/window, follow this link.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#the-tools-you-will-need",
    "href": "sessions/01-onboarding/index.html#the-tools-you-will-need",
    "title": "Python Onboarding",
    "section": "The Tools You Will Need",
    "text": "The Tools You Will Need\nWhile this course focuses on Python, we will use several other tools throughout.\n\nLanguage: Python\nDependency Management & Virtual Environments: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code/Jupyter Notebooks (or your preferred IDE)\n\nYou can install all the tools you’ll need by running the following one-liner run in PowerShell:\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop\n\nPython\nPython is an all-purpose programming language that is one of, if not the most popular, in the world1 and is widely used in almost every industry. Its popularity is owed to its flexibility as a language that can be used to achieve nearly any job. It is often referred to as the second-best tool for every job. Specialist languages might be better for specific tasks (for example, R for statistics), but Python is good at everything.\nIt is a strong choice for data science and analytics, being one of the best languages for data wrangling, data visualisation, statistics, and machine learning. It is also well-suited to web development, scientific computing, and automation.\n\n\nDependency Management\nOne of Python’s greatest weaknesses is dependency management. Despite its many strengths, there is no escaping the dependency hell that every Python user faces.\nDependency management refers to the process of tracking and managing all of the packages (dependencies) a project needs to run. It is a consideration in any programming language. It ensures:\n\nThe right packages are installed.\nThe correct versions are used.\nConflicts between packages are avoided.\n\nThere are many reasons that Python handles dependency management so poorly, but there are some tools that make this a little easier on users. We are using uv for dependency management. It is relatively new, but it is quickly becoming the consensus tool for dependency management in Python because it makes the process about as painless as it can be without moving to a different language entirely.\n\nVirtual Environments\nVirtual environments are a component of dependency management. Dependency management becomes much messier when you have many Python projects, each using their own packages, some overlapping and some requiring specific versions, either for compatibility or functionality reasons. Reducing some of this friction by isolating each project in its own virtual environment, like each project is walled off from all other projects, makes dependency management a little easier. Virtual environments allow you to manage dependencies for a specific project without the state of those dependencies affecting other projects or your wider system.\nVirtual environments help by:\n\nKeeping dependencies separate for each project.\nAvoiding version conflicts between projects.\nMaking dependency management more predictable and reproducible.\n\nWe will use uv to manage all dependencies, virtual environments, and even versions of Python.\n\n\n\nVersion Control\nVersion control is the practice of tracking and managing changes to code or files over time, allowing you to:\n\nRevert to earlier versions if needed.\nCollaborate with others on the same project easily.\nMaintain a history of changes.\n\nWe are using Git, a version control system, to host our work and GitHub Desktop to manage version control locally.\nVersion control and Git are topics entirely in their own right, and covering them in detail is out of the scope of this session. We hope to cover version control in a future session, but right now, you just need to be able to access materials for these sessions. You can find the materials in the Code Club repository.\nIf you have downloaded GitHub Desktop, the easiest way to access these materials and keep up-to-date is by cloning the Code Club repository (go to File, then Clone Repository, select URL, and paste the Code Club repository link in the URL field). You can then ensure that the materials you are using are the most current by clicking the Fetch Origin button in GitHub Desktop, which grabs the changes we’ve made from the central repository on GitHub.\n\n\nIDE\nIDEs (Integrated Development Environments) are software that simplifies programming and development by combining many of the most common tasks and helpful features for programming into a single tool. These typically include a code editor, debugging functionality, build tools, and features like syntax highlighting and code completion. When you start your code journey, you might not need all these tools, and fully-featured IDEs can be overwhelming. But as you become more comfortable with programming, all these different features will become very valuable.\nSome common IDEs that are used for Python include:\n\nVS Code\nPyCharm\nVim\nJupyter Notebooks/JupyterLab\nPositron\n\nWe will use VS Code or Jupyter Notebooks (which is not exactly an IDE but is similar).",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#project-setup",
    "href": "sessions/01-onboarding/index.html#project-setup",
    "title": "Python Onboarding",
    "section": "Project Setup",
    "text": "Project Setup\nEvery new Python project should start with using uv to set up a virtual environment for the project to keep everything organised and reduce the risk of finding yourself in dependency hell.\nThe entire project setup process can be handled in the command line. We will use PowerShell for consistency.\nWhen you open a PowerShell window, it should open in your C drive (e.g.,  C:\\Users\\user.name). If it does not, run cd ~, and it should return to your home directory.\nWe will create a new uv project in the home directory2 using the command uv init. The new project will contain everything we need, including a Python installation, a virtual environment, and the necessary project files for tracking and managing any packages installed in the virtual environment. To set up a new project called test-project, use the following command:\nuv init test-project\nHaving created this new directory, navigate to it using cd test-project. You can check the files in a directory using the command ls. If you run this command, you will see three files in the project directory (hello.py, pyproject.toml, and README.md). The project doesn’t yet have a Python installation or a virtual environment, but this will be added when we add external Python packages.\nYou can install Python packages using the command uv add. We can add some common Python packages that we will use in most projects (pandas, numpy, seaborn, and ipykernel3) using the following command:\nuv add pandas numpy seaborn ipykernel\nThe output from this command will reference the Python installation used and the creation of a virtual environment directory .venv. Now, if you run ls, you will see two new items in the directory, uv.lock and .venv.\nYour Python project is now set up, and you are ready to start writing some code. You can open VS Code from your PowerShell window by running code ..\nFor more information about creating and managing projects using uv, check out the uv documentation.\n\nOpening your project in VS Code\nTo open your newly-created uv project in VS Code, launch the application and click File &gt; Open Folder.... You’ll want to make sure you select the root level of your project. Once you’ve opened the folder, the file navigation pane in VS Code should display the files that uv has created, including a main.py example file. Click on this to open it.\nOnce VS Code realises you’ve opened a folder with Python code and a virtual environment, it should do the following:\n\nSuggest you install the Python extension (and, once you’ve created a Jupyter notebook, the Jupyter one) offered by Microsoft - go ahead and do this. If this doesn’t happen, you can install extensions manually from the Extensions pane on the left-hand side.\nSelect the uv-created .venv as the python Environment we’re going to use to actually run our code. If this doesn’t happen, press ctrl-shift-P, type “python environment” to find the Python - Create Environment... option, hit enter, choose “Venv” and proceed to “Use Existing”.\n\nIf all has gone well, you should be able to hit the “play” icon in the top right to execute main.py. The Terminal pane should open up below and display something like Hello from (your-project-name)!.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#footnotes",
    "href": "sessions/01-onboarding/index.html#footnotes",
    "title": "Python Onboarding",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough there are several ways to measure language popularity, the PYPL Index, HackerRank’s Developer Skills Report, and IEEE Spectrum all rank Python as the most popular language in the world, while Stack Overflow’s Developer Survey places Python third behind JavaScript and HTML/CSS.↩︎\nWe recommend using the C drive for all Python projects, especially if using version control. Storing projects like these on One Drive will create many unnecessary issues.↩︎\nStrictly speaking, we should install ipykernel as a development dependency (a dependency that is needed for any development but not when the project is put into production). In this case, we would add it by running uv add --dev ipykernel. However, in this case, it is simpler to just add it as a regular dependency, and it doesn’t harm.↩︎",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/08-functions/slides.html#what-are-functions",
    "href": "sessions/08-functions/slides.html#what-are-functions",
    "title": "Functions in Python",
    "section": "What are Functions?",
    "text": "What are Functions?\n\nFunctions are self-contained, reusable blocks of code that perform a specific task.\nThey take an input, perform a task on that input (such as a calculation or transformation), and return a result.\nFunctions help to organise code, avoid repetition, and make complex processes easier to understand and maintain."
  },
  {
    "objectID": "sessions/08-functions/slides.html#anatomy-of-a-function",
    "href": "sessions/08-functions/slides.html#anatomy-of-a-function",
    "title": "Functions in Python",
    "section": "Anatomy of a Function",
    "text": "Anatomy of a Function\n\nUnderstanding how to use (and create) functions can feel a little abstract at first.\nFunctions must have names (how they are called), arguments (inputs passed to it), statements (the tasks carried out), and return values (the output/result it returns).\nPython functions are structured as below:"
  },
  {
    "objectID": "sessions/08-functions/slides.html#shop-bought-versus-home-baked",
    "href": "sessions/08-functions/slides.html#shop-bought-versus-home-baked",
    "title": "Functions in Python",
    "section": "Shop-Bought Versus Home-Baked",
    "text": "Shop-Bought Versus Home-Baked\n\nAll languages come with built-in functions available - you will have used some of Python’s built-in functions already, for example print(), type() and sum().\nThey operate exactly the same as ones you make yourself - they are called, they take an argument and they return a result.\nYou can create your own functions or use pre-built functions either provided by the language itself or by others (imported from packages/libraries)."
  },
  {
    "objectID": "sessions/08-functions/slides.html#functions-are-not-new",
    "href": "sessions/08-functions/slides.html#functions-are-not-new",
    "title": "Functions in Python",
    "section": "Functions Are Not New",
    "text": "Functions Are Not New\n\nWhile functions are a fundamental building block of programming, they exist anywhere that you might write code.\nFor example, Excel includes lots of functions (for example, SUM(), VLOOKUP(), LINEST()), as does SQL ( for example, COUNT(), AVG(), COALESCE())!\nFunctions work very similarly in whatever language you might use, i.e. they have the same component parts. What changes is the syntax."
  },
  {
    "objectID": "sessions/08-functions/slides.html#advantages-of-functions",
    "href": "sessions/08-functions/slides.html#advantages-of-functions",
    "title": "Functions in Python",
    "section": "Advantages of Functions",
    "text": "Advantages of Functions\n\nReusability - Write code once and use it multiple times. Stay DRY (Don’t Repeat Yourself)!\nModularity - Break down complex tasks into smaller, manageable chunks.\nReadability - Make code clearer and more organised.\nMaintainability - Easier to make changes to one part of the code without breaking everything.\nReliability - More efficient code with fewer errors that others can follow."
  },
  {
    "objectID": "sessions/08-functions/slides.html#best-practices",
    "href": "sessions/08-functions/slides.html#best-practices",
    "title": "Functions in Python",
    "section": "Best Practices",
    "text": "Best Practices\n\nUse Descriptive Names - Give your function a name that describes what it does.\nPerform One Task - Functions should carry out a single task so as to avoid unnecessary complexity.\nDocument Your Functions - There are lots of ways to help anyone using your functions (including future you), such as docstrings and type hints.\nAvoid Side Effects - Functions should not change anything outside of itself.\nExplicitly Handle Errors - Setting guardrails helps avoid unexpected consequences.\nWrite Testable Code - Functions should be easy to test (and should be tested)."
  },
  {
    "objectID": "sessions/08-functions/slides.html#what-are-programming-paradigms",
    "href": "sessions/08-functions/slides.html#what-are-programming-paradigms",
    "title": "Functions in Python",
    "section": "What are Programming Paradigms",
    "text": "What are Programming Paradigms\n\nProgramming paradigms are just the different approaches or styles to programming that define how code is structured/organised.\nFunctional and Object-Oriented Programming (OOP) are two of the most common paradigms.\nLanguages like Java, C++, and Ruby are OOP, while languages like Haskell and Scala are functional.\nLots of languages (including Python) can support multiple paradigms."
  },
  {
    "objectID": "sessions/08-functions/slides.html#what-is-functional-programming",
    "href": "sessions/08-functions/slides.html#what-is-functional-programming",
    "title": "Functions in Python",
    "section": "What is Functional Programming?",
    "text": "What is Functional Programming?\n\nFunctional programming is a programming paradigm — a style of building the structure and elements of computer programs — that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data."
  },
  {
    "objectID": "sessions/08-functions/slides.html#what-is-functional-programming-actually",
    "href": "sessions/08-functions/slides.html#what-is-functional-programming-actually",
    "title": "Functions in Python",
    "section": "What is Functional Programming, Actually?",
    "text": "What is Functional Programming, Actually?\n\nIn simple (and perhaps not entirely accurate) terms, functional programming just focuses on the use of functions for writing code.\nFunctional programming treats coding as the evaluation of mathematical functions and avoids changes in state or mutable data.\nIt relies on “pure” functions that always produce the same output for the same inputs, and do not cause side effects!"
  },
  {
    "objectID": "sessions/08-functions/slides.html#conclusion",
    "href": "sessions/08-functions/slides.html#conclusion",
    "title": "Functions in Python",
    "section": "Conclusion",
    "text": "Conclusion\n\nFunctions are a great way to organise and simplify your code, and avoids repetition and inefficiency.\nUnderstanding how functions work is key to learning to use them, create them, and debug them.\nUnderstanding functional programming is key to following along in conversations with nerds."
  },
  {
    "objectID": "sessions/06-data-types/slides.html#a-brief-history-of-data-types",
    "href": "sessions/06-data-types/slides.html#a-brief-history-of-data-types",
    "title": "An Introduction To Data Types",
    "section": "A brief history of data types",
    "text": "A brief history of data types\n\n\nAll1 computers store data in binary (1s and 0s) – example shown on the right, represented as hexadecimal\nVariables add a level of convenience and abstraction by letting us name specific buckets to put data in, and data types give structure to these buckets.\nIn the early days of computing data was stored as raw binary\nThe need for specific data types came from the emergence of structured programming from the 1950s onward\nLanguages like FORTRAN and COBOL introduced the segregation of numeric datatypes and character types\nObject-oriented languages like C++ and Java further expanded on this with user-defined data types\nSpecifying the type of data allows the machine to allocate an appropriate amount of memory to it (was very important in the early days of computing, but still relevant)\nAllows us to prevent errors; setting the expectation on the exact type of data that a specific variable will contain.\n\n\n\n\n\nRaw data in hex format (ASCII representation on right).\n\n\n\n\n\nCore rope memory. More on this on the next slide. (Konstantin Lanzet, Wikimedia Commons)"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#history-lesson---core-rope-memory",
    "href": "sessions/06-data-types/slides.html#history-lesson---core-rope-memory",
    "title": "An Introduction To Data Types",
    "section": "History lesson - core rope memory",
    "text": "History lesson - core rope memory\n\nThe Apollo Guidance Computer for the Apollo programme which eventually landed the first person on the moon made use of core rope memory. The program code and fixed data (such as important physical and astronomical constant) were literally woven into a grid of magnetic round cores using a needle, with the sequence the wire took through the cores deciding the pattern of 0s and 1s. This highly technical work was done in bulk in factories by almost exclusively female workers.\n\n\n\nA closeup of a few cores in a core rope memory module, showing the hundreds of times the sense wire goes through each core2.\n\n\n\n\n\n\nA factory employee working on a core rope module3\n\n\n\n\n\nOne of the memory trays of the AGC - each rectangular module contains a self-contained core rope grid4"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#a-quick-note-on-type-systems",
    "href": "sessions/06-data-types/slides.html#a-quick-note-on-type-systems",
    "title": "An Introduction To Data Types",
    "section": "A quick note on type systems",
    "text": "A quick note on type systems\n\nProgramming languages have different philosophies. They are often referred as being “strong” or “weak” and “static” or “dynamic”.\nStrongly but dynamically-typed languages (e.g. Python)\n\nPython features dynamic typing. There is no need to explicitly declare variables as being a specific data type, and it does allow limited implicit conversions, but not as extensively as e.g. JavaScript.\n\nStatically-typed languages (C++, Rust, SQL)\n\nThe programmer has to specify the data type for a variable or object in the code itself and they are checked at compile time. Safer (catches errors early) and possibly more performant, but more tedious and less flexible\n\nWeakly-typed languages (e.g. Javascript)\n\nAllows extensive type coercion; mixing-and-matching of datatypes freely e.g. 5+“2”=“52”\n\n\n\n\n\nhttps://remotescout24.com/en/blog/806-typed-vs-untyped-programming-languages\n\n\n\n\n\nC++. This code generates a type error; we tried to assign a string value to an int\n\n\n\n\n\nJavaScript. This is valid JS code and ends with z being a string with the content “52”"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#overview",
    "href": "sessions/06-data-types/slides.html#overview",
    "title": "An Introduction To Data Types",
    "section": "Overview",
    "text": "Overview\n\n\n\nA logical overview of the basic data types in python. From https://pynative.com/python-data-types/"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#booleans",
    "href": "sessions/06-data-types/slides.html#booleans",
    "title": "An Introduction To Data Types",
    "section": "Booleans",
    "text": "Booleans\n\nLike most programming languages, python has a bool datatype. In some ways, this is the simplest type available. A Boolean can only be True or False, and is returned when evaluating an expression. For example:\nour_result = 10&gt;9 print(our_result)\nReturns True - we’re asking Python for the result of the comparison 10&gt;9, and to store this result in a variable called our_result. The data type of a true-false comparison result like that is bool, so our variable will also be of this type.\nBooleans will become highly relevant when we talk about conditionals and program flow.\n\n\n\n\nGeorge Boole (1815-1864) - the originator of Boolean logic"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#numeric-types",
    "href": "sessions/06-data-types/slides.html#numeric-types",
    "title": "An Introduction To Data Types",
    "section": "Numeric types",
    "text": "Numeric types\nNumeric types are for variables that will only contain numbers. Other programming languages often have many different numeric types, but Python (mercifully) only has two:\nint can contain any5 whole (no fraction or decimals) number; negative, positive or zero. E.g.\n\na = -4\nb = 3\nc = 9087358292578\n\nfloat can contain any number with a decimal point, to arbitrary6 precision. E,g,\n\nx = -2.2\ny = 3.0\nz = 2452.259259999999999\n\nIf you’re manually assigning a number to a variable, python will always choose an int or float depending on whether you’ve used a decimal point or not - so 2 and 2.0 are not equivalent in this context."
  },
  {
    "objectID": "sessions/06-data-types/slides.html#data-structures",
    "href": "sessions/06-data-types/slides.html#data-structures",
    "title": "An Introduction To Data Types",
    "section": "Data structures",
    "text": "Data structures\nWith data structures, we can address an element or elements by using square bracket notation - more on this below.\nStrings (str)\nThese are similar to a VARCHAR in SQL. They are ordered sequences (strings) of characters7. Enclosed by quotation marks8. E.g.\n\nour_string = \"Hello world\"\n\nLists (list)\nAn ordered sequence of objects, where each object can be another data type (int, float, string, bool, etc). Enclosed by square brackets, and the items separated by commas. E.g.\n\nour_list = [1, 2.3, \"abc\"]\n\nDictionaries (dict)\nDictionaries are key-value pairs, where each entry is a pair of entries. Enclosed by curly braces, the keys and values separated by a colon and each pair separated by a comma. E.g.\n\nour_dict = {\"org_code\":\"0DF\",\"name\":\"SCW CSU\",\"year\": 2013}"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#other-data-types",
    "href": "sessions/06-data-types/slides.html#other-data-types",
    "title": "An Introduction To Data Types",
    "section": "Other data types",
    "text": "Other data types\nBuilt-in\n\nWe’ve skipped over complex numbers and tuples, the latter being like a dict but non-changeable.\n\nOther packages\n\nYou may have heard of other data types such as arrays (which are kind like lists but multi-dimensional).\nArrays are not a built-in Python type but are offered by the numpy package.\npandas also offers additional data types such as timestamp (similar to SQL’s datetime).\ndataframes (from pandas) are an example of a higher-order class that makes use of datatypes within it; remember from previous sessions that a dataframe can contain strings, integers, timestamps etc."
  },
  {
    "objectID": "sessions/06-data-types/slides.html#final-thoughts",
    "href": "sessions/06-data-types/slides.html#final-thoughts",
    "title": "An Introduction To Data Types",
    "section": "Final thoughts",
    "text": "Final thoughts\nDon’t worry about memorising any of this! If you take one thing away from this session, make it the fact that data types exist, that being aware of them will help you understand problems with your code, and that resources and documentation are readily available online.\n\nFurther reading\n\nhttps://docs.python.org/3/tutorial/datastructures.html\nhttps://docs.python.org/3/library/stdtypes.html\nhttps://www.geeksforgeeks.org/python-data-types/\nhttps://www.w3schools.com/python/python_datatypes.asp"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#footnotes",
    "href": "sessions/06-data-types/slides.html#footnotes",
    "title": "An Introduction To Data Types",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nexperimental ternary computers and quantum computing are firmly out of scope of this presentation\nfrom https://www.righto.com/2019/07/software-woven-into-wire-core-rope-and.html\nfrom https://www.righto.com/2019/07/software-woven-into-wire-core-rope-and.html\nfrom https://www.righto.com/2019/07/software-woven-into-wire-core-rope-and.html\nthere is no clearly-defined maximum number for an integer in python; certainly not one you’re likely to ever encounter\nagain, limits exist but aren’t relevant here\nletters, numbers, symbols, etc. - any valid UTF-8 symbols\nin most instances either double quotes (\") or single quotes (') are fine - but it’s a good idea to pick one style and be consistent."
  },
  {
    "objectID": "sessions/schedule.html",
    "href": "sessions/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This is the schedule for Code Club in FY25/26.\nThe Demonstration, Presentation, and Notebook columns indicate the content to be expected for each session:\n\nDemonstration: A live show-and-tell relating to the discussion topic.\nPresentation: A slide deck covering the discussion topic.\nNotebook: A Jupyter Notebook containing code-along elements or examples for people to work through after the session.\n\nTutorials will be divided into Modules. We recommend that people attend or watch tutorials in the Core module in order to gain a fundamental understanding of coding concepts and resources. Further modules are to be confirmed, but will likely include Automation, Dashboards and Visualisation, and Data Science. People will be able to attend those modules that interest them.\nWe have mapped the contents of the syllabus to competencies in the National Competency Framework for Data Professionals in Health and Care so that you can see which sessions will help you on your way towards them. For full details of the skills in the framework, the self-assessment tool can be found on FutureNHS.\nPlease note that this is a first draft of the mapping of NCF competencies to our syllabus and it is awaiting review.\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  Session Date\n  Module\n  Session Name\n  Description\n  Demonstration\n  Presentation\n  Notebook\n  NCF Competency\n\n\n\n  \n    22/04/2025\n    N/A\n    Nectar Re-Launch\n    Showcasing the re-launch of Code Club\n    🎬\n    💻\n    -\n    -\n  \n  \n    01/05/2025\n    On-boarding\n     Python On-boarding\n    What to install, basic virtual environment management, introduction to VS Code\n    🎬\n    -\n    -\n    SA21 : Python Proficiency\n  \n  \n    15/05/2025\n    On-boarding\n    Jupyter Notebooks\n    Demonstration of Jupyter Notebooks\n    🎬\n    -\n    📖\n    SA21 : Python Proficiency\n  \n  \n    29/05/2025\n    Exploration & Visualisation\n    EDA with Pandas\n    Introduction to exploratory data analysis using pandas\n    🎬\n    -\n    📖\n    SA4 : Descriptive and Explicative Analytics\n  \n  \n    12/06/2025\n    Exploration & Visualisation\n    Visualisation with Seaborn\n    Aesthetically-pleasing visualisations\n    🎬\n    -\n    📖\n    SA1 : Data Visualisation\n  \n  \n    03/07/2025\n    Exploration & Visualisation\n    EDA with Seaborn\n    Using seaborn to visualise and explore data\n    🎬\n    -\n    📖\n    SA4 : Descriptive and Explicative Analytics\n  \n  \n    17/07/2025\n    Core concepts\n    Data Types\n    Introduction to data types\n    -\n    💻\n    📖\n    SA21 : Python Proficiency\n  \n  \n    31/07/2025\n    Core concepts\n    Control Flow\n    Introduction to control flow\n    -\n    💻\n    📖\n    SA21 : Python Proficiency\n  \n  \n    14/08/2025\n    Core concepts\n    Functions & Functional Programming\n    Introduction to functions and functional programming\n    -\n    💻\n    📖\n    SA21 : Python Proficiency\n  \n  \n    28/08/2025\n    Core concepts\n    Object-Oriented Programming\n    Introduction to object-oriented programming\n    -\n    💻\n    📖\n    SA21 : Python Proficiency\n  \n  \n    11/09/2025\n    Exploration & Visualisation\n    Streamlit dashboards\n    How to present data (visualisations) in a Streamlit dashboard\n    🎬\n    -\n    -\n    SA1 : Data Visualisation\n  \n  \n    25/09/2025\n    Data Science\n    Comparing Samples\n    Understanding data distributions and comparisons between samples\n    -\n    💻\n    📖\n    SA4 : Descriptive and Explicative Analytics\n  \n  \n    09/10/2025\n    Data Science\n    Analysing Relationships\n    Quantifying relationships with hypothesis tests and statistical significance\n    -\n    💻\n    📖\n    SA15 : Hypothesis Testing\n  \n  \n    23/10/2025\n    Data Science\n    Introduction to Linear Regression\n    Introduction to regression concepts and the component parts of linear regression\n    -\n    💻\n    📖\n    SA7 : Advanced Statistics\n  \n  \n    06/11/2025\n    Data Science\n    Implementing Linear Regression\n    Building linear models, assessing model fit, and interpreting coefficients\n    🎬\n    💻\n    📖\n    SA7 : Advanced Statistics\n  \n  \n    20/11/2025\n    Data Science\n    Beyond Linearity\n    Introduction to generalised linear regression models\n    🎬\n    💻\n    📖\n    SA7 : Advanced Statistics",
    "crumbs": [
      "Sessions",
      "Session Schedule"
    ]
  },
  {
    "objectID": "sessions/07-control-flow/index.html",
    "href": "sessions/07-control-flow/index.html",
    "title": "An Introduction to Control Flow",
    "section": "",
    "text": "This session is the second in a series of programming fundamentals. We recognise that this content might be a bit more dry and abstract, but it is important background to know when you start to actually use Python in your day to day work.\nMuch as the flow of a stream describes how it goes from its source to its mouth, control flow describes the logical path a program is expected to take when you run it. Just as you can divert the flow of a stream with structures like dams and bridges, you can change the direction a program flows by the use of control and repetition structures. The below slides aim to provide an introduction to these concepts and the way we can use them.",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "7. Control Flow"
    ]
  },
  {
    "objectID": "sessions/07-control-flow/index.html#control-or-decision-structures",
    "href": "sessions/07-control-flow/index.html#control-or-decision-structures",
    "title": "An Introduction to Control Flow",
    "section": "Control (or Decision) Structures",
    "text": "Control (or Decision) Structures\nLike a case statement in SQL, control structures can be used to select different options and actions based on the input variable. These follow the structure:\n\nif &lt;this boolean condition is true&gt;:\n  &lt;do this&gt;\nelif &lt;this boolean condition is true&gt;:\n  &lt;do that&gt;\nelse:\n  &lt;do something else&gt;\n\nIn its most basic form, only an if clause is required. The else clause allows the bucketing of all circumstances not handled previously so that code can be applied in any given circumstance.",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "7. Control Flow"
    ]
  },
  {
    "objectID": "sessions/07-control-flow/index.html#repetition-structures-or-loops",
    "href": "sessions/07-control-flow/index.html#repetition-structures-or-loops",
    "title": "An Introduction to Control Flow",
    "section": "Repetition Structures (or Loops)",
    "text": "Repetition Structures (or Loops)\nRepetition structures allow for sections of code to be repeated until a condition is met. for loops repeat code over a set number of iterations based on an iterable condition. while loops repeat code until a predetermined condition is met.\n\nfor Loops\nBelow are two examples of for code loops. The first loops through a list called ‘providers’ and prints each item. The second loops through a range of numbers and prints each.\n\nLogical Structure\nfor &lt;i&gt; in &lt;iterable&gt;:\n    &lt;code_to_iterate&gt;\n\nfor &lt;i&gt; in range(&lt;a&gt; - &lt;b&gt;):\n  print(&lt;i&gt;)\n\n\nPython\nfor provider in providers:\n  print(provider)\n\nfor num in range(0-6):\n  print(num)\n\n\n\nwith Loops\nwhile loops check the state of a boolean condition. In this case the loop runs until a declared variable is over 5 printing each incremental value.\n\nLogical Structure\nwhile &lt;boolean is true&gt;:\n    &lt;code_to_iterate&gt;\n\n\nPython\nvar = 0\nwhile var &lt;= 5:\n  print(var)",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "7. Control Flow"
    ]
  },
  {
    "objectID": "sessions/07-control-flow/index.html#exercises",
    "href": "sessions/07-control-flow/index.html#exercises",
    "title": "An Introduction to Control Flow",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a function that prints whether a number is negative, zero, or positive.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndef classify(x):\n    if x &lt; 0:\n        print(\"Negative\")\n    elif x == 0:\n        print(\"Zero\")\n    else:\n        print(\"Positive\")\n\n\n\n\n\nLoop through a list of ages and print if each person is a Child (&lt;13), Teenager (13–17), Adult (18–64), or Senior (65+).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nages = [10, 15, 30, 70]\nfor age in ages:\n    if age &lt; 13:\n        print(\"Child\")\n    elif age &lt; 18:\n        print(\"Teenager\")\n    elif age &lt; 65:\n        print(\"Adult\")\n    else:\n        print(\"Senior\")\n\nChild\nTeenager\nAdult\nSenior\n\n\n\n\n\n\nUse a while loop to count down from 10 to 0.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx = 10\nwhile x &gt;= 0:\n    print(x)\n    x -= 1\n\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n\n\n\n\n\n\nLoop from 1 to 20 and print Fizz for multiples of 3, Buzz for 5, FizzBuzz for both.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nfor i in range(1, 21):\n    if i % 3 == 0 and i % 5 == 0:\n        print(\"FizzBuzz\")\n    elif i % 3 == 0:\n        print(\"Fizz\")\n    elif i % 5 == 0:\n        print(\"Buzz\")\n    else:\n        print(i)\n\n1\n2\nFizz\n4\nBuzz\nFizz\n7\n8\nFizz\nBuzz\n11\nFizz\n13\n14\nFizzBuzz\n16\n17\nFizz\n19\nBuzz\n\n\n\n\n\n\nUse random.randint to simulate rolling a die until you get a 6.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nimport random\nrolls = 0\nwhile True:\n    rolls += 1\n    if random.randint(1, 6) == 6:\n        break\nprint(\"Rolled a 6 in\", rolls, \"tries\")\n\nRolled a 6 in 6 tries\n\n\n\n\n\n\nLoop through job titles and print if they contain “analyst” or “manager.”\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntitles = [\"Data Analyst\", \"HR Manager\", \"Intern\"]\nfor title in titles:\n    t = title.lower()\n    if \"analyst\" in t:\n        print(\"Analyst role\")\n    elif \"manager\" in t:\n        print(\"Manager role\")\n    else:\n        print(\"Other\")\n\nAnalyst role\nManager role\nOther",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "7. Control Flow"
    ]
  },
  {
    "objectID": "sessions/09-object-oriented-programming/index.html",
    "href": "sessions/09-object-oriented-programming/index.html",
    "title": "An Introduction to Object-Oriented Programming",
    "section": "",
    "text": "This session is the last in our series on fundamental programming concepts. It is an introduction to the programming paradigm that underpins Python, making it flexible and comparatively accessible to people starting out with coding.\nWhile the concepts discussed can feel a little abstract at times, they will make ever more sense the more practical experience you have with Python. This will help you understand how the packages you use function and will hopefully lead you one day to create some sophisticated programs yourself!\nIf you want to download the Jupyter notebook so that you can run the code yourself, modify it, and generally have a play, you can download it by clicking on the “Jupyter” link under “Other Formats” over on the right-hand side below the page contents.",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "9. Object-Oriented Programming"
    ]
  },
  {
    "objectID": "sessions/09-object-oriented-programming/index.html#session-slides",
    "href": "sessions/09-object-oriented-programming/index.html#session-slides",
    "title": "An Introduction to Object-Oriented Programming",
    "section": "Session Slides",
    "text": "Session Slides\nUse the left ⬅️ and right ➡️ arrow keys to navigate through the slides below. To view in a separate tab/window, follow this link.",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "9. Object-Oriented Programming"
    ]
  },
  {
    "objectID": "sessions/09-object-oriented-programming/index.html#a-closer-look-at-some-of-the-basics",
    "href": "sessions/09-object-oriented-programming/index.html#a-closer-look-at-some-of-the-basics",
    "title": "An Introduction to Object-Oriented Programming",
    "section": "A closer look at some of the basics",
    "text": "A closer look at some of the basics\nHere is a simple demonstration of how everything, down to individual integers, is treated as an object. When we run the dir() function on an integer and print the results, we can see all of the object attributes and methods associated with it.\n\nmy_int = 5\n\nprint(dir(my_int))\n\n['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floor__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__getstate__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__lshift__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'as_integer_ratio', 'bit_count', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'is_integer', 'numerator', 'real', 'to_bytes']\n\n\nTo see which are attributes and which are methods, you can run the following code. We won’t run it here, because it produces quite a long list!\n\nfor name in dir(my_int):\n    item = getattr(my_int, name)\n    if callable(item):  # if an item is callable, i.e. you can \"do\" something with it, it is a method\n        print(f\"{name} -&gt; method\")\n    else:\n        print(f\"{name} -&gt; attribute\") # otherwise, it is an attribute\n\nMethods and attributes that are surrounded by double underscores are ones that are used “behind the scenes” to help objects behave in a certain way (this exhibits the principle of Abstraction mentioned in the presentation). The ones without are the ones that you would call directly when working with the objects.\n\nUsing methods\nBy now, you are likely to have come across operations that can be used to transform your data by tacking them onto the end of your variable name. These are methods belonging to that variable’s object.\n\n# Here we are calling the \"append\" method of the list class.\n\nicb_list = ['QRL','QNQ','QU9','QSL'] # a list object\n\nicb_list.append('QNX') # calling the method and passing a parameter\n\nprint(icb_list)\n\n['QRL', 'QNQ', 'QU9', 'QSL', 'QNX']\n\n\nIf we apply the function dir() to our variable icb_list, we can see that “append” is one of the methods belonging to the list class.\n\nprint(dir(icb_list))\n\n['__add__', '__class__', '__class_getitem__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']\n\n\n\n\n\n\n\n\nRemember\n\n\n\nFunctions precede the variable function(my_variable), whereas methods are the ones that come after the variable my_variable.method().\n\n\nFunctions are not tied to a particular object class (for example, print() can be applied to most things), while methods are bound up with particular object classes (for example, the method pop is associated with lists, but not with integers).\nThe brackets that come after methods will often contain parameters that determine how the method is applied to the variable. This can be as simple as saying what we want to append to a list, as in the above example, or they can be as complex as the hyperparameters used to fine-tune a machine learning model.\n\n\nAccessing attributes\nTo access an attribute, you simply write my_variable.attribute. Below is an example of accessing the “numerator” and “denominator” attributes of an integer. In Python, integers have these to enable consistency when working with rational numbers, i.e. fractions.\n\nprint(my_int.numerator)\nprint(my_int.denominator)\n\n5\n1\n\n\n\\(\\frac{5}{1} = 5\\)\nWhen you start working with machine learning models in Python, you will often want to access attributes such as coef_ (regression) and feature_importances_ (decision tree algorithms).\nNow let’s move onto how we create our own classes.",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "9. Object-Oriented Programming"
    ]
  },
  {
    "objectID": "sessions/09-object-oriented-programming/index.html#the-anatomy-of-a-class",
    "href": "sessions/09-object-oriented-programming/index.html#the-anatomy-of-a-class",
    "title": "An Introduction to Object-Oriented Programming",
    "section": "The Anatomy of a Class",
    "text": "The Anatomy of a Class\nLet us first of all look at a simple class to see how it is structured as a whole. Then we will look at each element in turn.\n\nclass HealthProfessional:\n\n  daily_capacity = 7.5\n\n  def __init__(self, assignment_number, division, department):\n    self.assignment_number = assignment_number                 \n    self.division = division              \n    self.department = department                              \n\n  def treat_patient(self,patient_name):\n    print(f'Health professional {self.assignment_number} treated patient {patient_name}')\n\n\nDefinition\nThe line begins the definition of the class. It starts with the class keyword, followed by the class name and a colon. By convention, classes in Python begin with a capital letter.\n\nclass HealthProfessional:\n\n\n\nClass attribute\nThe next element is a class attribute, that is to say an attribute where the value will initially be the same for all objects created from that class. All of my health professionals have a default daily_capacity of 7.5 working hours per day. Class attributes sit just below the class name with one indent. Class attributes are optional.\n\nclass HealthProfessional:\n  \n  daily_capacity = 7.5\n\n\n\nConstructor method\nNext comes the constructor method, which goes by the name of __init__. The double underscores (a so-called “dunder” method) indicate that it is a method that remains internal to the object i.e. it is not something that is accessed by the end user.\nThe constructor method defines what happens when an object instance is created (instantiated). It determines which attributes require values to be passed to the object at instantiation and can also trigger some methods to be execute automatically.\nIt is written much in the same way as defining a function, starting with the keyword def. The first argument of the function is always self and is followed by arguments representing each of the object attributes.\nBelow that, each object attribute is bound to self with the self.attribute syntax and the = attribute syntax means that the corresponding value that gets passed to the object at instantiation will be assigned to that attribute.\n\n  def __init__(self, assignment_number, division, department):\n    self.assignment_number = assignment_number                 \n    self.division = division              \n    self.department = department    \n\n\nSome quick notes on self\n\nself stores the instance of each object within the computer’s memory.\nwhenever we define a class method, we include self because we want to run this method in relation to this instance when we call it.\nwhenever we define an object attribute, we include self because we want to be able access the attribute value that belongs to this instance.\n\n\n\n\nClass method\nNow, returning to the definition of a class, we come to the class method.\nIt is again defined just like a normal function, and this time we can give it whatever name we like. The first argument is always self, which is followed by any other arguments relevant to the method. In our example, we want to pass a patient_name to the HealthProfessional object so that it knows which patient it is being asked to treat.\n\n  def treat_patient(self,patient_name):\n    print(f'Health professional {self.assignment_number} treated patient {patient_name}')\n\n\n\nOur class in action\nNow we can have a look at our HealthProfessional class in action.\nFirst of all, we need to instantiate a HealthProfessional object. We need to assign it to a variable name so that we can easily refer to the object later on.\n\ndoctor_duggee = HealthProfessional(\n  assignment_number = 12345,          # writing out the argument names is optional\n  division = \"A\",\n  department= \"Surgery\"\n)\n\nWe can use the .treat_patient method:\n\ndoctor_duggee.treat_patient(\"Betty\")\n\nHealth professional 12345 treated patient Betty\n\n\nWe can access the object’s object attributes:\n\nprint(f'Health Professional {doctor_duggee.assignment_number} works in the {doctor_duggee.department} department')\n\nHealth Professional 12345 works in the Surgery department\n\n\nAnd we can access the class attribute:\n\ndoctor_duggee.daily_capacity\n\n7.5\n\n\nIt is also very easy to update an object’s attribute values. Compare this with updating the values in a dict, which can be quite fiddly.\n\ndoctor_duggee.department = 'Medicine for Older Persons'\ndoctor_duggee.division = 'C'\n\nprint(f'Health Professional {doctor_duggee.assignment_number} works in the {doctor_duggee.department} department in Division {doctor_duggee.division}')\n\nHealth Professional 12345 works in the Medicine for Older Persons department in Division C\n\n\nClass attribute values can be updated at object level in the same way. This change won’t affect new objects created from the class. Think of class attributes as holding a common default value.\n\ndoctor_duggee.daily_capacity = 8\nprint(f'Duggee\\'s daily capacity: {doctor_duggee.daily_capacity} hours.')\n\nDuggee's daily capacity: 8 hours.",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "9. Object-Oriented Programming"
    ]
  },
  {
    "objectID": "sessions/09-object-oriented-programming/index.html#inheritance",
    "href": "sessions/09-object-oriented-programming/index.html#inheritance",
    "title": "An Introduction to Object-Oriented Programming",
    "section": "Inheritance",
    "text": "Inheritance\nOne of the main strengths of object-oriented programming is the ability to create child classes from other classes (the original, parent classes). This allows us to create classes that modify or extend the data (attributes) and behaviours (methods) of the parent class, without altering the parent class itself. Multiple child classes can be created which share the attributes and methods consistently, but extend or modify them in their own way.\nLet us have a look at how a child class gets created from the parent class. Again, we will start off with the child class written out in full so that you can see how it would look. Then we will go through it line by line.\nWe are going to create “Doctor” and “Nurse” child classes of the HealthProfessional parent class. They will inherit the attributes and methods of the HealthProfessional class, but extend them in their own way.\n\nclass Doctor(HealthProfessional):\n    def __init__(self,assignment_number,division,department,seniority):\n        self.seniority = seniority\n        super().__init__(assignment_number,division,department)\n\n    def discharge_patient(self,patient_name):\n        print(f'Doctor {self.assignment_number} discharged patient {patient_name}')\n\n\nChild class definition\nFirst of all, when defining a child class, we start off with the keyword class followed by the name of the child class, followed by the name of the parent class in brackets, ending with a colon.\n\nclass Doctor(HealthProfessional):\n\n\n\nChild class constructor method and child attributes\nThen, with one indentation, we write the constructor method, just as we did with the parent class. Remember, we must always enter self as the first argument. Then we need to enter the names of the attributes that the child class will inherit from the parent, followed by any new object attributes. In the second line, the any new object attributes are bound to self.\n\n    def __init__(self,assignment_number,division,department,seniority):\n        self.seniority = seniority\n\n\n\nThe super(). function\nBelow any new attributes, we enter a line that draws down any methods and attributes from the parent class. super() is a function that calls the constructor function from the parent class.\n\n        super().__init__(assignment_number,division,department)\n\nNote that you do not need to refer to any methods or class attributes here. They automatically get replicated by the super() method. The only elements that you need to explicitly mention are the object attributes defined in the parent class.\n\n\n\n\n\n\nRemember\n\n\n\nClass attributes are the ones that have “default” values that are common to all objects created from a class. In our HealthProfessional example, this was daily_capacity. They are written just under the class name and are not bound to self. Object attributes receive their values when an object is instantiated and are specific to that object. They are bound to self.\n\n\n\n\nChild-specific methods\nLast in our example is the definition of any new methods that are specific to the child class. Doctor objects will be able to treat patients with the inherited .treat_patient() method, but we are also going to give them the ability to discharge patients with the .discharge_patient() method.\nAs with the .treat_patient() method in the parent class, we use the def keyword, followed by the name of the method for the child class, self as the first argument, and the name of any variable that we want to pass to the method, in this case the name of the patient we want to discharge.\n\n    def discharge_patient(self,patient_name):\n        print(f'Doctor {self.assignment_number} discharged patient {patient_name}')\n\n\n\nOur child class in action\nLet’s create an object instance of the Doctor class and try out the methods and attributes.\n\ndoctor_peppa = Doctor(\n  999999,\n  \"B\",\n  \"Ophthalmology\",\n  \"Consultant\"\n)\n\n\nprint(f'Assignment number: {doctor_peppa.assignment_number}')\nprint(f'Division: {doctor_peppa.division}')\nprint(f'Department: {doctor_peppa.department}')\nprint(f'Seniority: {doctor_peppa.seniority}')\n\nAssignment number: 999999\nDivision: B\nDepartment: Ophthalmology\nSeniority: Consultant\n\n\nNote that we can still retrieve the daily_capacity attribute, which was defined in the parent class HealthProfessional\n\ndoctor_peppa.daily_capacity\n\n7.5\n\n\nHowever, we can prove that Doctor Peppa is a Doctor object and not a HealthProfessional object by using Python’s type() function\n\ntype(doctor_peppa)\n\n__main__.Doctor\n\n\nWe can also test Doctor Peppa’s methods:\n\ndoctor_peppa.treat_patient(\"Suzy Sheep\")\n\nHealth professional 999999 treated patient Suzy Sheep\n\n\n\n\n\n\n\n\nInteresting…\n\n\n\nDid you notice how it said “Health professional”? This was actually unintentional, but it neatly demonstrates how the .treat_patient() method has been inherited from the parent class (because the text of the statement wasn’t amended when the child class was created).\n\n\n\ndoctor_peppa.discharge_patient(\"Suzy Sheep\")\n\nDoctor 999999 discharged patient Suzy Sheep\n\n\nWhat happens if Doctor Duggee tries to discharge a patient? Try uncommenting the Python below and running the cell.\n\n# doctor_duggee.discharge_patient(\"Happy\")\n\nWe get an error, because doctor_duggee is a HealthProfessional object, which does not contain the .discharge_patient() method.\nIf you want to turn doctor_duggee into a Doctor object, you need to instantiate him as such.\n\ndoctor_duggee = Doctor(\n  assignment_number = 12345,\n  division = \"A\",\n  department= \"Surgery\",\n  seniority=\"Resident\"\n)\n\ntype(doctor_duggee)\n\n__main__.Doctor\n\n\n\n\nDefining a second child class\nLet’s quickly define the Nurse child class of HealthProfessional to demonstrate how different child classes can share the same inherited characteristics, but still have their own.\nInstead of seniority, Nurse objects will have band and role attributes, and instead of .discharge_patient() the method .take_readings().\n\nclass Nurse(HealthProfessional):\n    def __init__(self,assignment_number,division,department,band,role):\n        self.band = band\n        self.role = role\n        super().__init__(assignment_number,division,department)\n\n  # Adding a new method to the child class\n    def take_readings(self,patient_name):\n        print(f'Nurse {self.assignment_number} took the vital signs readings of {patient_name}')\n\n\nnurse_tag = Nurse(\n  assignment_number = 10101,\n  division = \"A\",\n  department = \"Cancer Care\",\n  band = \"5\",\n  role = \"Staff Nurse\"\n)\n\nprint(f'Assignment number:  {nurse_tag.assignment_number}')\nprint(f'Division: {nurse_tag.division}')\nprint(f'Department: {nurse_tag.department}')\nprint(f'AfC Band: {nurse_tag.band}')\nprint(f'Role: {nurse_tag.role}')\nprint(f'Daily Capacity: {nurse_tag.daily_capacity}')\n\nnurse_tag.take_readings(\"Roly\")\n\nAssignment number:  10101\nDivision: A\nDepartment: Cancer Care\nAfC Band: 5\nRole: Staff Nurse\nDaily Capacity: 7.5\nNurse 10101 took the vital signs readings of Roly\n\n\n\n\nThe isinstance() function\nThere is also a function that can check whether an object belongs to a specified class. (This is particularly useful for checking data types when a specific type needs to be enforced, given that every variable is treated as an object belonging to a particular class. This data type enforcement doesn’t happen by default in Python since it is a dynamically-typed language. Leaving this up to Python could lead to a mismatch of data types, which could lead to errors further down the line).\n\nisinstance(nurse_tag,Nurse)\n\nTrue\n\n\n\n# Imagine we wanted to ensure that AfC bands always get stored as strings\n# for consistency since they can include alphanumeric values (e.g. \"8a\") as\n# well as purely numeric values (e.g. \"6\")\n\nmy_input = 7 # this will be interpreted as an integer\n\nif isinstance(my_input,str):\n  print('Valid input')\nelse:\n  print('Invalid input: this field only accepts text strings')\n\nInvalid input: this field only accepts text strings\n\n\nThose are the fundamentals of how to create classes and objects. Let’s have a look at something a bit more advanced to give you an idea of something you might want to create for a team of analysts.",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "9. Object-Oriented Programming"
    ]
  },
  {
    "objectID": "sessions/09-object-oriented-programming/index.html#a-practical-example",
    "href": "sessions/09-object-oriented-programming/index.html#a-practical-example",
    "title": "An Introduction to Object-Oriented Programming",
    "section": "A practical example",
    "text": "A practical example\nBelow is an example of how we might create an standard SCW chart template for different kinds of charts using a class. The string and Boolean values in the constructor method are default values that can be overwritten by any values passed to the object at instantiation.\n\nimport matplotlib.pyplot as plt\n\nclass SCWPlot:\n  def __init__(self, x_data, y_data, title=\"SCW Plot\", xlabel=\"X-axis\", ylabel=\"Y-axis\", grid=False):\n    self.x_data = x_data\n    self.y_data = y_data\n    self.title = title\n    self.xlabel = xlabel\n    self.ylabel = ylabel\n    self.grid = grid\n    # Colour palette based on SCW colours\n    self.palette = ['#1C355E','#005EB8','#00A9CE','#330072','#78BE20']\n\n  # This part is a method internal to the class. It is not accessed by users, but helps to standardise\n  # the methods relating to each type of chart. By convention, these internal methods begin with an underscore.\n  def _setup_plot(self):\n    plt.figure(figsize=(8, 5))\n    plt.title(self.title)\n    plt.xlabel(self.xlabel)\n    plt.ylabel(self.ylabel)\n    plt.grid(visible=True,which='major',axis='both')\n    plt.tight_layout()\n\n  # The two methods below are each for a different kind of chart that the user can plot. They both make use of\n  # the ._setup_plot() to determine certain shared, consistent characteristics.\n  def plot_line(self, linestyle='-', marker='o'):\n    self._setup_plot()            # use the common structure defined above\n    plt.plot(                     # plot a line chart with the data passed to the object\n        self.x_data, \n        self.y_data, \n        color=self.palette[0],    # use the first value in the palette list\n        linestyle=linestyle, \n        marker=marker\n    )\n    plt.show()                    # display the chart\n\n  def plot_bar(self):\n    self._setup_plot()            # use the common structure defined above\n    plt.bar(                      # plot a bar chart with data passed to the object\n      self.x_data, \n      self.y_data, \n      color=self.palette\n    ) \n    plt.show()                    # display the chart\n\nThen we will create a very simple dataset to use for testing the use of our class.\n\nx = ['A', 'B', 'C', 'D', 'E']\ny = [5, 7, 3, 8, 6]\n\nNow we can instantiate an SCWPlot object and call the two chart type methods to produce charts.\n\nplot = SCWPlot(x, y, title=\"SCW-Branded Plot\", xlabel=\"Category\", ylabel=\"Value\")\n\nplot.plot_line()\n\n\n\n\n\n\n\n\n\nplot.plot_bar()\n\n\n\n\n\n\n\n\nAs you can see, it becomes easy for the user to switch between different kinds of plots while maintaining a standard colour scheme and chart size.\nIn reality, you would not have all of the implementation code (i.e. the code written to create the class) in your script or notebook. This would stored in a separate Python file and the class would be imported into the script. In a future session we will talk about this and creating our own packages.\n\nSimple maintenance and extensibility\nCreating standardised functions as methods of a class helps to simplify maintaining and extending functionality. In the example above, things like the colour palette and figure size are defined once and these get cascaded to the methods relating to individual chart types, so they only need to be edited once. Furthermore, it is easy for a colleague to add another chart type (e.g. horizontal bar) with four lines of code, without necessarily having to understand in detail how the rest of the class has been constructed. This example produces relatively simple charts, but there is the potential to create a class that consistently produces timeseries charts with a preferred x-axis layout (something that can be quite fiddly to perfect!).\nWhile this class can be extended by adding more chart type methods, can you think of a possible reason for creating a child class from it?",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "9. Object-Oriented Programming"
    ]
  },
  {
    "objectID": "sessions/09-object-oriented-programming/index.html#exercises",
    "href": "sessions/09-object-oriented-programming/index.html#exercises",
    "title": "An Introduction to Object-Oriented Programming",
    "section": "Exercises",
    "text": "Exercises\n\nHow do you check what class a variable is?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmy_variable = 5\n\ntype(my_variable)\n\nint\n\n\n\n\n\n\nHow would you check whether the value of the variable in the previous solution is an integer?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nisinstance(my_variable,int)\n\nTrue\n\n\n\n\n\n\nWhich function can be used to display all of the methods and attributes belonging to an object?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndir(my_variable)\n\n['__abs__',\n '__add__',\n '__and__',\n '__bool__',\n '__ceil__',\n '__class__',\n '__delattr__',\n '__dir__',\n '__divmod__',\n '__doc__',\n '__eq__',\n '__float__',\n '__floor__',\n '__floordiv__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getnewargs__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__index__',\n '__init__',\n '__init_subclass__',\n '__int__',\n '__invert__',\n '__le__',\n '__lshift__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__neg__',\n '__new__',\n '__or__',\n '__pos__',\n '__pow__',\n '__radd__',\n '__rand__',\n '__rdivmod__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rfloordiv__',\n '__rlshift__',\n '__rmod__',\n '__rmul__',\n '__ror__',\n '__round__',\n '__rpow__',\n '__rrshift__',\n '__rshift__',\n '__rsub__',\n '__rtruediv__',\n '__rxor__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__sub__',\n '__subclasshook__',\n '__truediv__',\n '__trunc__',\n '__xor__',\n 'as_integer_ratio',\n 'bit_count',\n 'bit_length',\n 'conjugate',\n 'denominator',\n 'from_bytes',\n 'imag',\n 'is_integer',\n 'numerator',\n 'real',\n 'to_bytes']\n\n\n\n\n\n\nHow do you access an object’s attributes? What is the syntax?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nobject.attribute\n\nmy_variable.numerator\n\n5\n\n\n\n\n\n\nHow do you call an object’s methods? What is the syntax?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nobject.method()\n\nmy_variable.is_integer()\n\nTrue\n\n\n\n\n\n\nCreate a class called “Patient” with the characteristics below. You may wish to refer back to previous sessions on core programming concepts for guidance.\n\n\nA class attribute called “bed_space” and assign it the value 1\nObject attributes for “patient_number” and “diagnosis”, values for which will be assigned at instantiation, and “cured” which has the default value of False as well as “admitted” with the default value of True\nA method called “get_well” which updates the “cured” attribute to True and prints a statement that returns the “patient_number” and the “diagnosis” that the patient has been cured of. (Hint: you will need to use an f-string with the attributes embedded in the statement).\nA method called “get_discharged” which checks whether the patient has been cured and if this is True, prints a statement to say that the patient has been discharged and sets “admitted” to False. Othwerwise, it prints a statement to say that the patient is not ready to be discharged.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nclass Patient:\n\n  bed_space = 1\n\n  def __init__(self,patient_number,diagnosis):\n    self.patient_number = patient_number\n    self.diagnosis = diagnosis\n    self.cured = False\n    self.admitted = True\n\n  def get_well(self):\n    self.cured = True\n    print(f'Patient {self.patient_number} has been cured of {self.diagnosis}')\n  \n  def get_discharged(self):\n    if self.cured == True:\n      print(f'Patient {self.patient_number} has been discharged')\n      self.admitted = False\n    else:\n      print(f'Patient {self.patient_number} is not ready to be discharged')\n\n\n\n\n\nInstantiate a Patient object with a “patient_number” and “diagnosis”. Try discharging the patient before they have been cured, then call the get_well method and try discharging the patient again.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmy_patient = Patient(patient_number='12345',diagnosis='acute tummy ache')\n\nmy_patient.get_discharged()\n\nmy_patient.get_well()\n\nmy_patient.get_discharged()\n\nPatient 12345 is not ready to be discharged\nPatient 12345 has been cured of acute tummy ache\nPatient 12345 has been discharged\n\n\n\n\n\n\nWith the Patient object you created, how would you access the “admitted” attribute to check whether they are still an inpatient?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmy_patient.admitted\n\nFalse\n\n\n\n\n\n\nCreate a child class called “SurgeryPatient” that inherits the methods and attributes from Patient. Add an attribute called “theatre” to store the name of the operating theatre that gets assigned when the object is created and a method that checks whether the patient has been cured or not. When False, the method prints a message requesting that the patient be transferred to the theatre, when True the method prints a message to say that the patient can proceed to discharge.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nclass SurgeryPatient(Patient):\n  def __init__(self, patient_number, diagnosis, theatre):\n    self.theatre = theatre\n    super().__init__(patient_number, diagnosis)\n\n  def theatre_call(self):\n    if self.cured == False:\n      print(f'Please transfer patient {self.patient_number} to theatre {self.theatre}')\n    else:\n      print(f'Patient {self.patient_number} is now ready for discharge')\n\n\n\n\n\nInstantiate a SurgeryPatient object, test whether the attributes and methods have been inherited from the Patient class, and test the .theatre_call() method. Your solution doesn’t have to match exactly. Just be sure that you are able to test each element of the class.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmy_surgery_patient = SurgeryPatient(patient_number=54321,diagnosis='a broken leg',theatre='11F')\n\nprint(f'This patient occupies {my_surgery_patient.bed_space} bed')\n\nmy_surgery_patient.get_discharged()\n\nmy_surgery_patient.theatre_call()\n\nmy_surgery_patient.get_well()\n\nmy_surgery_patient.theatre_call()\n\nmy_surgery_patient.get_discharged()\n\nprint(f'Is this patient still on our inpatient ward? {my_surgery_patient.admitted}')\n\nThis patient occupies 1 bed\nPatient 54321 is not ready to be discharged\nPlease transfer patient 54321 to theatre 11F\nPatient 54321 has been cured of a broken leg\nPatient 54321 is now ready for discharge\nPatient 54321 has been discharged\nIs this patient still on our inpatient ward? False",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "9. Object-Oriented Programming"
    ]
  },
  {
    "objectID": "sessions/09-object-oriented-programming/index.html#bonus-challenge",
    "href": "sessions/09-object-oriented-programming/index.html#bonus-challenge",
    "title": "An Introduction to Object-Oriented Programming",
    "section": "Bonus challenge",
    "text": "Bonus challenge\n\n\n\n\n\n\nWe haven’t provided a suggested solution for this one, but do feel free to discuss potential answers in the Code Club channels.\n\n\n\nInstantiate a smallish number of Patient objects (no need to spend too much time on this, but you do need more than one!). Can you work out a way to total the amount of bed space occupied by your patients by summing together the bed_space class attribute values? Try using a loop.",
    "crumbs": [
      "Sessions",
      "Core Concepts",
      "9. Object-Oriented Programming"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html",
    "href": "sessions/03-eda-pandas/index.html",
    "title": "Exploring Data Using Pandas",
    "section": "",
    "text": "This is the first of four sessions looking at how to explore data in Python. This session will focus on introducing the Python library, pandas. We will use pandas to import, inspect, summarise, and transform the data, illustrating a typical exploratory data analysis workflow.\nWe are using Australian weather data, taken from Kaggle. This dataset is used to build machine learning models that predict whether it will rain tomorrow, using data about the weather every day from 2007 to 2017. To download the data, click here.\n# install necessary packages\n# !uv add skimpy\n\n# import packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom skimpy import skim\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#setting-the-scene",
    "href": "sessions/03-eda-pandas/index.html#setting-the-scene",
    "title": "Exploring Data Using Pandas",
    "section": "Setting the Scene",
    "text": "Setting the Scene\nBefore we start to explore any dataset, we need to establish what we are looking to do with the data. This should inform our decisions wwith any exploration, and any analysis that follows.\nQuestions:\n\nWhat are we trying to achieve?\nHow do our goals impact our analysis?\nWhat should we take into consideration before we write any code?\nWhat sort of questions might we be interested in with this dataset?\n\n\nWhat Our Data Can Tell Us (And What it Can’t)\nWe also need to consider what the data is and where it came from.\nQuestions:\n\nHow was the data collected?\nWhat is it missing?\nWhat do the variables in our dataset actually mean, and are they a good approximation of the concepts we are interested in?",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#exploring-the-dataset",
    "href": "sessions/03-eda-pandas/index.html#exploring-the-dataset",
    "title": "Exploring Data Using Pandas",
    "section": "Exploring the Dataset",
    "text": "Exploring the Dataset\nFirst, we should start with dataset-wide operations.\nQuestions:\n\nWhat do we want to know about a dataset when we first encounter it?\nHow do we get a quick overview of the data that can help us in our next steps?\nWe need to get a “feel” for the data before we can really make any decisions about how to analyse it. How do we get there with a new dataset?\n\nWe can start by getting a quick glance at the data. The starting point when you have just imported a new dataset is usually the pandas function pd.DataFrame.head(), which shows the top \\(n\\) rows of the dataset (by default it shows the top five rows).\n\n# view the top five rows\ndf.head()\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n0\n2008-12-01\nAlbury\n13.4\n22.9\n0.6\nNaN\nNaN\nW\n44.0\nW\n...\n71.0\n22.0\n1007.7\n1007.1\n8.0\nNaN\n16.9\n21.8\nNo\nNo\n\n\n1\n2008-12-02\nAlbury\n7.4\n25.1\n0.0\nNaN\nNaN\nWNW\n44.0\nNNW\n...\n44.0\n25.0\n1010.6\n1007.8\nNaN\nNaN\n17.2\n24.3\nNo\nNo\n\n\n2\n2008-12-03\nAlbury\n12.9\n25.7\n0.0\nNaN\nNaN\nWSW\n46.0\nW\n...\n38.0\n30.0\n1007.6\n1008.7\nNaN\n2.0\n21.0\n23.2\nNo\nNo\n\n\n3\n2008-12-04\nAlbury\n9.2\n28.0\n0.0\nNaN\nNaN\nNE\n24.0\nSE\n...\n45.0\n16.0\n1017.6\n1012.8\nNaN\nNaN\n18.1\n26.5\nNo\nNo\n\n\n4\n2008-12-05\nAlbury\n17.5\n32.3\n1.0\nNaN\nNaN\nW\n41.0\nENE\n...\n82.0\n33.0\n1010.8\n1006.0\n7.0\n8.0\n17.8\n29.7\nNo\nNo\n\n\n\n\n5 rows × 23 columns\n\n\n\nYou can also look at the bottom rows of the dataset, using pd.DataFrame.tail(). This might be useful if you are dealing with time-series data. Below, we specify that we want to look at the bottom ten rows.\n\n# view the bottom ten rows\ndf.tail(10)\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n145450\n2017-06-16\nUluru\n5.2\n24.3\n0.0\nNaN\nNaN\nE\n24.0\nSE\n...\n53.0\n24.0\n1023.8\n1020.0\nNaN\nNaN\n12.3\n23.3\nNo\nNo\n\n\n145451\n2017-06-17\nUluru\n6.4\n23.4\n0.0\nNaN\nNaN\nESE\n31.0\nS\n...\n53.0\n25.0\n1025.8\n1023.0\nNaN\nNaN\n11.2\n23.1\nNo\nNo\n\n\n145452\n2017-06-18\nUluru\n8.0\n20.7\n0.0\nNaN\nNaN\nESE\n41.0\nSE\n...\n56.0\n32.0\n1028.1\n1024.3\nNaN\n7.0\n11.6\n20.0\nNo\nNo\n\n\n145453\n2017-06-19\nUluru\n7.4\n20.6\n0.0\nNaN\nNaN\nE\n35.0\nESE\n...\n63.0\n33.0\n1027.2\n1023.3\nNaN\nNaN\n11.0\n20.3\nNo\nNo\n\n\n145454\n2017-06-20\nUluru\n3.5\n21.8\n0.0\nNaN\nNaN\nE\n31.0\nESE\n...\n59.0\n27.0\n1024.7\n1021.2\nNaN\nNaN\n9.4\n20.9\nNo\nNo\n\n\n145455\n2017-06-21\nUluru\n2.8\n23.4\n0.0\nNaN\nNaN\nE\n31.0\nSE\n...\n51.0\n24.0\n1024.6\n1020.3\nNaN\nNaN\n10.1\n22.4\nNo\nNo\n\n\n145456\n2017-06-22\nUluru\n3.6\n25.3\n0.0\nNaN\nNaN\nNNW\n22.0\nSE\n...\n56.0\n21.0\n1023.5\n1019.1\nNaN\nNaN\n10.9\n24.5\nNo\nNo\n\n\n145457\n2017-06-23\nUluru\n5.4\n26.9\n0.0\nNaN\nNaN\nN\n37.0\nSE\n...\n53.0\n24.0\n1021.0\n1016.8\nNaN\nNaN\n12.5\n26.1\nNo\nNo\n\n\n145458\n2017-06-24\nUluru\n7.8\n27.0\n0.0\nNaN\nNaN\nSE\n28.0\nSSE\n...\n51.0\n24.0\n1019.4\n1016.5\n3.0\n2.0\n15.1\n26.0\nNo\nNo\n\n\n145459\n2017-06-25\nUluru\n14.9\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nESE\n...\n62.0\n36.0\n1020.2\n1017.9\n8.0\n8.0\n15.0\n20.9\nNo\nNaN\n\n\n\n\n10 rows × 23 columns\n\n\n\nA quick glimpse at the data is useful, but we may also want to get quick descriptions of several aspects of the data. Such as the length of the dataset (len(), which can also be used to get the length of various Python objects), which tells us how many observations we have.\n\n# get the object length\nlen(df)\n\n145460\n\n\nAnother option is pd.DataFrame.shape(), which shows the length (number of rows) and width (number of columns).\n\n# get the object shape (number of rows, number of columns)\ndf.shape\n\n(145460, 23)\n\n\nSpeaking of columns, if we want a quick list of the column names, we can get this using pd.DataFrame.columns().\n\n# get all column names\ndf.columns\n\nIndex(['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation',\n       'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm',\n       'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',\n       'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am',\n       'Temp3pm', 'RainToday', 'RainTomorrow'],\n      dtype='object')\n\n\nA quick and easy way to get some valuable information about the dataset is pd.DataFrame.info(), including the total non-null observations and data type1 of each column.\n\n# get dataframe info (column indices, non-null counts, data types)\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 145460 entries, 0 to 145459\nData columns (total 23 columns):\n #   Column         Non-Null Count   Dtype  \n---  ------         --------------   -----  \n 0   Date           145460 non-null  object \n 1   Location       145460 non-null  object \n 2   MinTemp        143975 non-null  float64\n 3   MaxTemp        144199 non-null  float64\n 4   Rainfall       142199 non-null  float64\n 5   Evaporation    82670 non-null   float64\n 6   Sunshine       75625 non-null   float64\n 7   WindGustDir    135134 non-null  object \n 8   WindGustSpeed  135197 non-null  float64\n 9   WindDir9am     134894 non-null  object \n 10  WindDir3pm     141232 non-null  object \n 11  WindSpeed9am   143693 non-null  float64\n 12  WindSpeed3pm   142398 non-null  float64\n 13  Humidity9am    142806 non-null  float64\n 14  Humidity3pm    140953 non-null  float64\n 15  Pressure9am    130395 non-null  float64\n 16  Pressure3pm    130432 non-null  float64\n 17  Cloud9am       89572 non-null   float64\n 18  Cloud3pm       86102 non-null   float64\n 19  Temp9am        143693 non-null  float64\n 20  Temp3pm        141851 non-null  float64\n 21  RainToday      142199 non-null  object \n 22  RainTomorrow   142193 non-null  object \ndtypes: float64(16), object(7)\nmemory usage: 25.5+ MB\n\n\nIf we wanted to get a better sense of the null values in each column, we could calculate the percentage of null values by capturing whether each row of each column is null (pd.DataFrame.isnull()), summing the total null values in each column (pd.DataFrame.sum()), and then dividing by the length of the dataframe (/len()).\n\n# calculate the percentage of null values in each column\ndf.isnull().sum()/len(df)\n\nDate             0.000000\nLocation         0.000000\nMinTemp          0.010209\nMaxTemp          0.008669\nRainfall         0.022419\nEvaporation      0.431665\nSunshine         0.480098\nWindGustDir      0.070989\nWindGustSpeed    0.070555\nWindDir9am       0.072639\nWindDir3pm       0.029066\nWindSpeed9am     0.012148\nWindSpeed3pm     0.021050\nHumidity9am      0.018246\nHumidity3pm      0.030984\nPressure9am      0.103568\nPressure3pm      0.103314\nCloud9am         0.384216\nCloud3pm         0.408071\nTemp9am          0.012148\nTemp3pm          0.024811\nRainToday        0.022419\nRainTomorrow     0.022460\ndtype: float64\n\n\nIf we want a quick summary of all the numeric columns in the dataset, we can use pd.DataFrame.describe().\n\n# quick summary of numeric variables\ndf.describe()\n\n\n\n\n\n\n\n\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustSpeed\nWindSpeed9am\nWindSpeed3pm\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\n\n\n\n\ncount\n143975.000000\n144199.000000\n142199.000000\n82670.000000\n75625.000000\n135197.000000\n143693.000000\n142398.000000\n142806.000000\n140953.000000\n130395.00000\n130432.000000\n89572.000000\n86102.000000\n143693.000000\n141851.00000\n\n\nmean\n12.194034\n23.221348\n2.360918\n5.468232\n7.611178\n40.035230\n14.043426\n18.662657\n68.880831\n51.539116\n1017.64994\n1015.255889\n4.447461\n4.509930\n16.990631\n21.68339\n\n\nstd\n6.398495\n7.119049\n8.478060\n4.193704\n3.785483\n13.607062\n8.915375\n8.809800\n19.029164\n20.795902\n7.10653\n7.037414\n2.887159\n2.720357\n6.488753\n6.93665\n\n\nmin\n-8.500000\n-4.800000\n0.000000\n0.000000\n0.000000\n6.000000\n0.000000\n0.000000\n0.000000\n0.000000\n980.50000\n977.100000\n0.000000\n0.000000\n-7.200000\n-5.40000\n\n\n25%\n7.600000\n17.900000\n0.000000\n2.600000\n4.800000\n31.000000\n7.000000\n13.000000\n57.000000\n37.000000\n1012.90000\n1010.400000\n1.000000\n2.000000\n12.300000\n16.60000\n\n\n50%\n12.000000\n22.600000\n0.000000\n4.800000\n8.400000\n39.000000\n13.000000\n19.000000\n70.000000\n52.000000\n1017.60000\n1015.200000\n5.000000\n5.000000\n16.700000\n21.10000\n\n\n75%\n16.900000\n28.200000\n0.800000\n7.400000\n10.600000\n48.000000\n19.000000\n24.000000\n83.000000\n66.000000\n1022.40000\n1020.000000\n7.000000\n7.000000\n21.600000\n26.40000\n\n\nmax\n33.900000\n48.100000\n371.000000\n145.000000\n14.500000\n135.000000\n130.000000\n87.000000\n100.000000\n100.000000\n1041.00000\n1039.600000\n9.000000\n9.000000\n40.200000\n46.70000\n\n\n\n\n\n\n\nHowever, I prefer to bring in another package, skimpy, that does all of this very quickly and cleanly. We can get a detailed description of the entire dataset using skim().\n\n# a more informative summary function from the skimpy package\nskim(df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ Dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 145460 │ │ float64     │ 16    │                                                          │\n│ │ Number of columns │ 23     │ │ string      │ 7     │                                                          │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━━━┓  │\n│ ┃ column         ┃ NA     ┃ NA %                ┃ mean  ┃ sd    ┃ p0    ┃ p25  ┃ p50  ┃ p75  ┃ p100 ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━━━┩  │\n│ │ MinTemp        │   1485 │  1.0208992162793895 │ 12.19 │ 6.398 │  -8.5 │  7.6 │   12 │ 16.9 │ 33.9 │  ▃▇▇▃  │  │\n│ │ MaxTemp        │   1261 │  0.8669049910628351 │ 23.22 │ 7.119 │  -4.8 │ 17.9 │ 22.6 │ 28.2 │ 48.1 │  ▁▇▇▃  │  │\n│ │ Rainfall       │   3261 │   2.241853430496356 │ 2.361 │ 8.478 │     0 │    0 │    0 │  0.8 │  371 │   ▇    │  │\n│ │ Evaporation    │  62790 │    43.1665062560154 │ 5.468 │ 4.194 │     0 │  2.6 │  4.8 │  7.4 │  145 │   ▇    │  │\n│ │ Sunshine       │  69835 │   48.00976213391998 │ 7.611 │ 3.785 │     0 │  4.8 │  8.4 │ 10.6 │ 14.5 │ ▃▃▅▆▇▃ │  │\n│ │ WindGustSpeed  │  10263 │   7.055547916953114 │ 40.04 │ 13.61 │     6 │   31 │   39 │   48 │  135 │  ▂▇▂   │  │\n│ │ WindSpeed9am   │   1767 │   1.214766946239516 │ 14.04 │ 8.915 │     0 │    7 │   13 │   19 │  130 │   ▇▂   │  │\n│ │ WindSpeed3pm   │   3062 │   2.105046060772721 │ 18.66 │  8.81 │     0 │   13 │   19 │   24 │   87 │  ▅▇▂   │  │\n│ │ Humidity9am    │   2654 │  1.8245565791282827 │ 68.88 │ 19.03 │     0 │   57 │   70 │   83 │  100 │  ▁▂▇▇▆ │  │\n│ │ Humidity3pm    │   4507 │    3.09844630826344 │ 51.54 │  20.8 │     0 │   37 │   52 │   66 │  100 │ ▁▅▆▇▅▂ │  │\n│ │ Pressure9am    │  15065 │     10.356799120033 │  1018 │ 7.107 │ 980.5 │ 1013 │ 1018 │ 1022 │ 1041 │   ▂▇▅  │  │\n│ │ Pressure3pm    │  15028 │  10.331362573903478 │  1015 │ 7.037 │ 977.1 │ 1010 │ 1015 │ 1020 │ 1040 │   ▂▇▅  │  │\n│ │ Cloud9am       │  55888 │   38.42155919153032 │ 4.447 │ 2.887 │     0 │    1 │    5 │    7 │    9 │ ▇▂▃▂▇▅ │  │\n│ │ Cloud3pm       │  59358 │   40.80709473394748 │  4.51 │  2.72 │     0 │    2 │    5 │    7 │    9 │ ▆▂▃▂▇▃ │  │\n│ │ Temp9am        │   1767 │   1.214766946239516 │ 16.99 │ 6.489 │  -7.2 │ 12.3 │ 16.7 │ 21.6 │ 40.2 │  ▂▇▇▃  │  │\n│ │ Temp3pm        │   3609 │  2.4810944589577892 │ 21.68 │ 6.937 │  -5.4 │ 16.6 │ 21.1 │ 26.4 │ 46.7 │  ▁▇▇▃  │  │\n│ └────────────────┴────────┴─────────────────────┴───────┴───────┴───────┴──────┴──────┴──────┴──────┴────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┓  │\n│ ┃          ┃       ┃          ┃          ┃          ┃          ┃          ┃ chars per ┃ words    ┃ total     ┃  │\n│ ┃ column   ┃ NA    ┃ NA %     ┃ shortest ┃ longest  ┃ min      ┃ max      ┃ row       ┃ per row  ┃ words     ┃  │\n│ ┡━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━┩  │\n│ │ Date     │     0 │        0 │ 2008-12- │ 2008-12- │ 2007-11- │ 2017-06- │        10 │        1 │    145460 │  │\n│ │          │       │          │ 01       │ 01       │ 01       │ 25       │           │          │           │  │\n│ │ Location │     0 │        0 │ Sale     │ Melbourn │ Adelaide │ Woomera  │      8.71 │        1 │    145460 │  │\n│ │          │       │          │          │ eAirport │          │          │           │          │           │  │\n│ │ WindGust │ 10326 │ 7.098858 │ W        │ WNW      │ E        │ WSW      │      2.19 │     0.93 │    135134 │  │\n│ │ Dir      │       │ 79279527 │          │          │          │          │           │          │           │  │\n│ │ WindDir9 │ 10566 │ 7.263852 │ W        │ NNW      │ E        │ WSW      │      2.18 │     0.93 │    134894 │  │\n│ │ am       │       │ 60552729 │          │          │          │          │           │          │           │  │\n│ │          │       │        2 │          │          │          │          │           │          │           │  │\n│ │ WindDir3 │  4228 │ 2.906641 │ E        │ WNW      │ E        │ WSW      │      2.21 │     0.97 │    141232 │  │\n│ │ pm       │       │ 00096246 │          │          │          │          │           │          │           │  │\n│ │          │       │        4 │          │          │          │          │           │          │           │  │\n│ │ RainToda │  3261 │ 2.241853 │ No       │ Yes      │ No       │ Yes      │      2.22 │     0.98 │    142199 │  │\n│ │ y        │       │ 43049635 │          │          │          │          │           │          │           │  │\n│ │          │       │        6 │          │          │          │          │           │          │           │  │\n│ │ RainTomo │  3267 │ 2.245978 │ No       │ Yes      │ No       │ Yes      │      2.22 │     0.98 │    142193 │  │\n│ │ rrow     │       │ 27581465 │          │          │          │          │           │          │           │  │\n│ │          │       │        7 │          │          │          │          │           │          │           │  │\n│ └──────────┴───────┴──────────┴──────────┴──────────┴──────────┴──────────┴───────────┴──────────┴───────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#exploring-variables-columns-observations-rows",
    "href": "sessions/03-eda-pandas/index.html#exploring-variables-columns-observations-rows",
    "title": "Exploring Data Using Pandas",
    "section": "Exploring Variables (Columns) & Observations (Rows)",
    "text": "Exploring Variables (Columns) & Observations (Rows)\nIf we are going to narrow our focus to specific variables or groups of observations, we need to know how to select columns, filter values, and group the data. There are lots of different ways we can slice up the data. We won’t cover all of them here2, but we will try to cover a range that helps illustrate how pandas works and will help you build the intuition for working with data in pandas.\nWe can select columns in a variety of ways, but the “correct” way to select columns in most circumstances is using selection brackets (the square brackets []), also known as the indexing operator.\n\n# selecting a single column by name\ndf['Date']\n\n# alternative ways to select columns\n# df.loc[:, 'Date']\n# df.Date\n\n0         2008-12-01\n1         2008-12-02\n2         2008-12-03\n3         2008-12-04\n4         2008-12-05\n             ...    \n145455    2017-06-21\n145456    2017-06-22\n145457    2017-06-23\n145458    2017-06-24\n145459    2017-06-25\nName: Date, Length: 145460, dtype: object\n\n\nIf we want to select multiple columns, we can use double squared brackets ([[ ]]). This is the same process as before, but the inner brackets define a list, and the outer are the selection brackets.\n\n# selecting multiple columns (and all rows) by name\ndf[['Date', 'Location', 'Rainfall']]\n# df.loc[:, ['Date', 'Location', 'Rainfall']]\n\n\n\n\n\n\n\n\nDate\nLocation\nRainfall\n\n\n\n\n0\n2008-12-01\nAlbury\n0.6\n\n\n1\n2008-12-02\nAlbury\n0.0\n\n\n2\n2008-12-03\nAlbury\n0.0\n\n\n3\n2008-12-04\nAlbury\n0.0\n\n\n4\n2008-12-05\nAlbury\n1.0\n\n\n...\n...\n...\n...\n\n\n145455\n2017-06-21\nUluru\n0.0\n\n\n145456\n2017-06-22\nUluru\n0.0\n\n\n145457\n2017-06-23\nUluru\n0.0\n\n\n145458\n2017-06-24\nUluru\n0.0\n\n\n145459\n2017-06-25\nUluru\n0.0\n\n\n\n\n145460 rows × 3 columns\n\n\n\nWhile selection brackets are a quick and easy solution if we want to grab a subset of variables in the dataset, it is realy only intended to be used for simple operations using only column selection.\nFor row selection, we should use pd.DataFrame.iloc[]. The iloc function is used for “integer position” selection, which means you can select rows or columns using their integer position. For rows 10-15, you can select them using the following:\n\n# slicing by rows\ndf.iloc[10:16]\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n10\n2008-12-11\nAlbury\n13.4\n30.4\n0.0\nNaN\nNaN\nN\n30.0\nSSE\n...\n48.0\n22.0\n1011.8\n1008.7\nNaN\nNaN\n20.4\n28.8\nNo\nYes\n\n\n11\n2008-12-12\nAlbury\n15.9\n21.7\n2.2\nNaN\nNaN\nNNE\n31.0\nNE\n...\n89.0\n91.0\n1010.5\n1004.2\n8.0\n8.0\n15.9\n17.0\nYes\nYes\n\n\n12\n2008-12-13\nAlbury\n15.9\n18.6\n15.6\nNaN\nNaN\nW\n61.0\nNNW\n...\n76.0\n93.0\n994.3\n993.0\n8.0\n8.0\n17.4\n15.8\nYes\nYes\n\n\n13\n2008-12-14\nAlbury\n12.6\n21.0\n3.6\nNaN\nNaN\nSW\n44.0\nW\n...\n65.0\n43.0\n1001.2\n1001.8\nNaN\n7.0\n15.8\n19.8\nYes\nNo\n\n\n14\n2008-12-15\nAlbury\n8.4\n24.6\n0.0\nNaN\nNaN\nNaN\nNaN\nS\n...\n57.0\n32.0\n1009.7\n1008.7\nNaN\nNaN\n15.9\n23.5\nNo\nNaN\n\n\n15\n2008-12-16\nAlbury\n9.8\n27.7\nNaN\nNaN\nNaN\nWNW\n50.0\nNaN\n...\n50.0\n28.0\n1013.4\n1010.3\n0.0\nNaN\n17.3\n26.2\nNaN\nNo\n\n\n\n\n6 rows × 23 columns\n\n\n\nWe can do similar using a column’s integer position, but we have to select all rows (:) first:\n\n# using iloc with columns\ndf.iloc[:, 20]\n\n0         21.8\n1         24.3\n2         23.2\n3         26.5\n4         29.7\n          ... \n145455    22.4\n145456    24.5\n145457    26.1\n145458    26.0\n145459    20.9\nName: Temp3pm, Length: 145460, dtype: float64\n\n\nFinally, we can put both together to take a subset of both rows and columns:\n\n# using iloc with rows and columns\ndf.iloc[10:16, 20]\n\n10    28.8\n11    17.0\n12    15.8\n13    19.8\n14    23.5\n15    26.2\nName: Temp3pm, dtype: float64\n\n\nHowever, selecting by integer position is relatively limited. It is more likely we would want to subset the data based on the values of certain columns. We can filter rows by condition using pd.DataFrame.loc[]. The loc function slices by label, instead of integer position.\nFor example, we might want to look at a subset of the data based on location.\n\n# select all observations in Perth\ndf.loc[df['Location'] == 'Perth']\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n120638\n2008-07-01\nPerth\n2.7\n18.8\n0.0\n0.8\n9.1\nENE\n20.0\nNaN\n...\n97.0\n53.0\n1027.6\n1024.5\n2.0\n3.0\n8.5\n18.1\nNo\nNo\n\n\n120639\n2008-07-02\nPerth\n6.4\n20.7\n0.0\n1.8\n7.0\nNE\n22.0\nESE\n...\n80.0\n39.0\n1024.1\n1019.0\n0.0\n6.0\n11.1\n19.7\nNo\nNo\n\n\n120640\n2008-07-03\nPerth\n6.5\n19.9\n0.4\n2.2\n7.3\nNE\n31.0\nNaN\n...\n84.0\n71.0\n1016.8\n1015.6\n1.0\n3.0\n12.1\n17.7\nNo\nYes\n\n\n120641\n2008-07-04\nPerth\n9.5\n19.2\n1.8\n1.2\n4.7\nW\n26.0\nNNE\n...\n93.0\n73.0\n1019.3\n1018.4\n6.0\n6.0\n13.2\n17.7\nYes\nYes\n\n\n120642\n2008-07-05\nPerth\n9.5\n16.4\n1.8\n1.4\n4.9\nWSW\n44.0\nW\n...\n69.0\n57.0\n1020.4\n1022.1\n7.0\n5.0\n15.9\n16.0\nYes\nYes\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n123826\n2017-06-21\nPerth\n10.3\n19.9\n0.2\n1.8\n7.5\nNW\n37.0\nNNE\n...\n89.0\n60.0\n1017.1\n1013.8\n5.0\n6.0\n13.0\n18.5\nNo\nYes\n\n\n123827\n2017-06-22\nPerth\n13.0\n16.8\n61.2\n3.6\n0.0\nSSW\n46.0\nW\n...\n90.0\n75.0\n1005.6\n1008.9\n7.0\n7.0\n16.4\n15.6\nYes\nNo\n\n\n123828\n2017-06-23\nPerth\n13.3\n18.9\n0.4\n1.8\n6.5\nSE\n37.0\nSE\n...\n85.0\n65.0\n1019.2\n1019.4\n6.0\n6.0\n15.1\n18.0\nNo\nNo\n\n\n123829\n2017-06-24\nPerth\n11.5\n18.2\n0.0\n3.8\n9.3\nSE\n30.0\nESE\n...\n62.0\n47.0\n1025.9\n1023.4\n1.0\n3.0\n14.0\n17.6\nNo\nNo\n\n\n123830\n2017-06-25\nPerth\n6.3\n17.0\n0.0\n1.6\n7.9\nE\n26.0\nSE\n...\n75.0\n49.0\n1028.6\n1026.0\n1.0\n3.0\n11.5\n15.6\nNo\nNo\n\n\n\n\n3193 rows × 23 columns\n\n\n\nWe can also filter by multiple values, such as location and rainfall.\n\ndf.loc[(df['Rainfall'] == 0) & (df['Location'] == 'Perth')]\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n120638\n2008-07-01\nPerth\n2.7\n18.8\n0.0\n0.8\n9.1\nENE\n20.0\nNaN\n...\n97.0\n53.0\n1027.6\n1024.5\n2.0\n3.0\n8.5\n18.1\nNo\nNo\n\n\n120639\n2008-07-02\nPerth\n6.4\n20.7\n0.0\n1.8\n7.0\nNE\n22.0\nESE\n...\n80.0\n39.0\n1024.1\n1019.0\n0.0\n6.0\n11.1\n19.7\nNo\nNo\n\n\n120644\n2008-07-07\nPerth\n0.7\n18.3\n0.0\n0.8\n9.3\nN\n37.0\nNE\n...\n72.0\n36.0\n1028.9\n1024.2\n1.0\n5.0\n8.7\n17.9\nNo\nNo\n\n\n120645\n2008-07-08\nPerth\n3.2\n20.4\n0.0\n1.4\n6.9\nNNW\n24.0\nNE\n...\n58.0\n42.0\n1023.9\n1021.1\n6.0\n5.0\n10.2\n19.3\nNo\nYes\n\n\n120651\n2008-07-14\nPerth\n7.9\n19.7\n0.0\n0.2\n6.5\nNE\n31.0\nNE\n...\n86.0\n41.0\n1026.0\n1021.9\n6.0\n5.0\n11.7\n18.7\nNo\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n123823\n2017-06-18\nPerth\n7.5\n23.4\n0.0\n1.8\n9.2\nNNE\n28.0\nENE\n...\n67.0\n41.0\n1026.9\n1022.9\n0.0\n0.0\n14.2\n22.2\nNo\nNo\n\n\n123824\n2017-06-19\nPerth\n5.5\n23.0\n0.0\n3.0\n9.1\nSW\n19.0\nENE\n...\n84.0\n55.0\n1023.0\n1020.3\n1.0\n2.0\n11.5\n22.0\nNo\nNo\n\n\n123825\n2017-06-20\nPerth\n7.8\n22.5\n0.0\n2.8\n9.1\nNW\n26.0\nW\n...\n98.0\n59.0\n1019.3\n1015.9\n1.0\n1.0\n13.5\n21.6\nNo\nNo\n\n\n123829\n2017-06-24\nPerth\n11.5\n18.2\n0.0\n3.8\n9.3\nSE\n30.0\nESE\n...\n62.0\n47.0\n1025.9\n1023.4\n1.0\n3.0\n14.0\n17.6\nNo\nNo\n\n\n123830\n2017-06-25\nPerth\n6.3\n17.0\n0.0\n1.6\n7.9\nE\n26.0\nSE\n...\n75.0\n49.0\n1028.6\n1026.0\n1.0\n3.0\n11.5\n15.6\nNo\nNo\n\n\n\n\n2293 rows × 23 columns\n\n\n\nFor any complex process for subsetting the data, including multiple conditions, pd.DataFrame.loc[] is the best bet.\n\nSummarising Data\nNow that we know how to select the variables or observations we are interested in, we can start doing some descriptive analysis. The operations we use will depend on the questions we are trying to answer, and the possibilities will be almost endless.\nQuestions:\n\nWhat “functions” might we need to carry out on our data when we are exploring it?\n\nWe know that the weather data includes observations from all over the country, but we might want to check exactly how many different locations there are. We can use pd.DataFrame.nunique() to do this.\n\n# count unique values\ndf['Location'].nunique()\n\n49\n\n\nWe may also be interested in the locations themselves, which may tell us more about the spatial distribution of our data. In this case, we can use pd.DataFrame.unique().\n\n# get unique values\ndf['Location'].unique()\n\narray(['Albury', 'BadgerysCreek', 'Cobar', 'CoffsHarbour', 'Moree',\n       'Newcastle', 'NorahHead', 'NorfolkIsland', 'Penrith', 'Richmond',\n       'Sydney', 'SydneyAirport', 'WaggaWagga', 'Williamtown',\n       'Wollongong', 'Canberra', 'Tuggeranong', 'MountGinini', 'Ballarat',\n       'Bendigo', 'Sale', 'MelbourneAirport', 'Melbourne', 'Mildura',\n       'Nhil', 'Portland', 'Watsonia', 'Dartmoor', 'Brisbane', 'Cairns',\n       'GoldCoast', 'Townsville', 'Adelaide', 'MountGambier', 'Nuriootpa',\n       'Woomera', 'Albany', 'Witchcliffe', 'PearceRAAF', 'PerthAirport',\n       'Perth', 'SalmonGums', 'Walpole', 'Hobart', 'Launceston',\n       'AliceSprings', 'Darwin', 'Katherine', 'Uluru'], dtype=object)\n\n\nAnother common operation we might look to do is calculating the mean value (pd.DataFrame.mean()) of a certain variable. What is the average value of sunshine across the entire dataset?\n\n# calculate variable mean\ndf['Sunshine'].mean()\n\nnp.float64(7.611177520661157)\n\n\nThis gives us the mean to many decimal places, and we probably don’t need to know the average sunshine hours to this level of precision. We can use the pd.DataFrame.round() function to round to two decimal places.\n\n# round mean value\ndf['Sunshine'].mean().round(2)\n\nnp.float64(7.61)\n\n\nMany operations will return the value with information about the object’s type included. The above values are wrapped in np.float64() because pd.DataFrame.mean() uses numpy to calculate the mean value. However, if you want to strip this information out so you only see the value itself, you can use print().\n\n# print mean value\nprint(df['Sunshine'].mean().round(2))\n\n7.61\n\n\nWhile we are often interested in the mean value when we talk about averages, we might want to know the median instead (pd.DataFrame.median()).\n\n# calculate other summary statistics\nprint(df['Sunshine'].median())\n\n8.4\n\n\nAnother common calculation is summing values (pd.DataFrame.sum()). We can use sum() to see the total hours of sunshine in our dataset, and we can use int() to convert this value to an integer (which also means we don’t need to use print()3).\n\n# calculate sum value and return an integer\nint(df['Sunshine'].sum())\n\n575595\n\n\nWe can also apply these summary operations on multiple variables, using the same selection logic as before (using double squared brackets).\n\nprint(df[['Sunshine', 'Rainfall']].mean())\n\nSunshine    7.611178\nRainfall    2.360918\ndtype: float64\n\n\nAnd we can apply multiple functions, using pd.DataFrame.agg().\n\ndf['Sunshine'].agg(['mean', 'median', 'sum']).round(1)\n\nmean           7.6\nmedian         8.4\nsum       575595.3\nName: Sunshine, dtype: float64\n\n\nThe next step when exploring specific variables will often be group-level summaries. The average amount of sunshine across the whole dataset has limited utility, but the average hours of sunshine in each location allows us to compare between locations and start to understand how different variables are related to each other. If we want to do a group-level operation, we have to use pd.DataFrame.groupby().\n\n# calculate group means\ndf.groupby(by='Location')['Sunshine'].mean().round(1)\n\nLocation\nAdelaide            7.7\nAlbany              6.7\nAlbury              NaN\nAliceSprings        9.6\nBadgerysCreek       NaN\nBallarat            NaN\nBendigo             NaN\nBrisbane            8.1\nCairns              7.6\nCanberra            7.4\nCobar               8.7\nCoffsHarbour        7.4\nDartmoor            6.5\nDarwin              8.5\nGoldCoast           NaN\nHobart              6.6\nKatherine           NaN\nLaunceston          NaN\nMelbourne           6.4\nMelbourneAirport    6.4\nMildura             8.5\nMoree               8.9\nMountGambier        6.5\nMountGinini         NaN\nNewcastle           NaN\nNhil                NaN\nNorahHead           NaN\nNorfolkIsland       7.0\nNuriootpa           7.7\nPearceRAAF          8.8\nPenrith             NaN\nPerth               8.8\nPerthAirport        8.8\nPortland            6.5\nRichmond            NaN\nSale                6.7\nSalmonGums          NaN\nSydney              7.2\nSydneyAirport       7.2\nTownsville          8.5\nTuggeranong         NaN\nUluru               NaN\nWaggaWagga          8.2\nWalpole             NaN\nWatsonia            6.4\nWilliamtown         7.2\nWitchcliffe         NaN\nWollongong          NaN\nWoomera             9.0\nName: Sunshine, dtype: float64\n\n\nThe groupby(by='Location') function tells us the grouping variable (location), then we select the variable we want to summarise by location (sunshine), and then we specify the operation (mean).\nThere are multiple locations that return NaN (Not a Number). This indicates that numpy was unable to calculate a mean value for those locations. This is likely to be because all sunshine values for those locations are null.\nWe can check this using pd.DataFrame.count(), which counts the total non-null values (whereas pd.DataFrame.size() counts the total values).\n\n# group by location and count non-null sunshine values\ndf.groupby('Location')['Sunshine'].count()\n\nLocation\nAdelaide            1769\nAlbany              2520\nAlbury                 0\nAliceSprings        2520\nBadgerysCreek          0\nBallarat               0\nBendigo                0\nBrisbane            3144\nCairns              2564\nCanberra            1521\nCobar                550\nCoffsHarbour        1494\nDartmoor            2566\nDarwin              3189\nGoldCoast              0\nHobart              3179\nKatherine              0\nLaunceston             0\nMelbourne           3192\nMelbourneAirport    3008\nMildura             2876\nMoree               2055\nMountGambier        2597\nMountGinini            0\nNewcastle              0\nNhil                   0\nNorahHead              0\nNorfolkIsland       2570\nNuriootpa           2848\nPearceRAAF          3004\nPenrith                0\nPerth               3188\nPerthAirport        3004\nPortland            2566\nRichmond               0\nSale                1818\nSalmonGums             0\nSydney              3328\nSydneyAirport       2993\nTownsville          2617\nTuggeranong            0\nUluru                  0\nWaggaWagga          2575\nWalpole                0\nWatsonia            3008\nWilliamtown         1355\nWitchcliffe            0\nWollongong             0\nWoomera             2007\nName: Sunshine, dtype: int64\n\n\nThe results show that all the locations that return NaN in our group mean calculation have zero non-null values.",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#transforming-data",
    "href": "sessions/03-eda-pandas/index.html#transforming-data",
    "title": "Exploring Data Using Pandas",
    "section": "Transforming Data",
    "text": "Transforming Data\nDatasets are rarely perfectly clean and tidy. We often need to transform the data before we can get the most out of it.\nQuestions:\n\nWhat sort of transformations would help us get the most out of the analysis of the Australian weather data?\n\nThe first step with any analysis is often converting columns to the correct types. With a longitudinal (time-series) dataset,the date column is a good place to start. We can use pd.DataFrame.dtypes to check the data type, either of a single column (using the selector brackets) or all columns in the dataset.\n\nprint(df.dtypes)\n\nDate              object\nLocation          object\nMinTemp          float64\nMaxTemp          float64\nRainfall         float64\nEvaporation      float64\nSunshine         float64\nWindGustDir       object\nWindGustSpeed    float64\nWindDir9am        object\nWindDir3pm        object\nWindSpeed9am     float64\nWindSpeed3pm     float64\nHumidity9am      float64\nHumidity3pm      float64\nPressure9am      float64\nPressure3pm      float64\nCloud9am         float64\nCloud3pm         float64\nTemp9am          float64\nTemp3pm          float64\nRainToday         object\nRainTomorrow      object\ndtype: object\n\n\nAll columns are either stored as object or float64. The object data type is for generic non-numeric data, but from the columns that are stored as objects, we can tell this is mostly categorical variables where the categories are represented as text. The float64 data type refers to data that is numeric and includes decimals (float64 = 64-bit floating point number).\nThe date column is stored as an object, but pandas can store dates as datetime64. We can convert dates using pd.to_datetime(). When transforming data, if we want to keep those transformations, we have to store those changes, using =. In this case, we want to convert the date column but we don’t want to create an entirely new dataframe to handle this change, so we can overwrite the current date column by using the selection brackets to identify the column we want to apply this change to.\n\n# convert date column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\nThe remaining object columns can be converted to categorical, which makes them easier to work with in subsequent analyses. We can use pd.DataFrame.astype() to convert column data types.\n\n# create a list of all object columns\nobject_cols = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\n\n# convert object columns to category\ndf[object_cols] = df[object_cols].astype('category')\n\nA more efficient, though synactically more complex, way of doing this is using lamda functions. We won’t cover lambda functons in this session (they will be discussed in detail in a future session), but below is how we can use them to convert objects to categories.\n\n# convert object columns to category data type\ndf = df.apply(lambda x: x.astype('category') if x.dtype == 'object' else x)\n\nAnother choice we might make is to remove missing values, using pd.DataFrame.dropna() to filter the null values and keep only the non-null values. We can use this to drop all null values across the entire dataset, or we can apply it to a subset of columns, using the subset argument.\n\n# filter observations where sunshine is NA\ndf.dropna(subset='Sunshine')\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n6049\n2009-01-01\nCobar\n17.9\n35.2\n0.0\n12.0\n12.3\nSSW\n48.0\nENE\n...\n20.0\n13.0\n1006.3\n1004.4\n2.0\n5.0\n26.6\n33.4\nNo\nNo\n\n\n6050\n2009-01-02\nCobar\n18.4\n28.9\n0.0\n14.8\n13.0\nS\n37.0\nSSE\n...\n30.0\n8.0\n1012.9\n1012.1\n1.0\n1.0\n20.3\n27.0\nNo\nNo\n\n\n6051\n2009-01-03\nCobar\n15.5\n34.1\n0.0\n12.6\n13.3\nSE\n30.0\nNaN\n...\nNaN\n7.0\nNaN\n1011.6\nNaN\n1.0\nNaN\n32.7\nNo\nNo\n\n\n6052\n2009-01-04\nCobar\n19.4\n37.6\n0.0\n10.8\n10.6\nNNE\n46.0\nNNE\n...\n42.0\n22.0\n1012.3\n1009.2\n1.0\n6.0\n28.7\n34.9\nNo\nNo\n\n\n6053\n2009-01-05\nCobar\n21.9\n38.4\n0.0\n11.4\n12.2\nWNW\n31.0\nWNW\n...\n37.0\n22.0\n1012.7\n1009.1\n1.0\n5.0\n29.1\n35.6\nNo\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n142298\n2017-06-20\nDarwin\n19.3\n33.4\n0.0\n6.0\n11.0\nENE\n35.0\nSE\n...\n63.0\n32.0\n1013.9\n1010.5\n0.0\n1.0\n24.5\n32.3\nNo\nNo\n\n\n142299\n2017-06-21\nDarwin\n21.2\n32.6\n0.0\n7.6\n8.6\nE\n37.0\nSE\n...\n56.0\n28.0\n1014.6\n1011.2\n7.0\n0.0\n24.8\n32.0\nNo\nNo\n\n\n142300\n2017-06-22\nDarwin\n20.7\n32.8\n0.0\n5.6\n11.0\nE\n33.0\nE\n...\n46.0\n23.0\n1015.3\n1011.8\n0.0\n0.0\n24.8\n32.1\nNo\nNo\n\n\n142301\n2017-06-23\nDarwin\n19.5\n31.8\n0.0\n6.2\n10.6\nESE\n26.0\nSE\n...\n62.0\n58.0\n1014.9\n1010.7\n1.0\n1.0\n24.8\n29.2\nNo\nNo\n\n\n142302\n2017-06-24\nDarwin\n20.2\n31.7\n0.0\n5.6\n10.7\nENE\n30.0\nENE\n...\n73.0\n32.0\n1013.9\n1009.7\n6.0\n5.0\n25.4\n31.0\nNo\nNo\n\n\n\n\n75625 rows × 23 columns\n\n\n\nWe haven’t stored this transformation, because filtering nulls without careful consideration is a bad idea, but it’s useful to know, nonetheless.\nThere are lots of ways we could transform the data, but the final example we will consider here is reshaping the data using pd.DataFrame.pivot(), which transforms the data from long to wide format data, and pd.DataFrame.melt(), which transforms it from wide to long format.\nPerhaps we want to focus on the maximum temperature per day in each location in 2015. We can use pd.Series.dt.year to get the year from the date column, and filter for the year 2015, before reshaping the data.\n\ndf2015 = df.loc[df['Date'].dt.year == 2015]\ndf_wide = df2015.pivot(index='Date', columns='Location', values='MaxTemp')\n\ndf_wide.head()\n\n\n\n\n\n\n\nLocation\nAdelaide\nAlbany\nAlbury\nAliceSprings\nBadgerysCreek\nBallarat\nBendigo\nBrisbane\nCairns\nCanberra\n...\nTownsville\nTuggeranong\nUluru\nWaggaWagga\nWalpole\nWatsonia\nWilliamtown\nWitchcliffe\nWollongong\nWoomera\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2015-01-01\n37.0\n21.9\n33.5\n40.3\n34.7\n27.4\n31.2\n31.3\n33.7\n32.6\n...\n32.6\n32.1\n42.0\n35.2\n23.6\n28.3\n33.7\n25.0\n25.3\n39.2\n\n\n2015-01-02\n44.1\n21.2\n39.6\n41.4\n30.5\n38.2\n39.8\n30.5\n33.7\n35.2\n...\n33.0\n34.1\n42.4\n38.9\n21.1\n40.6\n29.3\n23.6\n24.6\n43.3\n\n\n2015-01-03\n38.2\n21.5\n38.3\n36.4\n34.3\n37.5\n40.3\n28.9\n33.6\n34.7\n...\n28.1\n33.7\n39.8\n37.5\n21.8\n39.5\n32.8\n23.0\n25.7\n44.7\n\n\n2015-01-04\n30.5\n23.3\n33.1\n29.0\n34.8\n23.5\n29.0\n30.2\n29.4\n32.5\n...\n31.6\n32.8\n36.1\n33.8\n24.4\n25.1\n34.5\n29.8\n25.3\n37.6\n\n\n2015-01-05\n34.9\n24.9\n35.2\n27.1\n27.2\n26.6\n33.6\n28.1\n31.4\n29.6\n...\n31.6\n28.9\n38.8\n34.9\n29.5\n25.7\n27.0\n31.7\n23.1\n38.3\n\n\n\n\n5 rows × 49 columns\n\n\n\nPerhaps we want to look at the maximum and minimum temperatures in each location, together. We can reshape the data to support this4.\n\ndf_long = df2015.melt(\n    id_vars=['Date', 'Location'],\n    value_vars=['MaxTemp', 'MinTemp'],\n    var_name='Variable',\n    value_name='Value'\n)\n\ndf_long.head()\n\n\n\n\n\n\n\n\nDate\nLocation\nVariable\nValue\n\n\n\n\n0\n2015-01-01\nAlbury\nMaxTemp\n33.5\n\n\n1\n2015-01-02\nAlbury\nMaxTemp\n39.6\n\n\n2\n2015-01-03\nAlbury\nMaxTemp\n38.3\n\n\n3\n2015-01-04\nAlbury\nMaxTemp\n33.1\n\n\n4\n2015-01-05\nAlbury\nMaxTemp\n35.2",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#exercises",
    "href": "sessions/03-eda-pandas/index.html#exercises",
    "title": "Exploring Data Using Pandas",
    "section": "Exercises",
    "text": "Exercises\nSome of these questions are easily answered by scrolling up and finding the answer in the output of the above code, however, the goal is to find the answer using code. No one actually cares what the answer to any of these questions is, it’s the process that matters!\nRemember, if you don’t know the answer, it’s okay to Google it (or speak to others, including me, for help)!\n\n\nImport Data (to Reset)\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')\n\n\n\nWhat is the ‘Sunshine’ column’s data type?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# What is the 'Sunshine' column's data type?\nprint(df['Sunshine'].dtypes)\n\nfloat64\n\n\n\n\n\n\nIdentify all the columns that are of dtype ‘object’.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Identify all the columns that are of dtype 'object'\nprint(list(df.select_dtypes(include=['object'])))\n\n['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\n\n\n\n\n\n\nHow many of the dataframe’s columns are of dtype ‘object’?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# How many of the dataframe's columns are of dtype 'object'?\nlen(list(df.select_dtypes(include=['object'])))\n\n7\n\n\n\n\n\n\nHow many of the ‘Rainfall’ column values are NAs?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# How many of the 'Rainfall' column values are NAs?\nprint(df['Rainfall'].isna().sum())\n\n3261\n\n\n\n\n\n\nCreate a new dataframe which only includes the ‘Date’, ‘Location, ’Sunshine’, ‘Rainfall’, and ‘RainTomorrow’ columns.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nnew_df = df[['Date', 'Location', 'Sunshine', 'Rainfall', 'RainTomorrow']]\nnew_df.head()\n\n\n\n\n\n\n\n\nDate\nLocation\nSunshine\nRainfall\nRainTomorrow\n\n\n\n\n0\n2008-12-01\nAlbury\nNaN\n0.6\nNo\n\n\n1\n2008-12-02\nAlbury\nNaN\n0.0\nNo\n\n\n2\n2008-12-03\nAlbury\nNaN\n0.0\nNo\n\n\n3\n2008-12-04\nAlbury\nNaN\n0.0\nNo\n\n\n4\n2008-12-05\nAlbury\nNaN\n1.0\nNo\n\n\n\n\n\n\n\n\n\n\n\nConvert ‘RainTomorrow’ to a numeric variable, where ‘Yes’ = 1 and ‘No’ = 0.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# df['Location'].astype('category').cat.codes\n# df['RainTomorrow'].astype('category').cat.codes\ndf['RainTomorrow'].map({'Yes': 1, 'No': 0})\n\n0         0.0\n1         0.0\n2         0.0\n3         0.0\n4         0.0\n         ... \n145455    0.0\n145456    0.0\n145457    0.0\n145458    0.0\n145459    NaN\nName: RainTomorrow, Length: 145460, dtype: float64\n\n\n\n\n\n\nWhat is the average amount of rainfall for each location?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# average rainfall by location, sorted by value\ndf.groupby('Location')['Rainfall'].mean().sort_values(ascending=False)\n\nLocation\nCairns              5.742035\nDarwin              5.092452\nCoffsHarbour        5.061497\nGoldCoast           3.769396\nWollongong          3.594903\nWilliamtown         3.591108\nTownsville          3.485592\nNorahHead           3.387299\nSydney              3.324543\nMountGinini         3.292260\nKatherine           3.201090\nNewcastle           3.183892\nBrisbane            3.144891\nNorfolkIsland       3.127665\nSydneyAirport       3.009917\nWalpole             2.906846\nWitchcliffe         2.895664\nPortland            2.530374\nAlbany              2.263859\nBadgerysCreek       2.193101\nPenrith             2.175304\nTuggeranong         2.164043\nDartmoor            2.146567\nRichmond            2.138462\nMountGambier        2.087562\nLaunceston          2.011988\nAlbury              1.914115\nPerth               1.906295\nMelbourne           1.870062\nWatsonia            1.860820\nPerthAirport        1.761648\nCanberra            1.741720\nBallarat            1.740026\nWaggaWagga          1.709946\nPearceRAAF          1.669080\nMoree               1.630203\nBendigo             1.619380\nHobart              1.601819\nAdelaide            1.566354\nSale                1.510167\nMelbourneAirport    1.451977\nNuriootpa           1.390343\nCobar               1.127309\nSalmonGums          1.034382\nMildura             0.945062\nNhil                0.934863\nAliceSprings        0.882850\nUluru               0.784363\nWoomera             0.490405\nName: Rainfall, dtype: float64\n\n\n\n\n\n\nWhat is the average amount of rainfall for days that it will rain tomorrow?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# average rainfall depending on whether it will rain tomorrow or not\ndf.groupby('RainTomorrow')['Rainfall'].mean()\n\nRainTomorrow\nNo     1.270290\nYes    6.142104\nName: Rainfall, dtype: float64\n\n\n\n\n\n\nWhat is the average amount of sunshine in Perth when it will not rain tomorrow?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# average sunshine in Perth when it won't rain tomorrow\ndf.loc[(df['Location'] == 'Perth') & (df['RainTomorrow'] == 'No'), 'Sunshine'].mean()\n# df[(df['Location']=='Perth') & (df['RainTomorrow']=='No')]['Sunshine'].mean()\n\nnp.float64(9.705306603773584)\n\n\n\n\n\n\nWe want to understand the role that time plays in the dataset. Using the original dataframe, carry the following tasks and answer the corresponding questions:\n\nCreate columns representing the year and month from the ‘Date’ column. How many years of data are in the dataset?\nExamine the distribution of the ‘Sunshine’ NAs over time. Is time a component in the ‘Sunshine’ data quality issues?\nCalculate the average rainfall and sunshine by month. How do rainfall and sunshine vary through the year?\nCalculate the average rainfall and sunshine by year. How have rainfall and sunshine changed over time?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# get year and month columns\ndf = (\n    df.assign(Date=pd.to_datetime(df['Date']))\n    .assign(\n        Year=lambda x: x['Date'].dt.year,\n        Month=lambda x: x['Date'].dt.month\n    )\n)\n\n# count unique years\ndf['Year'].nunique()\n\n11\n\n\n\n# lambda function counting nulls by year\ndf.groupby('Year')['Sunshine'].apply(lambda x: x.isna().sum())\n\nYear\n2007        0\n2008      323\n2009     6146\n2010     6220\n2011     6053\n2012     6539\n2013     7570\n2014     9157\n2015     9441\n2016    11994\n2017     6392\nName: Sunshine, dtype: int64\n\n\n\n# rainfall and sunshine by month\ndf.groupby('Month')[['Rainfall', 'Sunshine']].mean().round(1)\n\n\n\n\n\n\n\n\nRainfall\nSunshine\n\n\nMonth\n\n\n\n\n\n\n1\n2.7\n9.2\n\n\n2\n3.2\n8.6\n\n\n3\n2.8\n7.6\n\n\n4\n2.3\n7.1\n\n\n5\n2.0\n6.3\n\n\n6\n2.8\n5.6\n\n\n7\n2.2\n6.1\n\n\n8\n2.0\n7.1\n\n\n9\n1.9\n7.7\n\n\n10\n1.6\n8.5\n\n\n11\n2.3\n8.7\n\n\n12\n2.5\n9.0\n\n\n\n\n\n\n\n\n# rainfall and sunshine by year\ndf.groupby('Year')[['Rainfall', 'Sunshine']].mean().round(1)\n\n\n\n\n\n\n\n\nRainfall\nSunshine\n\n\nYear\n\n\n\n\n\n\n2007\n3.2\n8.1\n\n\n2008\n2.3\n7.8\n\n\n2009\n2.2\n7.9\n\n\n2010\n2.7\n7.3\n\n\n2011\n2.8\n7.3\n\n\n2012\n2.4\n7.6\n\n\n2013\n2.3\n7.7\n\n\n2014\n2.0\n7.8\n\n\n2015\n2.2\n7.7\n\n\n2016\n2.4\n7.6\n\n\n2017\n2.5\n7.7",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#footnotes",
    "href": "sessions/03-eda-pandas/index.html#footnotes",
    "title": "Exploring Data Using Pandas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor more information about pandas data types, check out the pandas documentation on dtypes.↩︎\nFor more information, I’d recommend the pandas documentation, and this pandas tutorial on subsetting data.↩︎\nSome functions should be wrapped in print() in order to return a value that is easy to read, but others won’t. There will be an internal logic for which is which, but it’s not of huge importance to us. You are better off just testing functions out and wrapping them in print() if necessary.↩︎\nThis is often very useful when we need to visualise data, for example plotting the max and min temp for each location, is easier if the values are organised in the same column and differentiated using another column.↩︎",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html",
    "href": "sessions/02-jupyter_notebooks/index.html",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "This is the second session following Code Club’s relaunch. The focus is introducing jupyter notebooks and explaining to users how to get started with a new project and briefly introducing some key concepts.\nWe are also planning some time for Q&A following the first session.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#session-slides",
    "href": "sessions/02-jupyter_notebooks/index.html#session-slides",
    "title": "Jupyter Notebooks",
    "section": "Session Slides",
    "text": "Session Slides\nUse the left ⬅️ and right ➡️ arrow keys to navigate through the slides below. To view in a separate tab/window, follow this link.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#the-tools-you-will-need",
    "href": "sessions/02-jupyter_notebooks/index.html#the-tools-you-will-need",
    "title": "Jupyter Notebooks",
    "section": "The Tools You Will Need",
    "text": "The Tools You Will Need\nThough Jupyter notebooks can be used with a variety of coding languages and in different settings the key tools used in this session are:\n\nLanguage: Python\nDependency Management & Virtual Environments: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code/Jupyter Notebooks (or your preferred IDE)\n\nYou can install all the tools you’ll need by running the following one-liner run in PowerShell:\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop\nYou can find more information on these topics in the Python Onboarding session",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#project-setup",
    "href": "sessions/02-jupyter_notebooks/index.html#project-setup",
    "title": "Jupyter Notebooks",
    "section": "Project Setup",
    "text": "Project Setup\nOur project set-up will follow the same steps as used in the onboarding session, by using uv to set up a new project folder.\nTo get started we will use PowerShell powershell to open a command prompt, it should open in your C drive (e.g., C:\\Users\\user.name). If it does not, run cd ~, and it should return to your home directory. We recommend the use of a single folder to hold your python projects while learning, because we will be using git version control we will call this “Git”. we can use the command mkdir code_club to make this folder and then use cd code_club to relocate to this folder1.\nWe will create a new uv project in this directory using the command uv init. The new project will contain everything we need, including a Python installation, a virtual environment, and the necessary project files for tracking and managing any packages installed in the virtual environment. To set up a new project called test-project, use the following command:\nuv init test_project\nHaving created this new directory, navigate to it using cd test_project.\nFor this session you will need to add 3 Python packages, ipykernel2, pandas and seaborn We can use the following command:\nuv add ipykernel pandas seaborn\nWe are going to create a blank notebook in this file by running the command new-item first_notebook.ipynb if you now run ls you will note this file has been created\nYour Python project is now set up, and you are ready to start writing some code. You can open VS Code from your PowerShell window by running code ..\n\nOpening your project in VS Code\nYou could also do this from within VS Code as most IDEs include a terminal interface which will be demonstrated in session.\nFor now launch VS Code and click File &gt; Open Folder.... You’ll want to make sure you select the root level of your project. Once you’ve opened the folder, the file navigation pane in VS Code should display the files that uv has created, as well as the notebook you created: first_notebook.ipynb. Click on this to open it.\nOnce VS Code realises you’ve opened a folder with Python code and a virtual environment, it should do the following:\n\nSuggest you install the Python extension (and, once you’ve created a Jupyter notebook, the Jupyter one) offered by Microsoft - go ahead and do this. If this doesn’t happen, you can install extensions manually from the Extensions pane on the left-hand side.\nSelect the uv-created .venv as the python Environment we’re going to use to actually run our code. If this doesn’t happen, press ctrl-shift-P, type “python environment” to find the Python - Create Environment... option, hit enter, choose “Venv” and proceed to “Use Existing”.\n\nIf VS Code has found the virtual environment, it may pick up the correct kernel. If not you may need to select this manually this can be done by clicking in the top right where you can see Select Kernel (see below)\n\n\n\nClick ‘Select Kernel’\n\n\nWe can then select the appropriate kernel from python environments and looking for\n\n\n\nclick Python Environments\n\n\n\n\n\nclick venv - recommended\n\n\nOnce the kernel is enabled you are ready to start adding cells to your notebook. these can either be code cells which is where you include your program elements or markdown which enable the addition of headings, analysis and commentary.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#footnotes",
    "href": "sessions/02-jupyter_notebooks/index.html#footnotes",
    "title": "Jupyter Notebooks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe recommend using the C drive for all Python projects, especially if using version control. Storing projects like these on One Drive will create many unnecessary issues. It can be helpful to use a sub-directory to store projects but is not necessary and is not a requirement for code club↩︎\nStrictly speaking, we should install ipykernel as a development dependency (a dependency that is needed for any development but not when the project is put into production). In this case, we would add it by running uv add --dev ipykernel. However, in this case, it is simpler to just add it as a regular dependency, and it doesn’t harm.↩︎",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  }
]