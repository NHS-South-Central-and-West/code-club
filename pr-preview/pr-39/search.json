[
  {
    "objectID": "sessions/04-seaborn-visualisation/index.html",
    "href": "sessions/04-seaborn-visualisation/index.html",
    "title": "Visualisation with Seaborn",
    "section": "",
    "text": "Python has a rich ecosystem of libraries for data visualisation, each with different strengths. Some popular options include matplotlib for fine control over plots, plotly for interactive visualisations, and bokeh for web-ready dashboards. In this session, we’ll be using seaborn. It’s built on top of matplotlib but offers a simpler, high-level interface and nice looking default styles — it’s therefore a good choice when you who want to quickly create clear and informative plots without needing to tweak every detail.\nWe are using Australian weather data, taken from Kaggle. This dataset is used to build machine learning models that predict whether it will rain tomorrow, using data about the weather every day from 2007 to 2017. To download the data, click here.\nOne final note before we get started - This page is a combination of text and python code. We’ve tried to explain clearly what we’re about to do before we do it, but do also note the # comments within the python code cells themselves that occasionally explain a specific line of code in more detail.",
    "crumbs": [
      "Exploration & Visualisation",
      "4. Visualisations with Seaborn"
    ]
  },
  {
    "objectID": "sessions/04-seaborn-visualisation/index.html#initial-setup",
    "href": "sessions/04-seaborn-visualisation/index.html#initial-setup",
    "title": "Visualisation with Seaborn",
    "section": "Initial setup",
    "text": "Initial setup\nWe’re going to import some python packages. Remember that the plt, np, sns aliases are just for convenience - we could omit this completely or use different aliases if we prefer.\n\n\n\n\n\n\nNoteAside - why sns?\n\n\n\nSeaborn being imported as sns is an odd convention (they are the initials of the fictional character the package was named for) that will make it easier to read or copy/paste online examples.\n\n\n\n# install necessary packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# suppress some annoying warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning) \n\nsns.set_theme(style='darkgrid') # https://seaborn.pydata.org/generated/seaborn.set_theme.html\nsns.set_context(\"notebook\") # set an overall scale. Notebook is the default. In increasing size: paper, notebook, talk, poster.\nplt.rcParams['font.sans-serif'] = ['Calibri','Segoe UI','Arial'] # use a nicer font in matplotlib (if available)\n\nAs before, we need to import our dataset. We’re importing the csv file into an initial dataframe called df to start with.\n\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')",
    "crumbs": [
      "Exploration & Visualisation",
      "4. Visualisations with Seaborn"
    ]
  },
  {
    "objectID": "sessions/04-seaborn-visualisation/index.html#data-manipulation",
    "href": "sessions/04-seaborn-visualisation/index.html#data-manipulation",
    "title": "Visualisation with Seaborn",
    "section": "Data manipulation",
    "text": "Data manipulation\n\nColumn conversions\nBefore we start actually generating some visuals, we need to make sure our Date column contains proper datetimes. We’re also going to drop the years with partial data so that our dataset only has full years. Finally we’re going to change the RainTomorrow field to contain a 0 or a 1 rather than yes/no.\n\n# convert date column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# create a column that just contains the year\ndf['Year'] = df['Date'].dt.year\n\n# drop the partial years (2007,2017)\ndf = df[~df['Year'].isin([2007,2017])]\n\n# convert 'RainTomorrow' to a numeric variable, where 'Yes' = 1 and 'No' = 0.\ndf['RainToday']=df['RainToday'].replace({'Yes': 1, 'No': 0, 'NA':0}).fillna(0).astype(int)\ndf['RainTomorrow']=df['RainTomorrow'].map({'Yes': 1, 'No': 0,'NA': 0}).fillna(0).astype(int); \n\n# little tip: the semicolon suppresses textual output when we don't want it\n\n\n\nSort order and other helper tables\nWe need a month order for our “mmm” months - there is probably an official way of doing this…\n\nmonth_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\nWe also need a sort order for our city names to use as a column order for some of our charts later. We’ll just arrange them alphabetically.\n\ncolOrder_top5Cities=['Adelaide','Brisbane','Melbourne','Perth','Sydney']\n\nTo enhance a chart we’re going to build later, we’re going to dynamically calculate some text describing our data range.\n\n# Calculate the date range dynamically; we're going to use this later...\ndate_min = df['Date'].min().strftime('%Y')\ndate_max = df['Date'].max().strftime('%Y')\ndate_range = f\"{date_min} - {date_max}\"\n\nprint(date_range)\n\n2008 - 2016\n\n\n\n\nPivoting and grouping\nNext, we’re going to create some helper dataframes by filtering, grouping and pivoting the data. These will be used for different types of visuals later. Of course, we could have just created these groupings and pivots inline when we do the actual visualisation, but we’re doing it this way because:\n\nIt’s easier to follow\nIt’s tidier (and probably faster) to create these dataframes once as we’re going to be using them multiple times.\n\n\n# build a month column\ndf['Month'] = df['Date'].dt.strftime('%b') # Add a column that just contains the month in mmm format\ndf['Month'] = pd.Categorical(df['Month'], categories=month_order, ordered=True) # Make it categorical using our custom order so that it appears in the right order\n\n# we're going to filter to top 5 cities from now on\ndf_top5Cities = df[df['Location'].isin(['Perth','Adelaide','Sydney','Melbourne','Brisbane'])]\n\n# a dataframe with the number of rainy days per year and month, and location\ndf_top5Cities_rainyDays = df_top5Cities.groupby(['Location','Year', 'Month'])['RainToday'].sum().reset_index()\n\n# finally, we're going to create some grouped and pivoted dataframes. Picture these as PivotTables in Excel.\ndf_top5Cities_Rainfall_grouped = df_top5Cities.groupby(['Location', 'Month'])['Rainfall'].mean().reset_index()\ndf_top5Cities_Rainfall_pivoted = df_top5Cities_Rainfall_grouped.pivot(index=\"Location\",columns=\"Month\", values=\"Rainfall\")\ndf_top5Cities_monthly_rainyDays_pivoted = df_top5Cities.groupby(['Location', 'Month','Year'])['RainToday'].sum().reset_index().groupby(['Location','Month'])['RainToday'].mean().reset_index().pivot(index=\"Location\",columns=\"Month\", values=\"RainToday\")\n\nLet’s use head() to make sure we understand what each grouped/pivoted DF is for.\n\ndf_top5Cities_Rainfall_grouped.head(2)\n\n\n\n\n\n\n\n\nLocation\nMonth\nRainfall\n\n\n\n\n0\nAdelaide\nJan\n0.672199\n\n\n1\nAdelaide\nFeb\n0.973604\n\n\n\n\n\n\n\n\ndf_top5Cities_Rainfall_pivoted.head(2)\n\n\n\n\n\n\n\nMonth\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\nLocation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelaide\n0.672199\n0.973604\n1.171667\n1.461165\n2.293860\n2.447009\n2.873606\n2.399237\n1.809125\n0.906273\n0.730827\n1.192500\n\n\nBrisbane\n6.415574\n5.325389\n4.442276\n3.165385\n3.126446\n2.516318\n1.000000\n1.273381\n1.314498\n2.419424\n3.347761\n4.551613\n\n\n\n\n\n\n\n\ndf_top5Cities_monthly_rainyDays_pivoted.head(2)\n\n\n\n\n\n\n\nMonth\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\nLocation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelaide\n2.555556\n1.555556\n3.666667\n4.000000\n7.666667\n8.555556\n13.444444\n11.444444\n7.888889\n4.222222\n4.000000\n4.555556\n\n\nBrisbane\n8.000000\n7.111111\n10.000000\n5.333333\n5.555556\n6.222222\n4.111111\n3.555556\n4.111111\n5.888889\n6.888889\n8.777778\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteAside: why df[df[...?\n\n\n\ndf_top5Cities = df[df['Location'].isin(['Perth','Adelaide','Sydney','Melbourne','Brisbane'])]\n\nThe first (outer) df[ tells pandas that we want to select a subset of rows based on some condition.\nThe second (inner) df[ is going to tell pandas this condition. In this case, we’re using isin to return a dataframe that contains a series of True and False rows corresponding to whether the rows in our original dataframe had the Location column as one of our 5 cities.\nThe final dataframe is then a filtered copy where the inner condition is True.\n\nYes, there are other ways of doing this! For example by using .query() to specify our conditions.",
    "crumbs": [
      "Exploration & Visualisation",
      "4. Visualisations with Seaborn"
    ]
  },
  {
    "objectID": "sessions/04-seaborn-visualisation/index.html#doing-some-actual-plotting",
    "href": "sessions/04-seaborn-visualisation/index.html#doing-some-actual-plotting",
    "title": "Visualisation with Seaborn",
    "section": "Doing some actual plotting",
    "text": "Doing some actual plotting\nThe Seaborn home page has a very good introductory tutorial, reference documentation, and a nice collection of examples. You should familiarise yourself with the documentation; it’ll pay off massively if you actually grasp what each function and argument is for, rather than just copy/pasting examples and tweaking them until they work (without really understanding what they’re doing).\n\nA basic one-line line chart\n\nsns.lineplot(\n  data=df_top5Cities_Rainfall_pivoted.T #.T gives the transpose (flips rows and columns)\n  ) \n\n\n\n\n\n\n\n\n\n\nJust a little bit of customisation - a bar chart\nhttps://seaborn.pydata.org/generated/seaborn.barplot.html\n\nour_barplot = sns.barplot(\n  data=df_top5Cities_Rainfall_grouped \n  ,x=\"Month\"\n  ,y=\"Rainfall\"\n  ,hue=\"Location\" # read \"hue\" as \"series\"\n  ,palette=\"tab10\" # https://matplotlib.org/stable/users/explain/colors/colormaps.html\n  )\n\nour_barplot.set(title='Average daily rainfall by month and city',ylim=(0,8))\n\nsns.move_legend(our_barplot,\"upper left\", title=None, ncol=4) # https://seaborn.pydata.org/generated/seaborn.move_legend.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteAside - why did we need to set the title after the sns.barplot call?\n\n\n\nThe barplot function provided by Seaborn doesn’t actually allow setting of a title - it just generates a plot (including its axes) and returns this as a matplotlib Axes object (recall we mentioned earlier that Seaborn is a layer on top of the matplotlib library). By using the .set(...) method on our barplot object, we can modify this returned object to give it a title. We also could have used this to customise our axis labels (the defaults are fine here), set axis limits, or things like tick labels.\n\n\n\n\nHeatmaps\nThe Seaborn heatmap function will easily let us create a two-dimensional heatmap visual with a specific colour theme and custom number formatting.\n\n# We need to use some matplotlib code to set our output size, add a title, and capitalise our x-axis label\nf,ax = plt.subplots(figsize=(10, 5)) # matplotlib subplots are a common way of setting a figure layout\nax.set_title(f\"Average daily rainfall (/mm) each month ({date_range}) for Australia's top 5 cities\", fontsize=16, fontweight=\"bold\", pad=10) # using our previously set date_range variable\n\nsns.heatmap(df_top5Cities_Rainfall_pivoted # Heatmap expects rectangular (pivot-like) data\n            ,annot=True # Put numbers inside the cells\n            ,fmt=\".1f\" # Make the numbers have 1 decimal place\n            ,square=True # Square vs rectangular cells\n            ,cbar=False # Get rid of the colourbar legend on the side\n            ,cmap=\"Blues\" # Seems appropriate for rainfall. Colourmaps reference: https://matplotlib.org/stable/users/explain/colors/colormaps.html \n            ,ax=ax # Tell it to use the matplotlib axes we created earlier\n           )\n\n\n\n\n\n\n\n\n\n\nAnother heatmap with some further tweaks\nWe can make our heatmap look just a little better by apply some tweaks to the subplots object.\n\n# Again setting up matplotlib subplots so that we can make some changes later\nf,ax = plt.subplots(figsize=(10, 5)) \n\nsns.heatmap(df_top5Cities_monthly_rainyDays_pivoted # Heatmap expects rectangular (pivot-like) data\n            ,annot=True # Put numbers inside the cells\n            ,fmt=\".0f\" # Force the number format\n            ,square=True # Square vs rectangular cells\n            ,cbar=False # Get rid of the colourbar legend on the side\n            ,cmap=\"crest\" # Colourmaps reference: https://matplotlib.org/stable/users/explain/colors/colormaps.html \n            ,ax=ax # Tell it to use the matplotlib axes we created earlier\n           )\n\n# We need to use some matplotlib code to set our output size, add a title, and capitalise our x-axis label\nax.tick_params(axis='x', labelsize=11, rotation=45) # I think 45-degree month labels look nicer, but this is a matter of taste.\nax.tick_params(axis='y', labelsize=11)\n\n# Manually changing our axis labels for more control\nax.set_xlabel(\"Month\",fontweight=\"bold\",fontsize=12) \nax.set_ylabel(\"City\",fontweight=\"bold\",fontsize=12)\n\n# Set our title dynamically\nax.set_title(f\"Mean number of rainy days by month between {date_min} and {date_max} for Australia's top 5 cities\", fontsize=16, fontweight=\"bold\", pad=15);\n\n\n\n\n\n\n\n\n\n\nA fancy multi-chart visual\nThis chart uses the boxgrid object to arrange multiple different subcharts. We’re actually generating two sets of different visuals (linegrid and boxgrid) in one output. If you’re not sure what the for [...] in [...] syntax means, don’t worry - this will be covered in a future session.\n\n# Setting up the grid of box plots\n# Box plots are a bit of a rabbit hole and are extremely customisable; we're mostly using defaults here\nboxgrid = sns.FacetGrid(df_top5Cities \n                        ,col=\"Location\" # Defining the different facets\n                        ,col_wrap=5, height=4.5 # Layout and sizing for our facet grid\n                        ,col_order=colOrder_top5Cities  # Using our alphabetical order of city names to arrange our facets\n)\nboxgrid.map(sns.boxplot # This is what tells sns what sort of plots we want in our grid\n            ,\"Month\" # X\n            ,\"MaxTemp\" # Y\n            ,linewidth=1.5\n            ,color=\"skyblue\"\n            ,order=month_order\n            ,fliersize=0 # Seaborn boxplots by default include markers for outliers, which it calls \"fliers\". For this chart we'd like to disable these.\n            ) \n\n# Setting up the grid of line plots\n# Using the default matplotlib plot here\nlinegrid = sns.FacetGrid(df_top5Cities.groupby(['Location', 'Month'])['Rainfall'].mean().reset_index()\n                         ,col=\"Location\" \n                         ,hue=\"Location\",palette=\"Set2\" \n                         ,col_wrap=5, height=4.5 \n                         ,col_order=colOrder_top5Cities\n)\nlinegrid.map(plt.plot, \"Month\", \"Rainfall\",marker=\"o\")\n\n# Formatting axes\nfor ax in boxgrid.axes.flat:\n    ax.tick_params(axis='x', labelsize=9, rotation=45)\n    ax.tick_params(axis='y', labelsize=9)\nfor ax in linegrid.axes.flat:\n    ax.tick_params(axis='x', labelsize=9, rotation=45)\n    ax.tick_params(axis='y', labelsize=9)\n\nlinegrid.set_titles(col_template=\"{col_name}\",fontweight=\"bold\",fontsize=16)\nlinegrid.set_axis_labels(\"Month\",\"Average rainfall /mm\",fontweight=\"bold\",fontsize=10)\nboxgrid.set_titles(col_template=\"{col_name}\",fontweight=\"bold\",fontsize=16)\nboxgrid.set_axis_labels(\"Month\",\"Max temp /$\\\\degree$C\",fontweight=\"bold\",fontsize=10)\n\n# Setting overall titles and spacing\nlinegrid.fig.suptitle(f\"Mean daily rainfall by month for top 5 cities, {date_range}\", fontsize=16, color='black',fontweight='bold') \nboxgrid.fig.suptitle(f\"Max temperature by month for top 5 cities, {date_range}\", fontsize=16, color='black',fontweight='bold') \nlinegrid.fig.subplots_adjust(top=0.85)  \nboxgrid.fig.subplots_adjust(top=0.85);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Small multiples”-type time-series grid\nThis is mostly based on the example given at https://seaborn.pydata.org/examples/timeseries_facets.html.\nFirstly, for this one we need another dataframe that just contains the Sydney data.\n\ndf_Sydney = df[df['Location']==\"Sydney\"].groupby(['Month', 'Location','Year'], as_index=False)['MaxTemp'].max()\n\nWe’re going to use the relplot function to create a grid of plots with a specific set of variables across its rows and columns. For each cell we’re plotting that year’s data with a different colour, and plotting all the other years in grey in the background.\n\n# plot each year's time series in its own facet\ng = sns.relplot(data=df_Sydney\n                ,x=\"Month\"\n                ,y=\"MaxTemp\"\n                ,col=\"Year\"\n                ,hue=\"Year\"\n                ,kind=\"line\"\n                ,palette=\"viridis\"\n                ,linewidth=4\n                ,zorder=5\n                ,col_wrap=3, height=2, aspect=1.5, legend=False,\n)\n\n# iterate over each subplot to customize further\nfor year, ax in g.axes_dict.items():\n\n    # Add the title as an annotation within the plot\n    ax.text(.8\n            ,.85\n            ,year\n            ,transform=ax.transAxes\n            ,fontweight=\"bold\",fontsize=9)\n\n    # Plot every year's time series in the background\n    sns.lineplot(\n        data=df_Sydney, x=\"Month\", y=\"MaxTemp\", units=\"Year\",\n        estimator=None, color=\".7\", linewidth=1, ax=ax\n    )\n\n# reduce the frequency of the x axis ticks\nax.set_xticks(ax.get_xticks()[::2])\n\n# tweak the supporting aspects of the plot\ng.set_titles(\"\")\ng.fig.suptitle(f\"Max temperature by month in Sydney, {date_range}\", fontsize=16, color='black',fontweight='bold') \ng.set_axis_labels(\"\", \"Max Temp /$\\\\degree \\\\mathrm{C}$\");\ng.tight_layout();",
    "crumbs": [
      "Exploration & Visualisation",
      "4. Visualisations with Seaborn"
    ]
  },
  {
    "objectID": "sessions/12-analysing-relationships/index.html",
    "href": "sessions/12-analysing-relationships/index.html",
    "title": "Regression Fundamentals: Analysing Relationships",
    "section": "",
    "text": "This is the second in our series of sessions that builds the regression foundations. Here we will look at how to explore relationships in data, using both quantitative measures such as correlation and a range of visualisations methods.\nThis session discusses what it means to analyse relationships between variables, what is possible with different types of variables (and how this links to the previous session that looked at comparing samples), and what these methods can and cannot tell us.\nWe will use the Palmer Penguins dataset throughout this session, taken from the TidyTuesday GitHub repository (originally from the palmerpenguins R package1). We will import the data directly from GitHub, but if you would prefer to download it instead, click here.",
    "crumbs": [
      "Data Science",
      "12. Analysing Relationships"
    ]
  },
  {
    "objectID": "sessions/12-analysing-relationships/index.html#why-analyse-relationships",
    "href": "sessions/12-analysing-relationships/index.html#why-analyse-relationships",
    "title": "Regression Fundamentals: Analysing Relationships",
    "section": "Why Analyse Relationships?",
    "text": "Why Analyse Relationships?\nIn the first session in the regression foundations series we considered how to compare samples, and what comparisons between groups can tell us. We compared fatalities from car crashes between 4/20 and other days. But what if we wanted to examine how the number of fatalities changes as temperature increases or decreases? Or maybe we want to account for the number of cars on the road at different times of year? These questions require different tools.\n\nFrom Categorical to Continuous\nThe type of variables you are dealing with dictates what you can do to make inferences from your data. In our previous session, comparing samples, we were making comparisons by splitting our data into groups and comparing those groups. We compared the average number of car crash fatalities on 4/20 with the average number of car crash fatalities on all other days of the year. This is comparing the value of a continuous variable (fatalities) but split into categorical groups (4/20 and not 4/20). Here we will compare two continuous variables, considering how one variable (the outcome) changes in response to changes in the other variable (the predictor).\nThere is only a subtle difference between the idea of comparing samples and analysing relationships. You can frame a comparison between groups as analysing the relationship between the groups and the continuous variable, but you are still comparing the central tendency2 and dispersion3 for each group and inferring the relationship (or association) from this. When comparing two continuous variables, you can’t reduce either to their average, and are instead making statements about the way they vary together.\n\nVariable Types\nAt this point, it might be necessary to walk through variable types and what continuous and categorical variables really mean.\n\n\n\n\n\n\n\n\nVariable Type\nExample Values\nTypical Question\n\n\n\n\nContinuous\n5.2, 7.8, 102.3\nHow much?\n\n\nDiscrete\n1, 2, 3, 10\nHow many?\n\n\nCategorical (Nominal)\nRed, Blue, Green\nWhich type?\n\n\nCategorical (Ordinal)\nLow, Medium, High\nWhich level?\n\n\nBinary (Dichotomous)\nYes/No, Pass/Fail\nYes or no?\n\n\nTime/Date\n2025-06-10, 12:30 PM\nWhen?\n\n\nIdentifier\nID12345, username987\nWho or what? (unique)\n\n\n\nA nominal category has no natural order, while ordinal categories do.\n\n\n\nFrom Comparing Groups to Estimating Associations\nGroup comparison tells us whether Group A’s sample mean is meaningfully different from Group B’s, but this comparison doesn’t really tell us the size of the difference or the pattern in which the differences occur. If two samples are not drawn from the same population distribution, what does this really tell us? It is possible to structure our analysis such that this could be quite meaningful, but in many cases it won’t be.\nThis is why we need to take the next step, to analysing relationships. How do two variables move together? Strictly speaking, what we will be analysing today is less “relationships”, which implies causality, and more “associations”. We are unable to make claims about causality with the methods we are using, because we are only considering how variables vary together, and not directly estimating how one variable causes changes in another. Still, the methods we will discuss here get us one step closer to being able to measure effects and infer causality.\nIn this session, we’ll explore the penguins dataset to see how body mass relates to flipper length, how bill length relates to bill depth, and how to identify relationships between continuous traits.",
    "crumbs": [
      "Data Science",
      "12. Analysing Relationships"
    ]
  },
  {
    "objectID": "sessions/12-analysing-relationships/index.html#penguins-with-long-characteristics",
    "href": "sessions/12-analysing-relationships/index.html#penguins-with-long-characteristics",
    "title": "Regression Fundamentals: Analysing Relationships",
    "section": "Penguins with Long Characteristics",
    "text": "Penguins with Long Characteristics\nWe will use the Palmer Penguins dataset to analyse how the length of penguins’ body mass changes with flipper length and how penguin bill length is associated with bill depth. We can’t draw conclusions about causality from our data, but we are able to move from identifying differences to quantifying those differences. This takes us one step closer to making meaningful inferences.\n\nImport & Process Data\n\nimport pandas as pd\n\n# load penguins data from TidyTuesday URL\nurl = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-15/penguins.csv'\npenguins_raw = pd.read_csv(url)\npenguins_raw.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_len\nbill_dep\nflipper_len\nbody_mass\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\nThere are several missing values in this dataset. While we should generally be a little careful when discarding missing values, we will do so here just to simplify the process.\n\n# drop missing values\ndf = penguins_raw.dropna()\ndf.shape\n\n(333, 8)\n\n\n\n\nVisualising Relationships\nLet’s start by visualising how body mass varies by flipper length. We will split the data by species as well, in order to identify whether penguin species is a confounding factor4.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['figure.figsize'] = [8,6]\n\n# scatter of flipper length vs. body mass\nsns.scatterplot(\n    data=df,\n    x='flipper_len',\n    y='body_mass',\n    hue='species',\n    alpha=0.7\n)\n\nplt.xlabel('Flipper Length (mm)')\nplt.ylabel('Body Mass (g)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 1: The Relationship Between Flipper Length & Body Mass\n\n\n\n\n\nWhat patterns do you see in Figure 1? How does body mass change when flipper length increases? And how does species moderate the association between flipper length and body mass?\nFigure 1 shows that, when flipper length increases, body mass appears to increase. There is a clear pattern in the data, and though there are differences in the average value of flipper length and body mass when split by species, the species doesn’t appear to have a significant impact on the relationship between flipper length and body mass.\nWe can use the same plot to visualise the association between bill length and bill depth.\n\n# scatter of bill length vs. bill depth\nsns.scatterplot(\n    data=df,\n    x='bill_len',\n    y='bill_dep',\n    hue='species',\n    alpha=0.7\n)\n\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 2: The Relationship Between Bill Length & Bill Depth\n\n\n\n\n\nHow does the pattern in Figure 2 differ from Figure 1? What happens to bill depth when bill length changes, and how does species impact these changes?\nWhen you look at all the data in Figure 2 without accounting for species, the relationship appears to be very noisy. However, when factoring in species differences, the association looks positive.\nWe can also fit a regression line to our data, shown below in Figure 3, which will show us the “line of best fit” through bill length and bill depth.\n\n# add linear fit line\nsns.regplot(\n    data=df,\n    x='bill_len',\n    y='bill_dep',\n    scatter=True,\n    ci=95\n)\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 3: The Line of Best Fit Between Bill Length & Bill Depth\n\n\n\n\n\nIt is clear that any relationship between bill length and bill depth is more complicated than the relationship between flipper length and body mass. Not only does ignoring species make the association between these variables appear a lot more noisy, it also suggests that bill length is negatively associated with bill depth, which is the opposite conclusion to the conclusion we draw from Figure 2.\n\n\nComputing Correlations\nVisualising continuous variables using scatterplots can give us an indication of how two variables change together, but we are unable to quantify this association between two variables only using visualisations. We can put a number to what we are seeing in the plots by calculating the correlation between variables.\n\nPairwise Correlation\nMeasuring the correlation between variablse can be very useful as a way to explore data and gain quick insight into potential relationships between variables in your dataset. It is simple and intuitive, and serves as a good starting point for analysing data.\nWe can compute the correlation between two variables, using scipy.stats to calculate a specific measure of correlation, Pearson’s \\(r\\)5.\n\nfrom scipy.stats import pearsonr\n\nr, p = pearsonr(df['flipper_len'], df['body_mass'])\nprint(f\"Correlation (r) = {r:.2f}\")\n\nCorrelation (r) = 0.87\n\n\nA correlation of 0.87 is very strong. There is clearly a very strong association between flipper length and body mass. However, we can’t claim that flipper length causes body mass just based off this. Correlation does not imply causation6.\nWhen we visualised the relationship between bill length and bill depth, there appeared to be a grouping structure going on that complicated things, and the overall relationship appeared pretty noisy.\n\nr, p = pearsonr(df['bill_len'], df['bill_dep'])\nprint(f\"Correlation (r) = {r:.2f}\")\n\nCorrelation (r) = -0.23\n\n\nAs a result, the correlation score is much lower. A correlation of -0.23 tells us two things:\n\nThe negative correlation means that when bill length increases, bill depth tends to decrease.\nThe weaker correlation suggests that this decrease is a lot noisier, and it is much harder to estimate a penguin’s bill depth using their bill length.\n\nA correlation of +/- ~0.2 doesn’t necessarily mean there is no relationship. There are lots of ways correlation can mislead, because it is a limited measure. Visualising the relationship between bill length and bill depth showed us that species is highly relevant, and not factoring this in limits what we can say about this relationship.\n\n\nCorrelation Matrix\nWe may be interested in the pairwise correlation between multiple variables. If so, computing each correlation between pairs of variables is very cumbersome. Instead, we can compute a correlation matrix.\n\n# compute correlation matrix\n(\n    df.select_dtypes(include='number')\n    .corr()\n    .round(2)\n)\n\n\n\n\n\n\n\n\nbill_len\nbill_dep\nflipper_len\nbody_mass\nyear\n\n\n\n\nbill_len\n1.00\n-0.23\n0.65\n0.59\n0.03\n\n\nbill_dep\n-0.23\n1.00\n-0.58\n-0.47\n-0.05\n\n\nflipper_len\n0.65\n-0.58\n1.00\n0.87\n0.15\n\n\nbody_mass\n0.59\n-0.47\n0.87\n1.00\n0.02\n\n\nyear\n0.03\n-0.05\n0.15\n0.02\n1.00\n\n\n\n\n\n\n\nWe can also visualise a correlation matrix, shown below in Figure 4.\n\n# add correlation matrix to summarise relationships\ncorr_matrix = df.select_dtypes(include='number').corr()\n\n# plot heatmap\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n\nplt.show()\n\n\n\n\n\n\n\nFigure 4: Correlation Matrix\n\n\n\n\n\nIf you are concerned with a certain outcome and you want to quickly look at the correlation between all other continuous variables and the outcome, you can also compute this.\n\n# correlations of all numeric variables with body mass\n(\n    df.select_dtypes(include='number')\n    .corr()['body_mass']\n    .drop('body_mass')\n    .round(2)\n)\n\nbill_len       0.59\nbill_dep      -0.47\nflipper_len    0.87\nyear           0.02\nName: body_mass, dtype: float64\n\n\n\n\n\nCorrelation’s Limitations\nComputing correlation can be very informative, but there are a lot of ways it is limited. The most common method for calculating correlation, Pearson’s \\(r\\), assumes a linear pairwise relationship between variables. This means it is unable to capture certain relationships that do not meet these assumptions.\n\nNon-Linearity\nA correlation coefficient will miss strong non-linear relationships in data. We can demonstrate this by simulating a parabolic relationship (a u-shaped curve) between two variables, \\(x\\) and \\(y\\), where \\(y\\) is a function of \\(x\\).\n\nimport numpy as np\n\n# simulate a u‑shaped relationship example\nx_sim = np.linspace(-3, 3, 200)\ny_sim = x_sim**2 + np.random.normal(0, 1, 200)\nsim_data = pd.DataFrame({'x': x_sim, 'y': y_sim})\n\nWe know that the relationship between \\(x\\) and \\(y\\) is meaningful because we generated \\(y\\) from \\(x\\). However, when we calculate their correlation, it is tiny.\n\nr, p = pearsonr(sim_data['x'], sim_data['y'])\nprint(f\"Correlation (r) = {r:.2f}\")\n\nCorrelation (r) = 0.01\n\n\nFigure 5 visualises the relationship between \\(x\\) and \\(y\\), demonstrating that there is clearly a relationship between the two variables.\n\nsns.scatterplot(data=sim_data, x='x', y='y')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 5: A Non-Linear Relationship\n\n\n\n\n\nWhile there are correlation measures that handle non-linearity (such as Spearman’s rank correlation and Kendall’s tau (\\(\\tau\\))), they are much less common than Pearson’s \\(r\\).\n\n\nComplexity\nWhile the linearity assumption can cause Pearson’s \\(r\\) to miss strong non-linear relationships between variables, the biggest limiting factor with measuring correlations is the pairwise assumption. Correlation compares pairs of variables, which means treating the relationship between those pairs as independent, ignoring potential interactions with other variables. Very few relationships in the real world are strictly pairwise.\nWe saw an example of this earlier with correlation missing the relationship between bill length and bill depth because it couldn’t account for the group-level species effect.\nWe can account for grouping structures in our data visually, by plotting regression lines for each group. Figure 6 identifies the real story that correlation missed.\n\nsns.lmplot(data=df, x=\"bill_len\", y=\"bill_dep\", hue=\"species\", height=6, aspect=1.2)\n\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 6: Accounting for Grouping Structures\n\n\n\n\n\n\n\n\n\n\n\nThis also demonstrates why it is important to approach exploratory data analysis in a few different ways. While a correlation coefficient would have missed the relationship between bill length and bill depth, Figure 3 also would have suggested a negative relationship between two variables that are actually positively associated (though it is unclear if the relationship is causal). Meanwhile, Figure 2 suggested that species has a moderating effect on the relationship between the length of a penguin’s bill and its depth, and Figure 6 confirms the suspicions.\n\n\n\nCorrelation will not account for grouping structures, but it will also miss any other way that other variables can complicate the relationship between a pair of variables. Outcomes in the real world are rarely as simple as a pairwise relationship. We need different tools to capture this complexity.",
    "crumbs": [
      "Data Science",
      "12. Analysing Relationships"
    ]
  },
  {
    "objectID": "sessions/12-analysing-relationships/index.html#summary",
    "href": "sessions/12-analysing-relationships/index.html#summary",
    "title": "Regression Fundamentals: Analysing Relationships",
    "section": "Summary",
    "text": "Summary\nVisualising continuous variables and calculating their correlations can tell us a lot. These methods let us move beyond simple group comparisons to examine continuous associations. However, both approaches have important limitations: they struggle with non-linear patterns, they struggle with complexity, and most importantly, they cannot tell us whether the relationship between variables is causal.\nReal-world relationships rarely exist in isolation, so we need methods that can handle the complexity that occurs in the wild. This is where we turn to regression. Regression can handle multiple variables, account for confounding factors, and provide a more complete picture of how variables relate to each other in complex systems.",
    "crumbs": [
      "Data Science",
      "12. Analysing Relationships"
    ]
  },
  {
    "objectID": "sessions/12-analysing-relationships/index.html#footnotes",
    "href": "sessions/12-analysing-relationships/index.html#footnotes",
    "title": "Regression Fundamentals: Analysing Relationships",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd now a part of the Base R datasets package included as default in R installations (R &gt;= 4.5.0).↩︎\nThe central tendency describes the “typical” or “midpoint” value of your data. There are many ways to measure central tendency, but the most common are the mean (average value), median (middle value), and the mode (most common value).↩︎\nDispersion describes how much your data varies. Low dispersion means values are clustered tightly around the midpoint value, while high dispersion means your data can take a wide range of values, some much higher or lower than the midpoint. The most common measure of dispersion is standard deviation, which effectively measures the average distance that values fall from the mean.↩︎\nConfounding is when a third, unspecified variable influences the relationship between the explanatory variable(s) and the outcome. For example, ice cream sales is correlated with deaths by drowning, but temperature is the confounder (hot weather causes increases in both). A confounding factor creates the false impression that two variables are related or hides/distorts the real relationship.↩︎\nFor more information about Pearson’s \\(r\\), check out this blog post by Hoda Osama.↩︎\nCorrelation might not imply causation, but it is important to realise that the presence of correlation does not mean causation is not present. You just can’t conclude causation exists simply because you observe a correlation.↩︎",
    "crumbs": [
      "Data Science",
      "12. Analysing Relationships"
    ]
  },
  {
    "objectID": "sessions/08-functions/index.html",
    "href": "sessions/08-functions/index.html",
    "title": "Functions & Functional Programming",
    "section": "",
    "text": "This session is the third in a series of programming fundamentals. The concepts here can feel abstract at first, but they are a big part of how Python code is structured in real projects. By the end, you’ll see how functions make code shorter, cleaner, and easier to re-use.\nThe below slides aim to provide an introduction to these concepts and the way we can use them.",
    "crumbs": [
      "Core Concepts",
      "8. Functions & Functional Programming"
    ]
  },
  {
    "objectID": "sessions/08-functions/index.html#slides",
    "href": "sessions/08-functions/index.html#slides",
    "title": "Functions & Functional Programming",
    "section": "Slides",
    "text": "Slides\nUse the left ⬅️ and right ➡️ arrow keys to navigate through the slides below. To view in a separate tab/window, follow this link.",
    "crumbs": [
      "Core Concepts",
      "8. Functions & Functional Programming"
    ]
  },
  {
    "objectID": "sessions/08-functions/index.html#what-are-functions",
    "href": "sessions/08-functions/index.html#what-are-functions",
    "title": "Functions & Functional Programming",
    "section": "What are Functions?",
    "text": "What are Functions?\nA function is just a reusable set of instructions that takes input, does something with it, and gives you a result.\nIf you’ve used Excel, you already use functions all the time. For example, SUM(A1:A10) or VLOOKUP(…). You give them arguments (the input), they process it, and they return an output. If you’ve used SQL, it’s the same idea. COUNT(*), ROUND(price, 2), or UPPER(name) are functions. They save you from writing the same logic over and over, and they keep code tidy.\nIn Python, functions work the same way, but you can also write your own custom ones, so instead of just using what is built-in, you can create tools that do exactly what you need.",
    "crumbs": [
      "Core Concepts",
      "8. Functions & Functional Programming"
    ]
  },
  {
    "objectID": "sessions/08-functions/index.html#simple-built-in-user-defined-functions",
    "href": "sessions/08-functions/index.html#simple-built-in-user-defined-functions",
    "title": "Functions & Functional Programming",
    "section": "Simple Built-In & User-Defined Functions",
    "text": "Simple Built-In & User-Defined Functions\nPython already has many built-in functions which makes the language more functional.\n\nPrint Statements\nThe print() function sends output to the screen. It’s often the first Python function you use.\n\nprint(\"Hello, World!\")\n\nHello, World!\n\n\n\n\nComparing Operations\nWe’ll compare how to do things “manually” with loops vs. using Python’s built-in (or imported) functions. This shows how functions save time and reduce code.\n\nLength\nWe can count the number of items in a list using a for loop.\n\nvalues = [10, 20, 30, 40, 50]\n\nlength_manual = 0\nfor _ in values:\n    length_manual += 1\nprint(\"Length:\", length_manual)\n\nLength: 5\n\n\nHowever, it is much faster to just use len() instead.\n\nprint(\"Length:\", len(values))\n\nLength: 5\n\n\n\n\nSum\nWe can also sum the value of all the numbers in our values object.\n\ntotal_manual = 0\nfor val in values:\n    total_manual += val\nprint(\"Sum:\", total_manual)\n\nSum: 150\n\n\nOr we can use sum().\n\nprint(\"Sum:\", sum(values))\n\nSum: 150\n\n\n\n\nMean\nFinally, we can manually calculate the mean of our list of values by summing them and then dividing by the length of the list.\n\ntotal_for_mean = 0\ntotal_length = 0\n\nfor val in values:\n    total_for_mean += val\n\nfor val in values:\n    total_length += 1\n\nmean_manual = total_for_mean / total_length\nprint(\"Mean:\", mean_manual)\n\nMean: 30.0\n\n\nOr we can import numpy and use np.mean().\n\nimport numpy as np\n\nvalues = [10, 20, 30, 40, 50]\nprint(\"Mean:\", np.mean(values))\n\nMean: 30.0\n\n\n\n\n\nCombining Operations\nWe can create our own functions to group multiple calculations. The function below takes two numbers and returns a sentence describing their sum, difference, and product.\n\ndef summarise_numbers(a, b):\n\n    total = a + b\n    difference = a - b\n    product = a * b\n    return (\n        f\"The sum of {a} and {b} is {total}, \"\n        f\"the difference is {difference}, \"\n        f\"and their product is {product}.\"\n    )\n\nsummarise_numbers(10, 5)\n\n'The sum of 10 and 5 is 15, the difference is 5, and their product is 50.'\n\n\nTo illustrate how functions work, we can break them down step-by-step. def summarise_numbers(a, b) is the function header. def states that you are defining a function, summarise_numbers is the function name, and (a, b) is the input parameter (the numbers we are summarising). The function body (the indented code below the header) defines the steps the function should take, and the return statement declares the output from the function.",
    "crumbs": [
      "Core Concepts",
      "8. Functions & Functional Programming"
    ]
  },
  {
    "objectID": "sessions/08-functions/index.html#exploring-data-with-functions",
    "href": "sessions/08-functions/index.html#exploring-data-with-functions",
    "title": "Functions & Functional Programming",
    "section": "Exploring Data with Functions",
    "text": "Exploring Data with Functions\nWe can use functions to explore an entire dataset quickly and efficiently, where a manual process would require a lot of repetition.\n\nSetup\nFirst, we will import all of the packages we need.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\nfrom sklearn.datasets import fetch_california_housing\n\nsns.set_theme(style=\"whitegrid\")\n\n\n\nImport Data\nWe can then import the California housing dataset and store it in housing_raw, before previewing the housing_raw object.\n\nhousing_raw = fetch_california_housing(as_frame=True).frame\nhousing_raw.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n\n\n\n\n\n\n\nPreprocess Data\nWe’ll make a helper function to convert text to snake_case (lowercase with underscores). This is a common style for column names.\n\ndef to_snake_case(s: str) -&gt; str:\n    \"\"\"\n    Convert a given string to snake_case.\n    \"\"\"\n    s = s.strip()  # remove leading/trailing spaces\n    s = re.sub(r'[\\s-]+', '_', s)  # replace spaces and hyphens with underscores\n    s = re.sub(r'(?&lt;=[a-z])(?=[A-Z])', '_', s)  # add underscore before capital letters\n    s = re.sub(r'[^a-zA-Z0-9_]', '', s)  # remove anything not letter, number, or underscore\n    return s.lower()  # make everything lowercase\n\nThis function has the same basic structure as the function we defined earlier, but with some additional information that is good practice for writing reproducible code. In the function header, the input (s: str) includes the input parameter s and a type-hint starting that s should be a string. The -&gt; str immediately after states that the function will return a string. The triple-quoted text just below the function header describes what the function does. You can also include what the function expects and what it returns.\nNext, we can create a function that cleans our dataset, including the to_snake_case function as a step in the process. The other step is to drop all NAs and duplicates.\n\ndef preprocess_data(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Preprocess a dataframe by cleaning and standardizing column names.\n    \"\"\"\n    df = df.dropna().drop_duplicates().copy()  # remove missing rows and duplicates\n    df.columns = [to_snake_case(col) for col in df.columns]  # rename columns to snake_case\n    return df  # return cleaned dataframe\n\nWe can then apply this to our dataset.\n\ndf = preprocess_data(housing_raw)\ndf.head()\n\n\n\n\n\n\n\n\nmed_inc\nhouse_age\nave_rooms\nave_bedrms\npopulation\nave_occup\nlatitude\nlongitude\nmed_house_val\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n\n\n\n\n\n\n\nVisualise Distributions\nA great way to use functions for exploratory data analysis is for visualing multiple columns at once. If we visualise every column manually, this would require a lot of code. However, we can write a single function that returns a plot for every relevant column in a single figure.\nBelow is a function for plotting a histogram for each numeric column in a single figure.\n\ndef plot_numeric_columns(df: pd.DataFrame) -&gt; None:\n\n    \"\"\"\n    plot histograms for all numeric columns in one figure with subplots.\n    \"\"\"\n    \n    numeric_cols = df.select_dtypes(include=[np.number]).columns  # get numeric column names\n    n = len(numeric_cols)  # count how many numeric columns there are\n    if n == 0: # if there are no numeric columns  \n        print(\"no numeric columns found\") # tell the user\n        return # and stop the function\n\n    # determine how many plots per row (max 3)\n    ncols = min(n, 3)  # number of columns in subplot grid\n    nrows = (n + ncols - 1) // ncols  # number of rows in subplot grid (ceiling division)\n    fig, axes = plt.subplots(nrows, ncols, figsize=(6 * ncols, 4 * nrows))  # create figure and axes\n    if n == 1:  # if only one numeric column\n        axes = [axes]  # put single axis in a list for consistency\n    else:\n        axes = axes.flatten()  # flatten 2d array of axes into 1d list\n\n    for ax, col in zip(axes, numeric_cols):  # loop through axes and column names\n        ax.hist(df[col], bins=20, edgecolor=\"black\")  # draw histogram for column\n        ax.set_xlabel(col)  # set x-axis label\n        ax.set_ylabel(\"\")  # remove y-axis label\n\n    # remove any extra empty plots\n    for ax in axes[len(numeric_cols):]:  # loop over unused axes\n        fig.delaxes(ax)  # delete unused subplot\n\n    plt.tight_layout()  # adjust layout so plots don't overlap\n    plt.show()  # display the plots\n\nAnd then we can run this function on our California housing dataset.\n\nplot_numeric_columns(df)\n\n\n\n\n\n\n\n\nWe can do the same for categorical columns, using bar charts.\n\ndef plot_categorical_columns(df: pd.DataFrame) -&gt; None:\n    \n    \"\"\"\n    plot bar charts for all categorical columns in one figure with subplots.\n    \"\"\"\n    \n    cat_cols = df.select_dtypes(exclude=[np.number]).columns  # get non-numeric column names\n    n = len(cat_cols)  # count how many categorical columns there are\n    if n == 0:  # if there are no categorical columns\n        print(\"no categorical columns found\")  # tell the user\n        return  # and stop the function\n\n    # determine how many plots per row (max 3)\n    ncols = min(n, 3)  # number of columns in subplot grid\n    nrows = (n + ncols - 1) // ncols  # number of rows in subplot grid (ceiling division)\n    fig, axes = plt.subplots(nrows, ncols, figsize=(6 * ncols, 4 * nrows))  # create figure and axes\n    if n == 1:  # if only one categorical column\n        axes = [axes]  # put single axis in a list for consistency\n    else:\n        axes = axes.flatten()  # flatten 2d array of axes into 1d list\n\n    for ax, col in zip(axes, cat_cols):  # loop through axes and column names\n        df[col].value_counts().plot(kind=\"bar\", ax=ax, edgecolor=\"black\")  # draw bar chart\n        ax.set_xlabel(col)  # set x-axis label\n        ax.set_ylabel(\"\")  # remove y-axis label\n\n    # remove any extra empty plots\n    for ax in axes[len(cat_cols):]:  # loop over unused axes\n        fig.delaxes(ax)  # delete unused subplot\n\n    plt.tight_layout()  # adjust layout so plots don't overlap\n    plt.show()  # display the plots\n\nHowever, there are no categorical columns in this dataset1.\n\nplot_categorical_columns(df)\n\nno categorical columns found",
    "crumbs": [
      "Core Concepts",
      "8. Functions & Functional Programming"
    ]
  },
  {
    "objectID": "sessions/08-functions/index.html#summary",
    "href": "sessions/08-functions/index.html#summary",
    "title": "Functions & Functional Programming",
    "section": "Summary",
    "text": "Summary\nFunctions let you package steps into reusable, predictable tools. You will have used functions before in other settings, and when writing Python code you will regularly encounter built-in functions and functions imported from packages. The more you work in Python, the more you’ll see yourself building small helper functions to avoid repeating code.",
    "crumbs": [
      "Core Concepts",
      "8. Functions & Functional Programming"
    ]
  },
  {
    "objectID": "sessions/08-functions/index.html#exercises",
    "href": "sessions/08-functions/index.html#exercises",
    "title": "Functions & Functional Programming",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a function that returns the maximum and minimum values in a list.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\ndef min_max(lst):\n    return min(lst), max(lst)\n\nmin_max([4, 1, 9])\n\n(1, 9)\n\n\n\n\n\n\nModify summarise_numbers to also return the division result (a / b).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\ndef summarise_numbers(a, b):\n    total = a + b\n    difference = a - b\n    product = a * b\n    division = a / b\n    return total, difference, product, division\n\nsummarise_numbers(5, 10)\n\n(15, -5, 50, 0.5)\n\n\n\n\n\n\nWrite a function that counts how many even numbers are in a list.\n\nHint: This requires using a ‘modulo operator’2.\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\ndef count_evens(lst):\n    return sum(1 for x in lst if x % 2 == 0)\n\nvalues = [1, 2, 3, 4, 5]\ncount_evens(values)\n\n2\n\n\n\n\n\nFor the next three questions, you can use this sample dataset:\n\nsample_df = pd.DataFrame({\n    \"Name\": np.random.choice([\"Alice\", \"Bob\", \"Charlie\", \"John\"], size=20),\n    \"Department\": np.random.choice([\"HR\", \"IT\", \"Finance\"], size=20),\n    \"Age\": np.random.randint(21, 60, size=20),\n    \"Salary\": np.random.randint(30000, 80000, size=20),\n    \"Years_at_Company\": np.random.randint(1, 20, size=20)\n})\n\n\nCreate a function that takes a dataframe and returns only columns with numeric data.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\ndef select_numeric(df):\n    return df.select_dtypes(include=[np.number])\n\nselect_numeric(sample_df)\n\n\n\n\n\n\n\n\nAge\nSalary\nYears_at_Company\n\n\n\n\n0\n22\n64943\n2\n\n\n1\n45\n59528\n6\n\n\n2\n39\n55767\n3\n\n\n3\n40\n31503\n17\n\n\n4\n42\n55435\n7\n\n\n5\n49\n61878\n5\n\n\n6\n40\n30717\n12\n\n\n7\n31\n44472\n2\n\n\n8\n38\n58789\n19\n\n\n9\n28\n78885\n1\n\n\n10\n46\n54041\n4\n\n\n11\n47\n67443\n18\n\n\n12\n42\n40703\n4\n\n\n13\n29\n40503\n5\n\n\n14\n31\n66357\n1\n\n\n15\n24\n30358\n15\n\n\n16\n43\n79031\n10\n\n\n17\n29\n79512\n13\n\n\n18\n23\n45079\n18\n\n\n19\n45\n60638\n3\n\n\n\n\n\n\n\n\n\n\n\nRewrite plot_numeric_columns so it uses seaborn’s histplot instead of matplotlib’s hist.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\ndef plot_numeric_columns(df):\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    for col in numeric_cols:\n        sns.histplot(df[col], bins=20)\n        plt.show()\n\nplot_numeric_columns(sample_df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWrite a function that converts all string columns in a dataframe to lowercase.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\ndef lowercase_strings(df):\n    for col in df.select_dtypes(include=['object']):\n        df[col] = df[col].str.lower()\n    return df\n\nlowercase_strings(sample_df)\n\n\n\n\n\n\n\n\nName\nDepartment\nAge\nSalary\nYears_at_Company\n\n\n\n\n0\nbob\nhr\n22\n64943\n2\n\n\n1\nbob\nit\n45\n59528\n6\n\n\n2\nalice\nhr\n39\n55767\n3\n\n\n3\ncharlie\nfinance\n40\n31503\n17\n\n\n4\nalice\nit\n42\n55435\n7\n\n\n5\nalice\nhr\n49\n61878\n5\n\n\n6\nbob\nhr\n40\n30717\n12\n\n\n7\nalice\nhr\n31\n44472\n2\n\n\n8\ncharlie\nfinance\n38\n58789\n19\n\n\n9\ncharlie\nhr\n28\n78885\n1\n\n\n10\ncharlie\nit\n46\n54041\n4\n\n\n11\ncharlie\nhr\n47\n67443\n18\n\n\n12\nbob\nhr\n42\n40703\n4\n\n\n13\nalice\nit\n29\n40503\n5\n\n\n14\njohn\nit\n31\n66357\n1\n\n\n15\ncharlie\nfinance\n24\n30358\n15\n\n\n16\nbob\nfinance\n43\n79031\n10\n\n\n17\njohn\nit\n29\n79512\n13\n\n\n18\nbob\nhr\n23\n45079\n18\n\n\n19\nalice\nit\n45\n60638\n3",
    "crumbs": [
      "Core Concepts",
      "8. Functions & Functional Programming"
    ]
  },
  {
    "objectID": "sessions/08-functions/index.html#footnotes",
    "href": "sessions/08-functions/index.html#footnotes",
    "title": "Functions & Functional Programming",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is just here as a demonstration of how you would do this with categorical columns, and to show what it would look like if the function cannot find relevant columns and stops early.↩︎\nIf you don’t know what a modulo operator is (totally understandable), you can search this online and it will likely help you find the answer to this question. It is always okay (encouraged, even) to search for answers to code questions online.↩︎",
    "crumbs": [
      "Core Concepts",
      "8. Functions & Functional Programming"
    ]
  },
  {
    "objectID": "sessions/15-beyond-linearity/index.html",
    "href": "sessions/15-beyond-linearity/index.html",
    "title": "Regression Fundamentals: Beyond Linearity",
    "section": "",
    "text": "In previous sessions, we learned how linear regression works: fit a straight line to data, minimise prediction error, and use that line for prediction and inference. Linear regression is powerful, but it assumes relationships are linear. In reality, data often follows curves, has constraints, or exhibits patterns that straight lines can’t capture.\nThis session explores how the linear regression framework is adapted to non-linear patterns, using generalised linear models (GLMs), through link functions. We’ll see that the same core idea, fitting a line through data to describe the relationship between variables, works for binary outcomes, counts, and other constrained data types. The key is transforming the scale so linear models can work.\nWe’ll focus on logistic regression for binary outcomes, implement it with real data, and briefly introduce other extensions like multilevel models and GAMs.",
    "crumbs": [
      "Data Science",
      "15. Beyond Linearity"
    ]
  },
  {
    "objectID": "sessions/15-beyond-linearity/index.html#when-linear-regression-fails",
    "href": "sessions/15-beyond-linearity/index.html#when-linear-regression-fails",
    "title": "Regression Fundamentals: Beyond Linearity",
    "section": "When Linear Regression Fails",
    "text": "When Linear Regression Fails\nLinear regression assumes the relationship between predictors and outcome is linear. This works when each unit increase in \\(X\\) produces a constant change in \\(Y\\). But many real-world relationships don’t follow this pattern.\n\nExamples of Non-Linear Data\n\nBinary outcomes - Survived/died, yes/no, pass/fail. Outcomes are 0 or 1, not continuous.\nCount data - Number of hospital visits, customer complaints. Must be non-negative integers.\nProportions - Percentage passing an exam, recovery rates. Bounded between 0 and 1.\nGrowth curves - Disease spread, population growth. Exponential or S-shaped patterns.\n\nForcing linear regression onto these data types produces nonsensical predictions (probabilities above 1, negative counts) and violates model assumptions.",
    "crumbs": [
      "Data Science",
      "15. Beyond Linearity"
    ]
  },
  {
    "objectID": "sessions/15-beyond-linearity/index.html#the-problem-illustrated",
    "href": "sessions/15-beyond-linearity/index.html#the-problem-illustrated",
    "title": "Regression Fundamentals: Beyond Linearity",
    "section": "The Problem Illustrated",
    "text": "The Problem Illustrated\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\n\n# set visualisation style\nsns.set_theme(style=\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# simulate binary outcome data\nnp.random.seed(42)\nage = np.linspace(20, 80, 100)\nprob_true = 1 / (1 + np.exp(-(age - 50) / 8))\ndisease = np.random.binomial(1, prob_true)\n\n# fit linear regression\nmodel_linear = LinearRegression()\nmodel_linear.fit(age.reshape(-1, 1), disease)\npred_linear = model_linear.predict(age.reshape(-1, 1))\n\n# plot\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.scatterplot(x=age, y=disease, facecolors='white', edgecolors='black',\n                s=60, linewidths=1, alpha=0.7, ax=ax)\nsns.lineplot(x=age, y=pred_linear, color='#ED8B00', linewidth=2, ax=ax, label='Linear Model')\nax.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Valid Range')\nax.axhline(y=1, color='red', linestyle='--', alpha=0.5)\nax.set_xlabel('Age')\nax.set_ylabel('Disease (0 = No, 1 = Yes)')\nax.set_title('Linear Regression on Binary Data: Predictions Outside Valid Range')\nax.set_ylim(-0.2, 1.2)\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nLinear regression predicts values below 0 and above 1, which make no sense for binary outcomes.",
    "crumbs": [
      "Data Science",
      "15. Beyond Linearity"
    ]
  },
  {
    "objectID": "sessions/15-beyond-linearity/index.html#the-solution---link-functions",
    "href": "sessions/15-beyond-linearity/index.html#the-solution---link-functions",
    "title": "Regression Fundamentals: Beyond Linearity",
    "section": "The Solution - Link Functions",
    "text": "The Solution - Link Functions\nGeneralised linear models adapt the linear model framework to work with non-linear data. Link functions transform the scale so that a linear model fits the data. Instead of fitting a line directly to constrained data, we:\n\nTransform the outcome to an unbounded scale\nFit a linear model on the transformed scale\nTransform predictions back to the original scale\n\nThis preserves the linearity of the model while respecting data constraints.\n\n\nHow Logistic Regression Works (Click to Expand)\n\n\nThe Logit Link\nFor binary outcomes, we use the logit link, which models the log-odds.\n\\[\n\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1X\n\\]\nThe left side of this equation is the log-odds for an outcome of probability \\(p\\). This can be any value from \\(-\\infty\\) to \\(+\\infty\\). This is equivalent to the right side of the equation, which is the linear predictor we have encountered in previous sessions.\nTo get probabilities, we transform back using the inverse logit.\n\\[\np = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X)}}\n\\]\nThis produces an S-shaped curve that stays between 0 and 1.\n\n\nWhy Log-Odds?\nOdds represent the ratio of success to failure: \\(\\frac{p}{1-p}\\). If \\(p = 0.8\\), odds are \\(\\frac{0.8}{0.2} = 4\\) (4 to 1 in favour). Taking the log makes this unbounded:\n\n\\(p = 0.5\\) → odds = 1 → log-odds = 0\n\\(p\\) close to 0 → odds close to 0 → log-odds \\(\\to -\\infty\\)\n\\(p\\) close to 1 → odds \\(\\to \\infty\\) → log-odds \\(\\to +\\infty\\)\n\nNow we can fit a linear model on the log-odds scale.",
    "crumbs": [
      "Data Science",
      "15. Beyond Linearity"
    ]
  },
  {
    "objectID": "sessions/15-beyond-linearity/index.html#logistic-regression-in-practice",
    "href": "sessions/15-beyond-linearity/index.html#logistic-regression-in-practice",
    "title": "Regression Fundamentals: Beyond Linearity",
    "section": "Logistic Regression in Practice",
    "text": "Logistic Regression in Practice\nLet’s implement logistic regression using the Titanic dataset. Our goal is to predict passenger survival based on characteristics like age, sex, and passenger class.\n\nLoad and Prepare Data\n\n# load titanic data\nurl = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\ntitanic = pd.read_csv(url)\n\n# select relevant columns and drop missing values\ndf = titanic[['Survived', 'Pclass', 'Sex', 'Age', 'Fare']].dropna()\n\n# convert sex to binary\ndf['Sex'] = (df['Sex'] == 'female').astype(int)\n\nprint(f\"Dataset shape: {df.shape}\")\nprint(\"\\nFirst few rows:\")\nprint(df.head(10))\n\nDataset shape: (714, 5)\n\nFirst few rows:\n    Survived  Pclass  Sex   Age     Fare\n0          0       3    0  22.0   7.2500\n1          1       1    1  38.0  71.2833\n2          1       3    1  26.0   7.9250\n3          1       1    1  35.0  53.1000\n4          0       3    0  35.0   8.0500\n6          0       1    0  54.0  51.8625\n7          0       3    0   2.0  21.0750\n8          1       3    1  27.0  11.1333\n9          1       2    1  14.0  30.0708\n10         1       3    1   4.0  16.7000\n\n\n\n\nExploratory Data Analysis (Click to Expand)\n\nBefore modelling, examine the relationships between predictors and survival.\n\n# overall survival rate\nsurvival_rate = df['Survived'].mean()\nprint(f\"Overall survival rate: {survival_rate:.1%}\")\n\n# survival by sex\nsurvival_sex = df.groupby('Sex')['Survived'].mean()\nprint(\"\\nSurvival rate by sex:\")\nprint(survival_sex.to_frame().rename(columns={'Survived': 'Survival Rate'}))\n\nOverall survival rate: 40.6%\n\nSurvival rate by sex:\n     Survival Rate\nSex               \n0         0.205298\n1         0.754789\n\n\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# survival by sex\nsurvival_sex = df.groupby('Sex')['Survived'].mean()\naxes[0, 0].bar(['Male', 'Female'], survival_sex.values, color=['#005EB8', '#ED8B00'])\naxes[0, 0].set_ylabel('Survival Rate')\naxes[0, 0].set_title('Survival Rate by Sex')\naxes[0, 0].set_ylim(0, 1)\n\n# survival by class\nsurvival_class = df.groupby('Pclass')['Survived'].mean()\naxes[0, 1].bar(survival_class.index, survival_class.values, color='#005EB8')\naxes[0, 1].set_xlabel('Passenger Class')\naxes[0, 1].set_ylabel('Survival Rate')\naxes[0, 1].set_title('Survival Rate by Class')\naxes[0, 1].set_ylim(0, 1)\n\n# age distribution by survival\nsns.histplot(data=df, x='Age', hue='Survived', bins=30, ax=axes[1, 0], \n             palette={0: '#ED8B00', 1: '#005EB8'}, alpha=0.6)\naxes[1, 0].set_title('Age Distribution by Survival')\naxes[1, 0].set_xlabel('Age')\naxes[1, 0].legend(['Died', 'Survived'])\n\n# fare distribution by survival\nsns.histplot(data=df, x='Fare', hue='Survived', bins=30, ax=axes[1, 1],\n             palette={0: '#ED8B00', 1: '#005EB8'}, alpha=0.6, log_scale=True)\naxes[1, 1].set_title('Fare Distribution by Survival (Log Scale)')\naxes[1, 1].set_xlabel('Fare (£)')\naxes[1, 1].legend(['Died', 'Survived'])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWomen, first-class passengers, and children had higher survival rates. Fare correlates with class.\n\n\n\nFitting a Logistic Regression Model\nWe’ll fit two models: a simple model with only sex as a predictor, and a multiple model with all predictors.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# prepare data\nX = df[['Sex', 'Age', 'Pclass', 'Fare']]\ny = df['Survived']\n\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# fit logistic regression\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train, y_train)\n\nLogisticRegression(max_iter=1000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n1000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nInterpreting Coefficients\n\nimport statsmodels.api as sm\n\n# use statsmodels for detailed output\nX = sm.add_constant(X)\nglm = sm.Logit(y, X).fit()\n\nprint(glm.summary().tables[1])\n\nOptimization terminated successfully.\n         Current function value: 0.453242\n         Iterations 6\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          2.4698      0.525      4.707      0.000       1.442       3.498\nSex            2.5182      0.208     12.115      0.000       2.111       2.926\nAge           -0.0367      0.008     -4.780      0.000      -0.052      -0.022\nPclass        -1.2697      0.159     -8.005      0.000      -1.581      -0.959\nFare           0.0005      0.002      0.246      0.805      -0.004       0.005\n==============================================================================\n\n\nCoefficients are on the log-odds scale. Positive coefficients increase the probability of survival, negative coefficients decrease it.\n\nSex (Female) - Coefficient = 2.52. Being female increases log-odds of survival. On the probability scale, women had much higher survival rates.\nAge - Coefficient = -0.037. Younger passengers had slightly higher survival rates.\nPclass - Coefficient = -1.27. Higher class numbers (lower class) decrease survival probability.\nFare - Coefficient = 0.001. Higher fares (correlated with class) increase survival.\n\n\n\nConverting Coefficients to Odds Ratios\nExponentiate coefficients to get odds ratios.\n\nodds_ratios = np.exp(glm.params)\nconf_int = np.exp(glm.conf_int())\n\nresults_df = pd.DataFrame({\n    'Odds Ratio': odds_ratios,\n    '95% CI Lower': conf_int[0],\n    '95% CI Upper': conf_int[1]\n}).round(2)\n\nprint(\"\\nOdds Ratios:\")\nprint(results_df)\n\n\nOdds Ratios:\n        Odds Ratio  95% CI Lower  95% CI Upper\nconst        11.82          4.23         33.05\nSex          12.41          8.25         18.65\nAge           0.96          0.95          0.98\nPclass        0.28          0.21          0.38\nFare          1.00          1.00          1.00\n\n\n\nSex - Being female multiplies the odds of survival by ~11. Women were much more likely to survive.\nAge - Each year of age multiplies odds by ~0.97 (slight decrease).\nPclass - Each class decrease (e.g., 2nd to 3rd) multiplies odds by ~0.33.\n\n\n\nMaking Predictions\n\n# predict probabilities on test set\ny_pred_prob = clf.predict_proba(X_test)[:, 1]\ny_pred = clf.predict(X_test)\n\n# model performance\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test set accuracy: {accuracy:.1%}\")\n\n# confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\")\nprint(pd.DataFrame(cm, \n                   columns=['Predicted: Died', 'Predicted: Survived'],\n                   index=['Actual: Died', 'Actual: Survived']))\n\n# classification report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Died', 'Survived']))\n\nTest set accuracy: 75.5%\n\nConfusion Matrix:\n                  Predicted: Died  Predicted: Survived\nActual: Died                   68                   19\nActual: Survived               16                   40\n\nClassification Report:\n              precision    recall  f1-score   support\n\n        Died       0.81      0.78      0.80        87\n    Survived       0.68      0.71      0.70        56\n\n    accuracy                           0.76       143\n   macro avg       0.74      0.75      0.75       143\nweighted avg       0.76      0.76      0.76       143\n\n\n\nThe model correctly classifies ~76% of passengers. It performs better at predicting deaths than survivals, likely because more passengers died overall.\n\n\nVisualising Predictions\n\n# create a grid of ages and predict survival probability for male/female passengers\nage_range = np.linspace(0, 80, 100)\n\n# predictions for male, 3rd class passengers\nX_male = pd.DataFrame({\n    'Sex': 0,\n    'Age': age_range,\n    'Pclass': 3,\n    'Fare': df['Fare'].median()\n})\n\n# predictions for female, 3rd class passengers\nX_female = pd.DataFrame({\n    'Sex': 1,\n    'Age': age_range,\n    'Pclass': 3,\n    'Fare': df['Fare'].median()\n})\n\nprob_male = clf.predict_proba(X_male)[:, 1]\nprob_female = clf.predict_proba(X_female)[:, 1]\n\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.lineplot(x=age_range, y=prob_male, color='#005EB8', linewidth=2, \n             label='Male, 3rd Class', ax=ax)\nsns.lineplot(x=age_range, y=prob_female, color='#ED8B00', linewidth=2,\n             label='Female, 3rd Class', ax=ax)\nax.set_xlabel('Age')\nax.set_ylabel('Probability of Survival')\nax.set_title('Predicted Survival Probability by Age and Sex (3rd Class)')\nax.set_ylim(0, 1)\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFemale passengers had much higher predicted survival probabilities across all ages. Younger passengers (especially children) had higher survival rates, consistent with “women and children first”.",
    "crumbs": [
      "Data Science",
      "15. Beyond Linearity"
    ]
  },
  {
    "objectID": "sessions/15-beyond-linearity/index.html#other-extensions-of-linear-regression",
    "href": "sessions/15-beyond-linearity/index.html#other-extensions-of-linear-regression",
    "title": "Regression Fundamentals: Beyond Linearity",
    "section": "Other Extensions of Linear Regression",
    "text": "Other Extensions of Linear Regression\nLogistic regression is just one example of adapting the linear framework to non-linear data, but there are many ways this framework can be extended.\n\nPoisson Regression for Count Data\nWhen outcomes are counts (number of events, visits, occurrences), Poisson regression uses a log link.\n\n\nModel Formula (Click to Expand)\n\n\\[\n\\log(\\text{E}[Y]) = \\beta_0 + \\beta_1X\n\\]\nTransform back to get expected count: \\(\\text{E}[Y] = e^{\\beta_0 + \\beta_1X}\\). Predictions are always positive, matching count data.\n\nExample use cases - Number of hospital admissions, customer complaints, goals scored.\n\n\nGeneralised Additive Models (GAMs)\nGAMs fit smooth, flexible curves instead of straight lines.\n\n\nModel Formula (Click to Expand)\n\n\\[\nY = \\beta_0 + f_1(X_1) + f_2(X_2) + \\epsilon\n\\]\nEach \\(f_i(X_i)\\) is a smooth function that adapts to the data. GAMs capture complex non-linear patterns (U-shapes, wiggles) while remaining interpretable.\n\nExample use cases - Temperature effects on sales, age-related health trends, non-linear dose-response relationships.\nPython implementation uses pygam:\nfrom pygam import GAM, s\n\n# fit a GAM with smooth functions\ngam = GAM(s(0) + s(1))\ngam.fit(X, y)\n\n\nMultilevel Models\nWhen data has hierarchical structure (students in schools, patients in hospitals, repeated measures on individuals), multilevel models account for grouping.\n\n\nModel Formula (Click to Expand)\n\n\\[\nY_{ij} = \\beta_0 + u_j + \\beta_1X_{ij} + \\epsilon_{ij}\n\\]\n\n\\(u_j\\) is a group-specific effect (random intercept).\nGroups can have different baselines but share information.\n\n\nExample use cases - Educational data (students in schools), clinical trials (patients in sites), longitudinal data (repeated measures on individuals).\nPython implementation uses statsmodels or pymer4:\nimport statsmodels.formula.api as smf\n\n# fit a multilevel model\nmlm = smf.mixedlm(\"y ~ x\", data=df, groups=df[\"group_id\"])\nresult = mlm.fit()\n\n\nSurvival Analysis\nWhen modelling time until an event (death, failure, recovery), use survival models like Cox proportional hazards regression. These account for censoring (observations where the event hasn’t occurred yet).\nExample use cases - Patient survival times, equipment failure, customer churn.",
    "crumbs": [
      "Data Science",
      "15. Beyond Linearity"
    ]
  },
  {
    "objectID": "sessions/15-beyond-linearity/index.html#why-this-matters",
    "href": "sessions/15-beyond-linearity/index.html#why-this-matters",
    "title": "Regression Fundamentals: Beyond Linearity",
    "section": "Why This Matters",
    "text": "Why This Matters\nAll these models share the same foundation: linear regression. Once you understand fitting a line to data, you can:\n\nUse link functions for constrained outcomes (binary, counts, proportions)\nFit flexible curves with GAMs\nAccount for hierarchy with multilevel models\nModel time-to-event data with survival analysis\n\nThe linear framework is incredibly versatile. It’s not just for linear relationships.",
    "crumbs": [
      "Data Science",
      "15. Beyond Linearity"
    ]
  },
  {
    "objectID": "sessions/15-beyond-linearity/index.html#summary",
    "href": "sessions/15-beyond-linearity/index.html#summary",
    "title": "Regression Fundamentals: Beyond Linearity",
    "section": "Summary",
    "text": "Summary\n\nReal data often violates linear regression assumptions\nLink functions transform data so linear models work\nLogistic regression (logit link) handles binary outcomes\nOther GLMs (Poisson, etc.) handle counts and constrained data\nExtensions like GAMs and multilevel models add flexibility\nThe linear framework adapts to almost any problem\n\nLinear regression isn’t just a starting point. It’s a foundation you can build on for almost any modelling task.",
    "crumbs": [
      "Data Science",
      "15. Beyond Linearity"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html",
    "href": "sessions/01-onboarding/index.html",
    "title": "Python Onboarding",
    "section": "",
    "text": "This is the first session of Code Club’s relaunch. It focuses on giving users all the tools they need to get started using Python and demonstrates the setup for a typical Python project.",
    "crumbs": [
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#session-slides",
    "href": "sessions/01-onboarding/index.html#session-slides",
    "title": "Python Onboarding",
    "section": "Session Slides",
    "text": "Session Slides\nUse the left ⬅️ and right ➡️ arrow keys to navigate through the slides below. To view in a separate tab/window, follow this link.",
    "crumbs": [
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#the-tools-you-will-need",
    "href": "sessions/01-onboarding/index.html#the-tools-you-will-need",
    "title": "Python Onboarding",
    "section": "The Tools You Will Need",
    "text": "The Tools You Will Need\nWhile this course focuses on Python, we will use several other tools throughout.\n\nLanguage: Python\nDependency Management & Virtual Environments: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code/Jupyter Notebooks (or your preferred IDE)\n\nYou can install all the tools you’ll need by running the following one-liner run in PowerShell:\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop\n\nPython\nPython is an all-purpose programming language that is one of, if not the most popular, in the world1 and is widely used in almost every industry. Its popularity is owed to its flexibility as a language that can be used to achieve nearly any job. It is often referred to as the second-best tool for every job. Specialist languages might be better for specific tasks (for example, R for statistics), but Python is good at everything.\nIt is a strong choice for data science and analytics, being one of the best languages for data wrangling, data visualisation, statistics, and machine learning. It is also well-suited to web development, scientific computing, and automation.\n\n\nDependency Management\nOne of Python’s greatest weaknesses is dependency management. Despite its many strengths, there is no escaping the dependency hell that every Python user faces.\nDependency management refers to the process of tracking and managing all of the packages (dependencies) a project needs to run. It is a consideration in any programming language. It ensures:\n\nThe right packages are installed.\nThe correct versions are used.\nConflicts between packages are avoided.\n\nThere are many reasons that Python handles dependency management so poorly, but there are some tools that make this a little easier on users. We are using uv for dependency management. It is relatively new, but it is quickly becoming the consensus tool for dependency management in Python because it makes the process about as painless as it can be without moving to a different language entirely.\n\nVirtual Environments\nVirtual environments are a component of dependency management. Dependency management becomes much messier when you have many Python projects, each using their own packages, some overlapping and some requiring specific versions, either for compatibility or functionality reasons. Reducing some of this friction by isolating each project in its own virtual environment, like each project is walled off from all other projects, makes dependency management a little easier. Virtual environments allow you to manage dependencies for a specific project without the state of those dependencies affecting other projects or your wider system.\nVirtual environments help by:\n\nKeeping dependencies separate for each project.\nAvoiding version conflicts between projects.\nMaking dependency management more predictable and reproducible.\n\nWe will use uv to manage all dependencies, virtual environments, and even versions of Python.\n\n\n\nVersion Control\nVersion control is the practice of tracking and managing changes to code or files over time, allowing you to:\n\nRevert to earlier versions if needed.\nCollaborate with others on the same project easily.\nMaintain a history of changes.\n\nWe are using Git, a version control system, to host our work and GitHub Desktop to manage version control locally.\nVersion control and Git are topics entirely in their own right, and covering them in detail is out of the scope of this session. We hope to cover version control in a future session, but right now, you just need to be able to access materials for these sessions. You can find the materials in the Code Club repository.\nIf you have downloaded GitHub Desktop, the easiest way to access these materials and keep up-to-date is by cloning the Code Club repository (go to File, then Clone Repository, select URL, and paste the Code Club repository link in the URL field). You can then ensure that the materials you are using are the most current by clicking the Fetch Origin button in GitHub Desktop, which grabs the changes we’ve made from the central repository on GitHub.\n\n\nIDE\nIDEs (Integrated Development Environments) are software that simplifies programming and development by combining many of the most common tasks and helpful features for programming into a single tool. These typically include a code editor, debugging functionality, build tools, and features like syntax highlighting and code completion. When you start your code journey, you might not need all these tools, and fully-featured IDEs can be overwhelming. But as you become more comfortable with programming, all these different features will become very valuable.\nSome common IDEs that are used for Python include:\n\nVS Code\nPyCharm\nVim\nJupyter Notebooks/JupyterLab\nPositron\n\nWe will use VS Code or Jupyter Notebooks (which is not exactly an IDE but is similar).",
    "crumbs": [
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#project-setup",
    "href": "sessions/01-onboarding/index.html#project-setup",
    "title": "Python Onboarding",
    "section": "Project Setup",
    "text": "Project Setup\nEvery new Python project should start with using uv to set up a virtual environment for the project to keep everything organised and reduce the risk of finding yourself in dependency hell.\nThe entire project setup process can be handled in the command line. We will use PowerShell for consistency.\nWhen you open a PowerShell window, it should open in your C drive (e.g.,  C:\\Users\\user.name). If it does not, run cd ~, and it should return to your home directory.\nWe will create a new uv project in the home directory2 using the command uv init. The new project will contain everything we need, including a Python installation, a virtual environment, and the necessary project files for tracking and managing any packages installed in the virtual environment. To set up a new project called test-project, use the following command:\nuv init test-project\nHaving created this new directory, navigate to it using cd test-project. You can check the files in a directory using the command ls. If you run this command, you will see three files in the project directory (hello.py, pyproject.toml, and README.md). The project doesn’t yet have a Python installation or a virtual environment, but this will be added when we add external Python packages.\nYou can install Python packages using the command uv add. We can add some common Python packages that we will use in most projects (pandas, numpy, seaborn, and ipykernel3) using the following command:\nuv add pandas numpy seaborn ipykernel\nThe output from this command will reference the Python installation used and the creation of a virtual environment directory .venv. Now, if you run ls, you will see two new items in the directory, uv.lock and .venv.\nYour Python project is now set up, and you are ready to start writing some code. You can open VS Code from your PowerShell window by running code ..\nFor more information about creating and managing projects using uv, check out the uv documentation.\n\nOpening your project in VS Code\nTo open your newly-created uv project in VS Code, launch the application and click File &gt; Open Folder.... You’ll want to make sure you select the root level of your project. Once you’ve opened the folder, the file navigation pane in VS Code should display the files that uv has created, including a main.py example file. Click on this to open it.\nOnce VS Code realises you’ve opened a folder with Python code and a virtual environment, it should do the following:\n\nSuggest you install the Python extension (and, once you’ve created a Jupyter notebook, the Jupyter one) offered by Microsoft - go ahead and do this. If this doesn’t happen, you can install extensions manually from the Extensions pane on the left-hand side.\nSelect the uv-created .venv as the python Environment we’re going to use to actually run our code. If this doesn’t happen, press ctrl-shift-P, type “python environment” to find the Python - Create Environment... option, hit enter, choose “Venv” and proceed to “Use Existing”.\n\nIf all has gone well, you should be able to hit the “play” icon in the top right to execute main.py. The Terminal pane should open up below and display something like Hello from (your-project-name)!.",
    "crumbs": [
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#footnotes",
    "href": "sessions/01-onboarding/index.html#footnotes",
    "title": "Python Onboarding",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough there are several ways to measure language popularity, the PYPL Index, HackerRank’s Developer Skills Report, and IEEE Spectrum all rank Python as the most popular language in the world, while Stack Overflow’s Developer Survey places Python third behind JavaScript and HTML/CSS.↩︎\nWe recommend using the C drive for all Python projects, especially if using version control. Storing projects like these on One Drive will create many unnecessary issues.↩︎\nStrictly speaking, we should install ipykernel as a development dependency (a dependency that is needed for any development but not when the project is put into production). In this case, we would add it by running uv add --dev ipykernel. However, in this case, it is simpler to just add it as a regular dependency, and it doesn’t harm.↩︎",
    "crumbs": [
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/13-intro-linear-regression/index.html",
    "href": "sessions/13-intro-linear-regression/index.html",
    "title": "Regression Fundamentals: Introduction to Linear Regression",
    "section": "",
    "text": "This is the third session in our regression foundations series. Previous sessions discussed how to evaluate the differences between samples and the relationships between pairs of variables. This session will focus on bringing all of that together in order to make more precise inferences and predictions using data.\nWe’ll continue using the Palmer Penguins dataset to build intuition for regression concepts, then apply these methods to understand what drives penguin body mass and how we can predict it from other physical characteristics.",
    "crumbs": [
      "Data Science",
      "13. Introduction to Linear Regression"
    ]
  },
  {
    "objectID": "sessions/13-intro-linear-regression/index.html#what-is-regression",
    "href": "sessions/13-intro-linear-regression/index.html#what-is-regression",
    "title": "Regression Fundamentals: Introduction to Linear Regression",
    "section": "What is Regression?",
    "text": "What is Regression?\nRegression is a method for modelling the relationship between one or more explanatory variables (or predictors) and an outcome of interest. The goal of regression is to find a function that predicts the outcome from the predictors.\nRegression is incredibly flexible in both approach and application. It can fit to all kinds of data problem and can be used for descriptive, explanatory, and predictive analysis. For this reason, it is a foundational tool in statistics and data science.",
    "crumbs": [
      "Data Science",
      "13. Introduction to Linear Regression"
    ]
  },
  {
    "objectID": "sessions/13-intro-linear-regression/index.html#why-build-models",
    "href": "sessions/13-intro-linear-regression/index.html#why-build-models",
    "title": "Regression Fundamentals: Introduction to Linear Regression",
    "section": "Why Build Models?",
    "text": "Why Build Models?\nIn our previous sessions, we compared groups (4/20 vs other days) and examined relationships (penguin flipper length vs body mass). But both approaches had limitations: group comparisons can’t handle continuous predictors, and correlation only measures linear association between two variables at a time.\nRegression bridges these gaps. It lets us:\n\nQuantify how much \\(Y\\) changes when \\(X\\) increases by one unit.\nInclude multiple predictors simultaneously.\nMake predictions for new observations.\nTest specific hypotheses about relationships.\nAccount for confounding variables.\n\nInstead of just knowing flipper length and body mass are correlated, we can say that each additional millimeter increase in flipper length predicts an \\(X\\) increase in body mass. And we can go much further than this, controlling for environment, treating species as a grouping structure in the data, or facotoring in non-linearity. With regression, the possibilities are practically endless.\n\nFrom Association to Inference & Prediction\nCorrelation tells us variables move together. Regression tells us by how much and in what direction, and gives us the tools to make inferences and predictions.\nConsider these questions that regression can answer but correlation cannot:\n\nIf a penguin’s flipper is 200mm, what body mass do we predict?\nHow does the flipper-mass relationship differ between species?\nWhat’s our uncertainty around these predictions?\nWhich variables matter most for predicting body mass?",
    "crumbs": [
      "Data Science",
      "13. Introduction to Linear Regression"
    ]
  },
  {
    "objectID": "sessions/13-intro-linear-regression/index.html#the-linear-regression-model",
    "href": "sessions/13-intro-linear-regression/index.html#the-linear-regression-model",
    "title": "Regression Fundamentals: Introduction to Linear Regression",
    "section": "The Linear Regression Model",
    "text": "The Linear Regression Model\nThe most common type of regression model, and the foundation from which so many other types of regression model are built, is linear regression. Linear regression assumes the relationship between the predictors and the outcome are linear, meaning that when predictors change in value the outcome changes by a constant amount. Visually, this just means fitting a straight line through data. For example, if you are paid £10 an hour, your pay increases by exactly £10 for each additional hour worked. This is a linear relationship.\nIf you have a predictor \\(X\\) and an outcome \\(Y\\), linear regression finds the path through the data that best fits these points, otherwise known as the “line of best fit”. This works by finding a line that passes through the data with the minimum amount of prediction error, which is the observed value minus the predicted value for each data pint (and is oftn referred to as the residual). The most common way for calculating the line of best fit is called Ordinary Least Squares (OLS)[^OLS].\n\n\nCalculating the Line of Best fit Using OLS (Click to Expand)\n\n\nOrdinary Least Squares Estimation\nWe won’t spend too much time on OLS in this session, but the method it uses for calculating the line of best fit is as follows:\n\nCalculate the residual for each observation (the actual value minus the predicted value).\nSquare the residuals (so that positive and negative errors do not cancel each other out).\nSum the squared residuals, sometimes called the Residual Sum of Squares (RSS).\nFind the line that produces the smallest RSS.\n\n\nThe formula for a simple linear regression model, predicting \\(Y\\) with one predictor \\(X\\):\n\\[\nY =\n\\underbrace{\\vphantom{\\beta_0} \\overset{\\color{#41B6E6}{\\text{Intercept}}}{\\color{#41B6E6}{\\beta_0}} +\n\\overset{\\color{#005EB8}{\\text{Slope}}}{\\color{#005EB8}{\\beta_1}}X \\space \\space}_{\\text{Explained Variance}} +\n\\overset{\\mathstrut \\color{#ED8B00}{\\text{Error}}}{\\underset{\\text{Unexplained}}{\\color{#ED8B00}{\\epsilon}}}\n\\]\nThis breaks the problem down into three components, and estimates two parameters:\n\n\\(\\beta_1\\) - The slope, estimating the effect that \\(X\\) has on the outcome, \\(Y\\).\n\\(\\beta_0\\) - The intercept, estimating the average value of \\(Y\\) when \\(X = 0\\).\n\\(\\epsilon\\) - The error term, capturing the remaining variance in the outcome \\(Y\\) that is not explained by the rest of the model.\n\n\n\n\n\n\n\nDon’t worry if this is intimidating at first. The main thing you need to remember is that this equation is just fitting a straight line through the data.\n\n\n\nThe regression line represents our best guess of the true relationship, and the scatter around the line represents uncertainty.",
    "crumbs": [
      "Data Science",
      "13. Introduction to Linear Regression"
    ]
  },
  {
    "objectID": "sessions/13-intro-linear-regression/index.html#building-our-first-model",
    "href": "sessions/13-intro-linear-regression/index.html#building-our-first-model",
    "title": "Regression Fundamentals: Introduction to Linear Regression",
    "section": "Building Our First Model",
    "text": "Building Our First Model\n\nImport & Prepare Data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# load penguins data\nurl = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-15/penguins.csv'\npenguins_raw = pd.read_csv(url)\n\n# clean data\ndf = penguins_raw.dropna()\ndf.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_len\nbill_dep\nflipper_len\nbody_mass\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n\n\n\n\n\n\n\nVisualising the Relationship\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df, x='flipper_len', y='body_mass', alpha=0.7)\nplt.title('Flipper Length vs Body Mass')\nplt.xlabel('Flipper Length (mm)')\nplt.ylabel('Body Mass (g)')\nplt.show()\n\n\n\n\n\n\n\n\nThe relationship looks strongly linear and positive. This is a good candidate for linear regression.\n\n\nFitting the Model\n\n# fit simple linear regression\nX = df[['flipper_len']]  # predictor\ny = df['body_mass']      # outcome\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# extract coefficients\nintercept = model.intercept_\nslope = model.coef_[0]\n\nThe intercept (\\(\\beta_0\\)) = -5872.1g. The intercept tells us the predicted body mass when flipper length is 0mm. This intercept value is not meaningful because a flipper length of zero is implausible and ultimately nonsensical (if they have flippers, they must be longer than 0mm)1, but we need the intercept to fit the model.\nThe slope (\\(\\beta_1\\)) = 50.2g/mm. The slope tells us that each additional mm increase in flipper length predicts ~50g increase in body mass.\n\n\nVisualising the Fitted Line\nWe can add the fitted line to our scatterplot and use this to make predictions by tracing the flipper length from which we want to predict body mass and then finding the body mass at that value.\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(data=df, x='flipper_len', y='body_mass', alpha=0.7)\n\n# add regression line\nx_range = np.linspace(df['flipper_len'].min(), df['flipper_len'].max(), 100)\ny_pred = intercept + slope * x_range\nplt.plot(x_range, y_pred, color='red', linewidth=2, label=f'y = {intercept:.0f} + {slope:.1f}x')\n\nplt.title('Linear Regression: Flipper Length Predicting Body Mass')\nplt.xlabel('Flipper Length (mm)')\nplt.ylabel('Body Mass (g)')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nMaking Predictions\nUsing a fitted line to make predictions is imprecise. Instead, we can use the linear regression formula detailed earlier to predict penguin body mass from flipper length. We just have to plug in the intercept and slope into the formula, along with the flipper length we want to predict from, and this will give us the predicted body mass.\nThe applied formula looks like this:\n\\[\n\\text{Body Mass} = -5872.1 + 50.2 \\times \\text{Flipper Length}\n\\]\nOr we can generate predictions using code:\n\n# predict body mass for different flipper lengths\nnew_flipper_lengths = np.array([[190], [200], [210], [220]])\npredictions = model.predict(new_flipper_lengths)\n\nprediction_df = pd.DataFrame({\n    'Flipper Length (mm)': new_flipper_lengths.flatten(),\n    'Predicted Body Mass (g)': predictions.round(0)\n})\n\nprint(\"Predictions for new observations:\")\nprint(prediction_df)\n\nPredictions for new observations:\n   Flipper Length (mm)  Predicted Body Mass (g)\n0                  190                   3657.0\n1                  200                   4159.0\n2                  210                   4660.0\n3                  220                   5162.0",
    "crumbs": [
      "Data Science",
      "13. Introduction to Linear Regression"
    ]
  },
  {
    "objectID": "sessions/13-intro-linear-regression/index.html#statistical-inference",
    "href": "sessions/13-intro-linear-regression/index.html#statistical-inference",
    "title": "Regression Fundamentals: Introduction to Linear Regression",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nSo far we’ve focused on prediction. But regression also lets us make inferences about the relationship between variables.\n\n# use statsmodels for explanatory models\nX_sm = sm.add_constant(df['flipper_len'])  # add intercept column\nmodel_sm = sm.OLS(df['body_mass'], X_sm).fit()\n\nprint(model_sm.summary().tables[1])\n\n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nconst       -5872.0927    310.285    -18.925      0.000   -6482.472   -5261.713\nflipper_len    50.1533      1.540     32.562      0.000      47.123      53.183\n===============================================================================\n\n\nWhen we are interested in explaining rather than predicting, the coefficient (coef) is what matters most. This is the intercept and slope values we calculated earlier. The coefficients tell us the size of the effect observed in the data. If you want to explain the relationship between variables, the effect size is the most important part of the model.\nThe other outputs are useful for understanding how well your model fits to the data and how certain the model is about the estimates. The standard error (std error) tells us how much the actual values vary around the fitted line, quantifying uncertainty. The 95% confidence intervals ([0.025 & 0.975]) are another way of expressing this idea. They effectively show a range of plausible coefficient values.\nFinally, the t-statistic (t) is a test statistic2 and the p-value (P&gt;|t|) is the probability of observing an effect as large (or larger) as observed in the data if there was actually no relationship between flipper length and body mass (and if the assumptions of the model are valid). You can think of the p-value as an expression of how surprising it would be to see a coefficient as large as the one we found if there wasn’t actually a relationship between flipper length and body mass.",
    "crumbs": [
      "Data Science",
      "13. Introduction to Linear Regression"
    ]
  },
  {
    "objectID": "sessions/13-intro-linear-regression/index.html#summary",
    "href": "sessions/13-intro-linear-regression/index.html#summary",
    "title": "Regression Fundamentals: Introduction to Linear Regression",
    "section": "Summary",
    "text": "Summary\nLinear regression transforms correlation into a predictive and explanatory tool. We can quantify relationships and make predictions. Our penguin model shows that flipper length strongly predicts body mass, with each additional mm corresponding to ~50g increase in weight.\nThe key insights from regression go beyond simple correlation: we can make specific predictions, quantify uncertainty, and begin to understand the mechanisms behind relationships. However, real-world relationships are often more complex than simple linear models can capture.\nIn the next session, we will go into greater detail about the implementation of linear regression, looking at how we fit models, their assumptions, and how we interpret, present, and communicate our outputs.",
    "crumbs": [
      "Data Science",
      "13. Introduction to Linear Regression"
    ]
  },
  {
    "objectID": "sessions/13-intro-linear-regression/index.html#footnotes",
    "href": "sessions/13-intro-linear-regression/index.html#footnotes",
    "title": "Regression Fundamentals: Introduction to Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor explanatory models, it is generally best-practice to transform the data so that the intercept is meaningful. For example, subtracting the mean flipper length from each observation (otherwise known as centering) so that the zero value represents the average flipper length and the intercept represents the average body mass for the average flipper length.↩︎\nWe will not discuss test statistics in detail here, but they are standardised measures of how unusual your results are. It is used to calculate the p-value. The t-statistic is the coefficient divided by its standard error.↩︎",
    "crumbs": [
      "Data Science",
      "13. Introduction to Linear Regression"
    ]
  },
  {
    "objectID": "sessions/14-implementing-linear-regression/index.html",
    "href": "sessions/14-implementing-linear-regression/index.html",
    "title": "Regression Fundamentals: Implementing Linear Regression",
    "section": "",
    "text": "In the previous session, we explored what linear regression is, how it works conceptually, and why it’s such a powerful tool. We saw how regression finds the line of best fit by minimising prediction errors, and we understood the core components: intercepts, slopes, and residuals.\nNow it’s time to put that theory into practice. In this session, we’ll learn how to actually build regression models in Python, interpret their outputs, check whether our models are valid, and generate insights from them.\nWe’ll cover two main approaches to regression in Python:\nLet’s get started by loading our data and packages.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# set visualisation style\nsns.set_theme(style=\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\n# load palmer penguins data\nurl = 'https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-04-15/penguins.csv'\ndf = pd.read_csv(url)\n\n# drop missing values for simplicity\ndf = df.dropna()",
    "crumbs": [
      "Data Science",
      "14. Implementing Linear Regression"
    ]
  },
  {
    "objectID": "sessions/14-implementing-linear-regression/index.html#building-predictive-models-using-scikit-learn",
    "href": "sessions/14-implementing-linear-regression/index.html#building-predictive-models-using-scikit-learn",
    "title": "Regression Fundamentals: Implementing Linear Regression",
    "section": "Building Predictive Models Using Scikit-Learn",
    "text": "Building Predictive Models Using Scikit-Learn\nscikit-learn (sklearn) is Python’s most popular library for machine learning. Its regression tools are designed for building predictive models, and it is optimised for generalising well on new data.\n\nSimple Linear Regression\nA simple linear regression has one predictor variable. Let’s predict body mass from flipper length.\n\n# prepare the data\n# sklearn requires 2D arrays for X (predictors) and 1D arrays for y (outcome)\nX = df[['flipper_len']]  # double brackets create a dataframe\ny = df['body_mass']       # single bracket creates a series\n\n# create and fit the model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# get the intercept\nintercept = model.intercept_\nprint(f\"Intercept = {intercept:.2f}g\")\n\n# get the slope\nslope = model.coef_[0]\nprint(f\"Slope = {slope:.2f}g per mm\")\n\nIntercept = -5872.09g\nSlope = 50.15g per mm\n\n\nThe fitted model object contains everything we need: coefficients, predictions, and model performance metrics.\nThe intercept (\\(\\beta_0\\)) = -5872.1g. The intercept tells us the predicted body mass when flipper length is 0mm. This intercept value is not meaningful because a flipper length of zero is implausible and ultimately nonsensical (if they have flippers, they must be longer than 0mm)1, but we need the intercept to fit the model.\nThe slope (\\(\\beta_1\\)) = 50.2g/mm. The slope tells us that each additional mm increase in flipper length predicts ~50g increase in body mass.\n\n\nMaking Predictions\nOnce we have a fitted model, we can use it to predict body mass for any flipper length.\n\n# predict body mass for all penguins in our dataset\ny_pred = model.predict(X)\n\n# add predictions to our dataframe\ndf['predicted_mass'] = y_pred\ndf['residual'] = df['body_mass'] - df['predicted_mass']\n\n# look at some predictions\nprint(\"Sample predictions:\")\nprint(df[['flipper_len', 'body_mass', 'predicted_mass', 'residual']].head(10))\n\nSample predictions:\n    flipper_len  body_mass  predicted_mass    residual\n0         181.0     3750.0     3205.648453  544.351547\n1         186.0     3800.0     3456.414782  343.585218\n2         195.0     3250.0     3907.794176 -657.794176\n4         193.0     3450.0     3807.487644 -357.487644\n5         190.0     3650.0     3657.027846   -7.027846\n6         181.0     3625.0     3205.648453  419.351547\n7         195.0     4675.0     3907.794176  767.205824\n12        182.0     3200.0     3255.801719  -55.801719\n13        191.0     3800.0     3707.181112   92.818888\n14        198.0     4400.0     4058.253974  341.746026\n\n\nWe can plot the predictions too.\n\n# plot\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.scatterplot(data=df, x='flipper_len', y='body_mass', color='#005EB8', alpha=0.5, ax=ax, label='Observed')\nsns.scatterplot(data=df, x='flipper_len', y='predicted_mass', color='red', alpha=0.5, ax=ax, label='Predicted')\n\nax.set_xlabel('Flipper Length (mm)')\nax.set_ylabel('Body Mass (g)')\nax.set_title('Observed vs Predicted Body Mass')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can also predict for new data.\n\n# predict for hypothetical penguins\nnew_penguins = pd.DataFrame({\n    'flipper_len': [180, 200, 220]\n})\n\nnew_predictions = model.predict(new_penguins)\n\nprint(\"\\nPredictions for new penguins:\")\nfor flipper, mass in zip(new_penguins['flipper_len'], new_predictions):\n    print(f\"Flipper length: {flipper}mm --&gt; Predicted mass: {mass:.0f}g\")\n\n\nPredictions for new penguins:\nFlipper length: 180mm --&gt; Predicted mass: 3155g\nFlipper length: 200mm --&gt; Predicted mass: 4159g\nFlipper length: 220mm --&gt; Predicted mass: 5162g\n\n\n\n\nModel Performance Metrics\nAn important part of predictive modelling is evaluation. We need to test how well our model is performing! sklearn provides a wide variety of metrics for assessing model performance. We will look at \\(\\text{R}^2\\) and RMSE.\n\n# r-squared: proportion of variance explained\nr2 = r2_score(y, y_pred)\n\n# root mean squared error: average prediction error in grams\nrmse = np.sqrt(mean_squared_error(y, y_pred))\n\nThe \\(\\text{R}^2\\) = 0.76, which suggests that flipper length explains a significant amount of the variance in penguin body mass, while the RMSE indicates that the average prediction error is 392.16g. This means that predictions are, on average, off by 392.16g.\n\n\n\n\n\n\nThere is no rule of thumb or minimum threshold for good prediction error. It all depends on what you are trying to predict and what is needed for predictions to be useful. Another model trying to predict something completely different might have much larger prediction errors but this still be useful in context. It is always necessary to judge a model in context.\n\n\n\n\n\nMultiple Linear Regression\nNow let’s add more predictors. Multiple regression lets us understand the effect of each variable while controlling for the others.\n\n# add bill length and bill depth as predictors\nX_multi = df[['flipper_len', 'bill_len', 'bill_dep']]\ny = df['body_mass']\n\n# fit the model\nmodel_multi = LinearRegression()\nmodel_multi.fit(X_multi, y)\n\n# extract coefficients\nprint(\"Multiple regression coefficients:\")\nprint(f\"Intercept: {model_multi.intercept_:.2f}g\")\nfor name, coef in zip(X_multi.columns, model_multi.coef_):\n    print(f\"  {name}: {coef:.2f}g per unit\")\n\n# compare model performance\ny_pred_multi = model_multi.predict(X_multi)\nr2_multi = r2_score(y, y_pred_multi)\nrmse_multi = np.sqrt(mean_squared_error(y, y_pred_multi))\n\nprint(f\"\\nModel comparison:\")\nprint(f\"Simple model (flipper only):   R² = {r2:.3f}, RMSE = {rmse:.1f}g\")\nprint(f\"Multiple model (three vars):   R² = {r2_multi:.3f}, RMSE = {rmse_multi:.1f}g\")\n\nMultiple regression coefficients:\nIntercept: -6445.48g\n  flipper_len: 50.76g per unit\n  bill_len: 3.29g per unit\n  bill_dep: 17.84g per unit\n\nModel comparison:\nSimple model (flipper only):   R² = 0.762, RMSE = 392.2g\nMultiple model (three vars):   R² = 0.764, RMSE = 390.6g\n\n\nAdding bill measurements improves our model slightly. Each predictor contributes information beyond what the others provide.",
    "crumbs": [
      "Data Science",
      "14. Implementing Linear Regression"
    ]
  },
  {
    "objectID": "sessions/14-implementing-linear-regression/index.html#building-inferential-models-using-statsmodels",
    "href": "sessions/14-implementing-linear-regression/index.html#building-inferential-models-using-statsmodels",
    "title": "Regression Fundamentals: Implementing Linear Regression",
    "section": "Building Inferential Models Using Statsmodels",
    "text": "Building Inferential Models Using Statsmodels\nstatsmodels is designed for statistical inference2. It provides detailed output about uncertainty, hypothesis tests, and model diagnostics. This is what you want when your goal is understanding relationships rather than pure prediction.\nAdding predictors is straightforward with the formula interface.\n\n# multiple regression with formula\nmodel_sm = smf.ols('body_mass ~ flipper_len + bill_len + bill_dep', data=df).fit()\n\nprint(model_sm.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              body_mass   R-squared:                       0.764\nModel:                            OLS   Adj. R-squared:                  0.762\nMethod:                 Least Squares   F-statistic:                     354.9\nDate:                Thu, 06 Nov 2025   Prob (F-statistic):          9.26e-103\nTime:                        13:30:25   Log-Likelihood:                -2459.8\nNo. Observations:                 333   AIC:                             4928.\nDf Residuals:                     329   BIC:                             4943.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept   -6445.4760    566.130    -11.385      0.000   -7559.167   -5331.785\nflipper_len    50.7621      2.497     20.327      0.000      45.850      55.675\nbill_len        3.2929      5.366      0.614      0.540      -7.263      13.849\nbill_dep       17.8364     13.826      1.290      0.198      -9.362      45.035\n==============================================================================\nOmnibus:                        5.596   Durbin-Watson:                   1.982\nProb(Omnibus):                  0.061   Jarque-Bera (JB):                5.469\nSkew:                           0.312   Prob(JB):                       0.0649\nKurtosis:                       3.068   Cond. No.                     5.44e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.44e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nThe statsmodels summary is dense but incredibly informative. Here’s what matters most.\n\ncoef - The estimated effect size of each variable holding the others constant\nstd err - Uncertainty in the estimate\nt - Test statistic (coef / std err)\nP&gt;|t| - p-value for null hypothesis that coefficient = 0\n[0.025, 0.975] - 95% confidence interval\n\nFocus on effect sizes first, then uncertainty. The coefficient tells you the magnitude of the relationship. The confidence interval tells you how precisely we’ve estimated it. Don’t obsess over \\(\\text{R}^2\\)3 or p-values. The most important part of the model is the coefficients.\n\n\n\n\n\n\nThe p-value is useful as a diagnostic, indicating whether you have enough data, but it doesn’t tell you whether the effect is meaningful or important.\n\n\n\nWe can also produce a simpler regression table, if the above is overwhelming.\n\n# extract just the coefficients table\nprint(model_sm.summary().tables[1])\n\n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept   -6445.4760    566.130    -11.385      0.000   -7559.167   -5331.785\nflipper_len    50.7621      2.497     20.327      0.000      45.850      55.675\nbill_len        3.2929      5.366      0.614      0.540      -7.263      13.849\nbill_dep       17.8364     13.826      1.290      0.198      -9.362      45.035\n===============================================================================\n\n\n\n\nStyled Tables (Click to Expand)\n\nFor presentations and reports, we can create publication-ready tables.\n\nfrom great_tables import GT, md, html\nfrom great_tables import loc, style\n\n# fit three models for comparison\nmodel1 = smf.ols('body_mass ~ flipper_len', data=df).fit()\nmodel2 = smf.ols('body_mass ~ flipper_len + bill_len + bill_dep', data=df).fit()\nmodel3 = smf.ols('body_mass ~ flipper_len + bill_len + bill_dep + C(species)', data=df).fit()\n\n# create a dataframe with model results\nresults_df = pd.DataFrame({\n    'Variable': ['Intercept', 'Flipper Length', 'Bill Length', 'Bill Depth', \n                 'Chinstrap', 'Gentoo'],\n    'Model 1': [\n        f\"{model1.params['Intercept']:.1f}\",\n        f\"{model1.params['flipper_len']:.2f}***\",\n        '—', '—', '—', '—'\n    ],\n    'Model 2': [\n        f\"{model2.params['Intercept']:.1f}\",\n        f\"{model2.params['flipper_len']:.2f}***\",\n        f\"{model2.params['bill_len']:.2f}***\",\n        f\"{model2.params['bill_dep']:.2f}\",\n        '—', '—'\n    ],\n    'Model 3': [\n        f\"{model3.params['Intercept']:.1f}\",\n        f\"{model3.params['flipper_len']:.2f}***\",\n        f\"{model3.params['bill_len']:.2f}***\",\n        f\"{model3.params['bill_dep']:.2f}**\",\n        f\"{model3.params['C(species)[T.Chinstrap]']:.1f}***\",\n        f\"{model3.params['C(species)[T.Gentoo]']:.1f}***\"\n    ]\n})\n\n# add model statistics\nstats_df = pd.DataFrame({\n    'Variable': ['', 'N', 'R²', 'Adj. R²', 'AIC'],\n    'Model 1': ['', f\"{int(model1.nobs)}\", f\"{model1.rsquared:.3f}\", \n                f\"{model1.rsquared_adj:.3f}\", f\"{model1.aic:.1f}\"],\n    'Model 2': ['', f\"{int(model2.nobs)}\", f\"{model2.rsquared:.3f}\", \n                f\"{model2.rsquared_adj:.3f}\", f\"{model2.aic:.1f}\"],\n    'Model 3': ['', f\"{int(model3.nobs)}\", f\"{model3.rsquared:.3f}\", \n                f\"{model3.rsquared_adj:.3f}\", f\"{model3.aic:.1f}\"]\n})\n\ncombined_df = pd.concat([results_df, stats_df], ignore_index=True)\n\n# create styled table\ntable = (\n    GT(combined_df)\n    .tab_header(\n        title=\"Regression Models Predicting Penguin Body Mass\",\n        subtitle=\"Coefficients with significance stars (* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001)\"\n    )\n    .tab_spanner(\n        label=\"Model Specifications\",\n        columns=['Model 1', 'Model 2', 'Model 3']\n    )\n    .tab_style(\n        style=style.text(weight=\"bold\"),\n        locations=loc.body(rows=[6, 7, 8, 9, 10])\n    )\n    .tab_style(\n        style=style.borders(sides=\"top\", weight=\"2px\"),\n        locations=loc.body(rows=[6])\n    )\n    .cols_align(\n        align=\"center\",\n        columns=['Model 1', 'Model 2', 'Model 3']\n    )\n    .cols_align(\n        align=\"left\",\n        columns=['Variable']\n    )\n)\n\ntable\n\n\n\n\n\n\n\nRegression Models Predicting Penguin Body Mass\n\n\nCoefficients with significance stars (* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001)\n\n\nVariable\nModel Specifications\n\n\nModel 1\nModel 2\nModel 3\n\n\n\n\nIntercept\n-5872.1\n-6445.5\n-4282.1\n\n\nFlipper Length\n50.15***\n50.76***\n20.23***\n\n\nBill Length\n—\n3.29***\n39.72***\n\n\nBill Depth\n—\n17.84\n141.77**\n\n\nChinstrap\n—\n—\n-496.8***\n\n\nGentoo\n—\n—\n965.2***\n\n\n\n\n\n\n\n\nN\n333\n333\n333\n\n\nR²\n0.762\n0.764\n0.849\n\n\nAdj. R²\n0.761\n0.762\n0.847\n\n\nAIC\n4926.1\n4927.6\n4781.7",
    "crumbs": [
      "Data Science",
      "14. Implementing Linear Regression"
    ]
  },
  {
    "objectID": "sessions/14-implementing-linear-regression/index.html#adding-categorical-variables",
    "href": "sessions/14-implementing-linear-regression/index.html#adding-categorical-variables",
    "title": "Regression Fundamentals: Implementing Linear Regression",
    "section": "Adding Categorical Variables",
    "text": "Adding Categorical Variables\nstatsmodels handles categorical variables automatically with the formula interface.\n\n# add species as a categorical predictor\n# statsmodels automatically creates dummy variables\nmodel_species_sm = smf.ols('body_mass ~ flipper_len + bill_len + bill_dep + C(species)', \n                           data=df).fit()\n\nprint(model_species_sm.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              body_mass   R-squared:                       0.849\nModel:                            OLS   Adj. R-squared:                  0.847\nMethod:                 Least Squares   F-statistic:                     369.1\nDate:                Thu, 06 Nov 2025   Prob (F-statistic):          4.22e-132\nTime:                        13:30:25   Log-Likelihood:                -2384.8\nNo. Observations:                 333   AIC:                             4782.\nDf Residuals:                     327   BIC:                             4805.\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n===========================================================================================\n                              coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------------\nIntercept               -4282.0802    497.832     -8.601      0.000   -5261.438   -3302.723\nC(species)[T.Chinstrap]  -496.7583     82.469     -6.024      0.000    -658.995    -334.521\nC(species)[T.Gentoo]      965.1983    141.770      6.808      0.000     686.301    1244.096\nflipper_len                20.2264      3.135      6.452      0.000      14.059      26.394\nbill_len                   39.7184      7.227      5.496      0.000      25.501      53.936\nbill_dep                  141.7714     19.163      7.398      0.000     104.072     179.470\n==============================================================================\nOmnibus:                        7.321   Durbin-Watson:                   2.248\nProb(Omnibus):                  0.026   Jarque-Bera (JB):                7.159\nSkew:                           0.348   Prob(JB):                       0.0279\nKurtosis:                       3.179   Cond. No.                     6.01e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 6.01e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nThe C() wrapper tells statsmodels to treat species as categorical. The model automatically creates “dummy variables”, which splits the categorical variable into multiple columns, with each column being a binary variable representing a category (where the variable equals one when that category appears in the original column).\nWhen creating the dummy variables, the model drops one category to use it as the reference category against which the other categories are compared. This means that coefficients for those categories tell us how much the outcome variable increases or decreases compared against the reference category that was dropped.\nFor example, the model specified above uses Adelie penguins as the reference category (it chooses by alphabetical order by default), and the coefficients represent the species effects relative to Adelie penguins.\n\nprint(\"\\nSpecies effects relative to Adelie:\")\nfor var in model_species_sm.params.index:\n    if 'species' in var:\n        coef = model_species_sm.params[var]\n        ci = model_species_sm.conf_int().loc[var]\n        species = var.split('[T.')[1].rstrip(']')\n        print(f\"{species:12s}: {coef:7.0f}g heavier [{ci[0]:7.0f}, {ci[1]:7.0f}]\")\n\n\nSpecies effects relative to Adelie:\nChinstrap   :    -497g heavier [   -659,    -335]\nGentoo      :     965g heavier [    686,    1244]",
    "crumbs": [
      "Data Science",
      "14. Implementing Linear Regression"
    ]
  },
  {
    "objectID": "sessions/14-implementing-linear-regression/index.html#coefficient-plots",
    "href": "sessions/14-implementing-linear-regression/index.html#coefficient-plots",
    "title": "Regression Fundamentals: Implementing Linear Regression",
    "section": "Coefficient Plots",
    "text": "Coefficient Plots\nCoefficient plots show effect sizes and their uncertainty at a glance.\n\n# extract coefficients and confidence intervals\ncoefs = model_species_sm.params.drop('Intercept')\nconf_int = model_species_sm.conf_int().drop('Intercept')\n\n# create dataframe for plotting\ncoef_data = pd.DataFrame({\n    'Variable': coefs.index,\n    'Coefficient': coefs.values,\n    'Lower': conf_int[0].values,\n    'Upper': conf_int[1].values\n})\n\n# plot\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.scatterplot(data=coef_data, x='Coefficient', y='Variable', s=200, \n                color='#005EB8', ax=ax)\nfor idx, row in coef_data.iterrows():\n    ax.plot([row['Lower'], row['Upper']], [row['Variable'], row['Variable']], \n            color='#005EB8', linewidth=2)\nax.axvline(x=0, color='red', linestyle='--', linewidth=1)\nax.set_xlabel('Coefficient (Effect on Body Mass, g)')\nax.set_title('Coefficient Plot with 95% Confidence Intervals')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nVariables whose CI doesn’t cross zero have effects distinguishable from zero. But remember: focus on effect sizes, not just significance!",
    "crumbs": [
      "Data Science",
      "14. Implementing Linear Regression"
    ]
  },
  {
    "objectID": "sessions/14-implementing-linear-regression/index.html#the-gauss-markov-theorem",
    "href": "sessions/14-implementing-linear-regression/index.html#the-gauss-markov-theorem",
    "title": "Regression Fundamentals: Implementing Linear Regression",
    "section": "The Gauss-Markov Theorem",
    "text": "The Gauss-Markov Theorem\nUnder certain conditions, OLS is BLUE:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent of each other\nHomoscedasticity: Variance of errors is constant across X\nNo perfect multicollinearity: Predictors aren’t perfectly correlated\nExogeneity: Errors have mean zero (no systematic bias)\n\nAdditionally, for valid inference (confidence intervals, p-values), we need:\n\nNormality: Errors are normally distributed (or we have a large sample)\n\nLet’s check these assumptions systematically.",
    "crumbs": [
      "Data Science",
      "14. Implementing Linear Regression"
    ]
  },
  {
    "objectID": "sessions/14-implementing-linear-regression/index.html#diagnostic-plots",
    "href": "sessions/14-implementing-linear-regression/index.html#diagnostic-plots",
    "title": "Regression Fundamentals: Implementing Linear Regression",
    "section": "Diagnostic Plots",
    "text": "Diagnostic Plots\nWe’ll create a set of diagnostic plots to check our assumptions. First, let’s fit a model to diagnose.\n\n# fit our model with species\nmodel = model_species_sm  # use the statsmodels model from earlier\n\n# get predictions and residuals\ndf['fitted'] = model.fittedvalues\ndf['residuals'] = model.resid\ndf['standardised_residuals'] = model.resid_pearson\n\n\n1. Residuals vs Fitted (Linearity & Homoscedasticity)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.scatterplot(x=df['fitted'], y=df['residuals'], alpha=0.6, ax=ax)\nax.axhline(y=0, color='red', linestyle='--', linewidth=2)\nax.set_xlabel('Fitted Values')\nax.set_ylabel('Residuals')\nax.set_title('Residuals vs Fitted Values')\nplt.show()\n\n\n\n\n\n\n\n\nThe observations should appear to be randomly scattered around zero, and the spread around zero should be constant across the range of fitted values. If you can see a pattern or trend, this suggests the linearity assumption is violated, and if the observations are funnel-shaped, this suggests that the homoscedasticity assumption is violated.\n\n\n2. Q-Q Plot (Normality)\n\nfrom scipy import stats\n\nfig, ax = plt.subplots(figsize=(10, 6))\nstats.probplot(df['residuals'], dist=\"norm\", plot=ax)\nax.set_title('Q-Q Plot')\nplt.show()\n\n\n\n\n\n\n\n\nThe closer to the red line, the better. However, some deviation is generally fine, and as sample size increases this becomes less of a concern. The very small deviations from the line above are not a cause for concern.\n\n\n3. Scale-Location Plot (Homoscedasticity)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.scatterplot(x=df['fitted'], y=np.sqrt(np.abs(df['standardised_residuals'])), alpha=0.6, ax=ax)\nax.set_xlabel('Fitted Values')\nax.set_ylabel('√|Standardised Residuals|')\nax.set_title('Scale-Location Plot')\nplt.show()\n\n\n\n\n\n\n\n\nIf there is an observable trend or pattern in your Scale-Location plot, this indicates heteroscedasticity.\n\n\n4. Residuals vs Leverage (Influential Points)\n\nfrom statsmodels.stats.outliers_influence import OLSInfluence\n\ninfluence = OLSInfluence(model)\nleverage = influence.hat_matrix_diag\ncooks_d = influence.cooks_distance[0]\n\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.scatterplot(x=leverage, y=df['standardised_residuals'], alpha=0.6, ax=ax)\nax.axhline(y=0, color='red', linestyle='--', linewidth=1)\nax.set_xlabel('Leverage')\nax.set_ylabel('Standardised Residuals')\nax.set_title('Residuals vs Leverage')\n\n# highlight high influence points\nhigh_influence = cooks_d &gt; 4/len(df)\nif high_influence.any():\n    ax.scatter(leverage[high_influence], df['standardised_residuals'][high_influence], \n               color='red', s=100, alpha=0.6, label='High influence')\n    ax.legend()\nplt.show()\n\n\n\n\n\n\n\n\nPoints that are significantly higher or lower than zero and with high leverage are potentially a problem.\nHowever, I would caution against just removing outliers because they have a significant influence. If those outliers are real observations, should they really be discarded?\n\n\nQuick Diagnostic Function (Click to Expand)\n\nLet’s create a helper function to generate all diagnostic plots at once.\n\ndef plot_diagnostics(model, data=None):\n    \"\"\"\n    Create a 2x2 grid of diagnostic plots for a statsmodels regression model.\n    \n    Parameters:\n    -----------\n    model : statsmodels regression model\n        A fitted statsmodels OLS model\n    data : pandas DataFrame, optional\n        The data used to fit the model (for some plot enhancements)\n    \"\"\"\n    # get residuals and fitted values\n    fitted = model.fittedvalues\n    residuals = model.resid\n    standardised_residuals = model.resid_pearson\n    \n    # calculate leverage and influence\n    influence = OLSInfluence(model)\n    leverage = influence.hat_matrix_diag\n    \n    # create 2x2 plot\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # 1. residuals vs fitted\n    sns.scatterplot(x=fitted, y=residuals, alpha=0.6, ax=axes[0, 0])\n    axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n    axes[0, 0].set_xlabel('Fitted Values')\n    axes[0, 0].set_ylabel('Residuals')\n    axes[0, 0].set_title('Residuals vs Fitted')\n    \n    # 2. q-q plot\n    stats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\n    axes[0, 1].set_title('Q-Q Plot')\n    \n    # 3. scale-location\n    sns.scatterplot(x=fitted, y=np.sqrt(np.abs(standardised_residuals)), alpha=0.6, ax=axes[1, 0])\n    axes[1, 0].set_xlabel('Fitted Values')\n    axes[1, 0].set_ylabel('√|Standardised Residuals|')\n    axes[1, 0].set_title('Scale-Location')\n    \n    # 4. residuals vs leverage\n    sns.scatterplot(x=leverage, y=standardised_residuals, alpha=0.6, ax=axes[1, 1])\n    axes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=1)\n    axes[1, 1].set_xlabel('Leverage')\n    axes[1, 1].set_ylabel('Standardised Residuals')\n    axes[1, 1].set_title('Residuals vs Leverage')\n    \n    plt.tight_layout()\n    plt.show()\n\n# use the function\nplot_diagnostics(model_species_sm)",
    "crumbs": [
      "Data Science",
      "14. Implementing Linear Regression"
    ]
  },
  {
    "objectID": "sessions/14-implementing-linear-regression/index.html#checking-for-multicollinearity",
    "href": "sessions/14-implementing-linear-regression/index.html#checking-for-multicollinearity",
    "title": "Regression Fundamentals: Implementing Linear Regression",
    "section": "Checking for Multicollinearity",
    "text": "Checking for Multicollinearity\nWhen predictors are highly correlated with each other, coefficient estimates become unstable. We can check this using Variance Inflation Factors (VIF).\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# calculate VIF for each predictor\n# need to add constant for VIF calculation\nX_with_const = sm.add_constant(df[['flipper_len', 'bill_len', 'bill_dep']])\n\nvif_data = pd.DataFrame()\nvif_data['Variable'] = X_with_const.columns\nvif_data['VIF'] = [variance_inflation_factor(X_with_const.values, i) \n                   for i in range(X_with_const.shape[1])]\n\nprint(\"Variance Inflation Factors:\")\nprint(vif_data)\n\nVariance Inflation Factors:\n      Variable         VIF\n0        const  691.005294\n1  flipper_len    2.633327\n2     bill_len    1.850958\n3     bill_dep    1.593411\n\n\nThe rule of thumb:\n\nVIF of greater than five = No multicollinearity\nVIF of between five and ten = Moderate multicollinearity\nVIF greater than ten = High multicollinearity",
    "crumbs": [
      "Data Science",
      "14. Implementing Linear Regression"
    ]
  },
  {
    "objectID": "sessions/14-implementing-linear-regression/index.html#what-happens-when-assumptions-are-violated",
    "href": "sessions/14-implementing-linear-regression/index.html#what-happens-when-assumptions-are-violated",
    "title": "Regression Fundamentals: Implementing Linear Regression",
    "section": "What Happens When Assumptions Are Violated?",
    "text": "What Happens When Assumptions Are Violated?\nUnderstanding assumptions helps you know when you can bend or break the rules.\n\nLinearity - Coefficients are biased. Try transformations (log, polynomial) or non-linear models.\nIndependence - Standard errors are wrong (usually too small). Use clustered standard errors or multilevel models.\nHomoscedasticity - Standard errors are wrong. Use robust standard errors (HC3, HC4). Predictions are still unbiased.\nNormality - With large samples, not a big problem due to central limit theorem. With small samples, confidence intervals may be wrong.\nMulticollinearity - Coefficients are unstable and hard to interpret, but predictions are fine. Remove or combine correlated predictors.\n\nAs you gain experience, you’ll learn when violations matter and when they don’t. For now, check diagnostics and flag any concerns.",
    "crumbs": [
      "Data Science",
      "14. Implementing Linear Regression"
    ]
  },
  {
    "objectID": "sessions/14-implementing-linear-regression/index.html#footnotes",
    "href": "sessions/14-implementing-linear-regression/index.html#footnotes",
    "title": "Regression Fundamentals: Implementing Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor explanatory models, it is generally best-practice to transform the data so that the intercept is meaningful. For example, subtracting the mean flipper length from each observation (otherwise known as centering) so that the zero value represents the average flipper length and the intercept represents the average body mass for the average flipper length.↩︎\nStatistical inference is the process of drawing conclusions about a population (or an effect occurring in the real-world) based on sample data.↩︎\nA model with \\(\\text{R}^2\\) = 0.3 can still be incredibly useful if the effects are meaningful and the predictions are good enough for your purpose.↩︎\nFixing the problem can take many shapes. It could involve transforming the data to fit the model better, adding (or removing) variables that improve model fit, or fitting a different type of regression model entirely.↩︎",
    "crumbs": [
      "Data Science",
      "14. Implementing Linear Regression"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#what-is-a-notebook",
    "href": "sessions/02-jupyter_notebooks/slides.html#what-is-a-notebook",
    "title": "Jupyter Notebooks",
    "section": "What is a notebook",
    "text": "What is a notebook\n\nThe standard for programming in python is the .py file which can hold a block of code which can contain lines of code that allow you to export the results as visualisations or data files.\nJupyter Notebooks have been developed with the data science and analytical community.\nNotebooks are a collection interactive cells which a user can run as a collection or individually, based on the current state of program.\nCells can be denoted as Code, Markdown or Raw Depending on use case.\n\nCode cells use a process called a kernel to run programme elements in the user selected code base (e.g. Python or R).\nMarkdown cells allow the user to include formatted text and other elements (such as links and images).\nRaw cells have no processing attached and output as plain text."
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#a-brief-history-of-jupyter-notebooks",
    "href": "sessions/02-jupyter_notebooks/slides.html#a-brief-history-of-jupyter-notebooks",
    "title": "Jupyter Notebooks",
    "section": "A brief history of Jupyter notebooks",
    "text": "A brief history of Jupyter notebooks\n\nIn 2001, Fernando Perez started development of the iPython project as a way of incorporating prompts and access to previous output, as he continued development he amalgamated iPython with 2 other projects\nIn 2014, Project Jupyter was born out of the initial iPython project. The key aim was to make the project independent of a programming language and allow different code bases to use notebooks. The Name is a reference to the three initial languages: Julia, Python, and R.\nJupyter Notebooks and more recently Jupiter Labs are more than just the notebook, they are interactive development environments launched from the command line.\nJupyter notebooks are used by many online platforms and service providers including: Kaggle, Microsoft Fabric, and the NHS Federated Data Platform."
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#pros-and-cons-of-using-a-notebook",
    "href": "sessions/02-jupyter_notebooks/slides.html#pros-and-cons-of-using-a-notebook",
    "title": "Jupyter Notebooks",
    "section": "Pros and cons of using a notebook",
    "text": "Pros and cons of using a notebook\nOn the plus side…\n\nNotebooks are highly interactive and allow cells to be run in any order.\nYou can re-run each cell separately, so iterative testing is more granular.\nNotebooks can be used to provide a structured report for an end user regardless of coding knowledge.\n\nHaving said that…\n\nIf you are not careful you can save a notebook in a state that cannot run as intended if changes are not checked.\nIt can be harder to understand complex code interactions."
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#the-toolkit",
    "href": "sessions/02-jupyter_notebooks/slides.html#the-toolkit",
    "title": "Jupyter Notebooks",
    "section": "The Toolkit",
    "text": "The Toolkit\n\nYou will need the following pre-installed:\n\nLanguage: Python\nDependency management: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code (or your preferred IDE)\n\nYou can install all these tools by running the following in PowerShell:\n\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop"
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#walkthrough-and-demonstration",
    "href": "sessions/02-jupyter_notebooks/slides.html#walkthrough-and-demonstration",
    "title": "Jupyter Notebooks",
    "section": "Walkthrough and demonstration",
    "text": "Walkthrough and demonstration\nif reviewing these slides this section is only available in the recording, though the initial steps used should be available on the associated Code Club site page"
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#resources",
    "href": "sessions/02-jupyter_notebooks/slides.html#resources",
    "title": "Jupyter Notebooks",
    "section": "Resources",
    "text": "Resources\n\nCheck out the History of iPython\nYou can find out more about Project Jupyter\nThe demonstration makes use of this markdown cheatsheet\nLikewise this is the Jupyter shortcuts Cheat Sheet"
  },
  {
    "objectID": "sessions/09-object-oriented-programming/index.html",
    "href": "sessions/09-object-oriented-programming/index.html",
    "title": "An Introduction to Object-Oriented Programming",
    "section": "",
    "text": "This session is the last in our series on fundamental programming concepts. It is an introduction to the programming paradigm that underpins Python, making it flexible and comparatively accessible to people starting out with coding.\nWhile the concepts discussed can feel a little abstract at times, they will make ever more sense the more practical experience you have with Python. This will help you understand how the packages you use function and will hopefully lead you one day to create some sophisticated programs yourself!\nIf you want to download the Jupyter notebook so that you can run the code yourself, modify it, and generally have a play, you can download it by clicking on the “Jupyter” link under “Other Formats” over on the right-hand side below the page contents.",
    "crumbs": [
      "Core Concepts",
      "9. Object-Oriented Programming"
    ]
  },
  {
    "objectID": "sessions/09-object-oriented-programming/index.html#session-slides",
    "href": "sessions/09-object-oriented-programming/index.html#session-slides",
    "title": "An Introduction to Object-Oriented Programming",
    "section": "Session Slides",
    "text": "Session Slides\nUse the left ⬅️ and right ➡️ arrow keys to navigate through the slides below. To view in a separate tab/window, follow this link.",
    "crumbs": [
      "Core Concepts",
      "9. Object-Oriented Programming"
    ]
  },
  {
    "objectID": "sessions/09-object-oriented-programming/index.html#a-closer-look-at-some-of-the-basics",
    "href": "sessions/09-object-oriented-programming/index.html#a-closer-look-at-some-of-the-basics",
    "title": "An Introduction to Object-Oriented Programming",
    "section": "A closer look at some of the basics",
    "text": "A closer look at some of the basics\nHere is a simple demonstration of how everything, down to individual integers, is treated as an object. When we run the dir() function on an integer and print the results, we can see all of the object attributes and methods associated with it.\n\nmy_int = 5\n\nprint(dir(my_int))\n\n['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floor__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__getstate__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__lshift__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'as_integer_ratio', 'bit_count', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'is_integer', 'numerator', 'real', 'to_bytes']\n\n\nTo see which are attributes and which are methods, you can run the following code. We won’t run it here, because it produces quite a long list!\n\nfor name in dir(my_int):\n    item = getattr(my_int, name)\n    if callable(item):  # if an item is callable, i.e. you can \"do\" something with it, it is a method\n        print(f\"{name} -&gt; method\")\n    else:\n        print(f\"{name} -&gt; attribute\") # otherwise, it is an attribute\n\nMethods and attributes that are surrounded by double underscores are ones that are used “behind the scenes” to help objects behave in a certain way (this exhibits the principle of Abstraction mentioned in the presentation). The ones without are the ones that you would call directly when working with the objects.\n\nUsing methods\nBy now, you are likely to have come across operations that can be used to transform your data by tacking them onto the end of your variable name. These are methods belonging to that variable’s object.\n\n# Here we are calling the \"append\" method of the list class.\n\nicb_list = ['QRL','QNQ','QU9','QSL'] # a list object\n\nicb_list.append('QNX') # calling the method and passing a parameter\n\nprint(icb_list)\n\n['QRL', 'QNQ', 'QU9', 'QSL', 'QNX']\n\n\nIf we apply the function dir() to our variable icb_list, we can see that “append” is one of the methods belonging to the list class.\n\nprint(dir(icb_list))\n\n['__add__', '__class__', '__class_getitem__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']\n\n\n\n\n\n\n\n\nTipRemember\n\n\n\nFunctions precede the variable function(my_variable), whereas methods are the ones that come after the variable my_variable.method().\n\n\nFunctions are not tied to a particular object class (for example, print() can be applied to most things), while methods are bound up with particular object classes (for example, the method pop is associated with lists, but not with integers).\nThe brackets that come after methods will often contain parameters that determine how the method is applied to the variable. This can be as simple as saying what we want to append to a list, as in the above example, or they can be as complex as the hyperparameters used to fine-tune a machine learning model.\n\n\nAccessing attributes\nTo access an attribute, you simply write my_variable.attribute. Below is an example of accessing the “numerator” and “denominator” attributes of an integer. In Python, integers have these to enable consistency when working with rational numbers, i.e. fractions.\n\nprint(my_int.numerator)\nprint(my_int.denominator)\n\n5\n1\n\n\n\\(\\frac{5}{1} = 5\\)\nWhen you start working with machine learning models in Python, you will often want to access attributes such as coef_ (regression) and feature_importances_ (decision tree algorithms).\nNow let’s move onto how we create our own classes.",
    "crumbs": [
      "Core Concepts",
      "9. Object-Oriented Programming"
    ]
  },
  {
    "objectID": "sessions/09-object-oriented-programming/index.html#the-anatomy-of-a-class",
    "href": "sessions/09-object-oriented-programming/index.html#the-anatomy-of-a-class",
    "title": "An Introduction to Object-Oriented Programming",
    "section": "The Anatomy of a Class",
    "text": "The Anatomy of a Class\nLet us first of all look at a simple class to see how it is structured as a whole. Then we will look at each element in turn.\n\nclass HealthProfessional:\n\n  daily_capacity = 7.5\n\n  def __init__(self, assignment_number, division, department):\n    self.assignment_number = assignment_number                 \n    self.division = division              \n    self.department = department                              \n\n  def treat_patient(self,patient_name):\n    print(f'Health professional {self.assignment_number} treated patient {patient_name}')\n\n\nDefinition\nThe line begins the definition of the class. It starts with the class keyword, followed by the class name and a colon. By convention, classes in Python begin with a capital letter.\n\nclass HealthProfessional:\n\n\n\nClass attribute\nThe next element is a class attribute, that is to say an attribute where the value will initially be the same for all objects created from that class. All of my health professionals have a default daily_capacity of 7.5 working hours per day. Class attributes sit just below the class name with one indent. Class attributes are optional.\n\nclass HealthProfessional:\n  \n  daily_capacity = 7.5\n\n\n\nConstructor method\nNext comes the constructor method, which goes by the name of __init__. The double underscores (a so-called “dunder” method) indicate that it is a method that remains internal to the object i.e. it is not something that is accessed by the end user.\nThe constructor method defines what happens when an object instance is created (instantiated). It determines which attributes require values to be passed to the object at instantiation and can also trigger some methods to be execute automatically.\nIt is written much in the same way as defining a function, starting with the keyword def. The first argument of the function is always self and is followed by arguments representing each of the object attributes.\nBelow that, each object attribute is bound to self with the self.attribute syntax and the = attribute syntax means that the corresponding value that gets passed to the object at instantiation will be assigned to that attribute.\n\n  def __init__(self, assignment_number, division, department):\n    self.assignment_number = assignment_number                 \n    self.division = division              \n    self.department = department    \n\n\nSome quick notes on self\n\nself stores the instance of each object within the computer’s memory.\nwhenever we define a class method, we include self because we want to run this method in relation to this instance when we call it.\nwhenever we define an object attribute, we include self because we want to be able access the attribute value that belongs to this instance.\n\n\n\n\nClass method\nNow, returning to the definition of a class, we come to the class method.\nIt is again defined just like a normal function, and this time we can give it whatever name we like. The first argument is always self, which is followed by any other arguments relevant to the method. In our example, we want to pass a patient_name to the HealthProfessional object so that it knows which patient it is being asked to treat.\n\n  def treat_patient(self,patient_name):\n    print(f'Health professional {self.assignment_number} treated patient {patient_name}')\n\n\n\nOur class in action\nNow we can have a look at our HealthProfessional class in action.\nFirst of all, we need to instantiate a HealthProfessional object. We need to assign it to a variable name so that we can easily refer to the object later on.\n\ndoctor_duggee = HealthProfessional(\n  assignment_number = 12345,          # writing out the argument names is optional\n  division = \"A\",\n  department= \"Surgery\"\n)\n\nWe can use the .treat_patient method:\n\ndoctor_duggee.treat_patient(\"Betty\")\n\nHealth professional 12345 treated patient Betty\n\n\nWe can access the object’s object attributes:\n\nprint(f'Health Professional {doctor_duggee.assignment_number} works in the {doctor_duggee.department} department')\n\nHealth Professional 12345 works in the Surgery department\n\n\nAnd we can access the class attribute:\n\ndoctor_duggee.daily_capacity\n\n7.5\n\n\nIt is also very easy to update an object’s attribute values. Compare this with updating the values in a dict, which can be quite fiddly.\n\ndoctor_duggee.department = 'Medicine for Older Persons'\ndoctor_duggee.division = 'C'\n\nprint(f'Health Professional {doctor_duggee.assignment_number} works in the {doctor_duggee.department} department in Division {doctor_duggee.division}')\n\nHealth Professional 12345 works in the Medicine for Older Persons department in Division C\n\n\nClass attribute values can be updated at object level in the same way. This change won’t affect new objects created from the class. Think of class attributes as holding a common default value.\n\ndoctor_duggee.daily_capacity = 8\nprint(f'Duggee\\'s daily capacity: {doctor_duggee.daily_capacity} hours.')\n\nDuggee's daily capacity: 8 hours.",
    "crumbs": [
      "Core Concepts",
      "9. Object-Oriented Programming"
    ]
  },
  {
    "objectID": "sessions/09-object-oriented-programming/index.html#inheritance",
    "href": "sessions/09-object-oriented-programming/index.html#inheritance",
    "title": "An Introduction to Object-Oriented Programming",
    "section": "Inheritance",
    "text": "Inheritance\nOne of the main strengths of object-oriented programming is the ability to create child classes from other classes (the original, parent classes). This allows us to create classes that modify or extend the data (attributes) and behaviours (methods) of the parent class, without altering the parent class itself. Multiple child classes can be created which share the attributes and methods consistently, but extend or modify them in their own way.\nLet us have a look at how a child class gets created from the parent class. Again, we will start off with the child class written out in full so that you can see how it would look. Then we will go through it line by line.\nWe are going to create “Doctor” and “Nurse” child classes of the HealthProfessional parent class. They will inherit the attributes and methods of the HealthProfessional class, but extend them in their own way.\n\nclass Doctor(HealthProfessional):\n    def __init__(self,assignment_number,division,department,seniority):\n        self.seniority = seniority\n        super().__init__(assignment_number,division,department)\n\n    def discharge_patient(self,patient_name):\n        print(f'Doctor {self.assignment_number} discharged patient {patient_name}')\n\n\nChild class definition\nFirst of all, when defining a child class, we start off with the keyword class followed by the name of the child class, followed by the name of the parent class in brackets, ending with a colon.\n\nclass Doctor(HealthProfessional):\n\n\n\nChild class constructor method and child attributes\nThen, with one indentation, we write the constructor method, just as we did with the parent class. Remember, we must always enter self as the first argument. Then we need to enter the names of the attributes that the child class will inherit from the parent, followed by any new object attributes. In the second line, the any new object attributes are bound to self.\n\n    def __init__(self,assignment_number,division,department,seniority):\n        self.seniority = seniority\n\n\n\nThe super(). function\nBelow any new attributes, we enter a line that draws down any methods and attributes from the parent class. super() is a function that calls the constructor function from the parent class.\n\n        super().__init__(assignment_number,division,department)\n\nNote that you do not need to refer to any methods or class attributes here. They automatically get replicated by the super() method. The only elements that you need to explicitly mention are the object attributes defined in the parent class.\n\n\n\n\n\n\nTipRemember\n\n\n\nClass attributes are the ones that have “default” values that are common to all objects created from a class. In our HealthProfessional example, this was daily_capacity. They are written just under the class name and are not bound to self. Object attributes receive their values when an object is instantiated and are specific to that object. They are bound to self.\n\n\n\n\nChild-specific methods\nLast in our example is the definition of any new methods that are specific to the child class. Doctor objects will be able to treat patients with the inherited .treat_patient() method, but we are also going to give them the ability to discharge patients with the .discharge_patient() method.\nAs with the .treat_patient() method in the parent class, we use the def keyword, followed by the name of the method for the child class, self as the first argument, and the name of any variable that we want to pass to the method, in this case the name of the patient we want to discharge.\n\n    def discharge_patient(self,patient_name):\n        print(f'Doctor {self.assignment_number} discharged patient {patient_name}')\n\n\n\nOur child class in action\nLet’s create an object instance of the Doctor class and try out the methods and attributes.\n\ndoctor_peppa = Doctor(\n  999999,\n  \"B\",\n  \"Ophthalmology\",\n  \"Consultant\"\n)\n\n\nprint(f'Assignment number: {doctor_peppa.assignment_number}')\nprint(f'Division: {doctor_peppa.division}')\nprint(f'Department: {doctor_peppa.department}')\nprint(f'Seniority: {doctor_peppa.seniority}')\n\nAssignment number: 999999\nDivision: B\nDepartment: Ophthalmology\nSeniority: Consultant\n\n\nNote that we can still retrieve the daily_capacity attribute, which was defined in the parent class HealthProfessional\n\ndoctor_peppa.daily_capacity\n\n7.5\n\n\nHowever, we can prove that Doctor Peppa is a Doctor object and not a HealthProfessional object by using Python’s type() function\n\ntype(doctor_peppa)\n\n__main__.Doctor\n\n\nWe can also test Doctor Peppa’s methods:\n\ndoctor_peppa.treat_patient(\"Suzy Sheep\")\n\nHealth professional 999999 treated patient Suzy Sheep\n\n\n\n\n\n\n\n\nTipInteresting…\n\n\n\nDid you notice how it said “Health professional”? This was actually unintentional, but it neatly demonstrates how the .treat_patient() method has been inherited from the parent class (because the text of the statement wasn’t amended when the child class was created).\n\n\n\ndoctor_peppa.discharge_patient(\"Suzy Sheep\")\n\nDoctor 999999 discharged patient Suzy Sheep\n\n\nWhat happens if Doctor Duggee tries to discharge a patient? Try uncommenting the Python below and running the cell.\n\n# doctor_duggee.discharge_patient(\"Happy\")\n\nWe get an error, because doctor_duggee is a HealthProfessional object, which does not contain the .discharge_patient() method.\nIf you want to turn doctor_duggee into a Doctor object, you need to instantiate him as such.\n\ndoctor_duggee = Doctor(\n  assignment_number = 12345,\n  division = \"A\",\n  department= \"Surgery\",\n  seniority=\"Resident\"\n)\n\ntype(doctor_duggee)\n\n__main__.Doctor\n\n\n\n\nDefining a second child class\nLet’s quickly define the Nurse child class of HealthProfessional to demonstrate how different child classes can share the same inherited characteristics, but still have their own.\nInstead of seniority, Nurse objects will have band and role attributes, and instead of .discharge_patient() the method .take_readings().\n\nclass Nurse(HealthProfessional):\n    def __init__(self,assignment_number,division,department,band,role):\n        self.band = band\n        self.role = role\n        super().__init__(assignment_number,division,department)\n\n  # Adding a new method to the child class\n    def take_readings(self,patient_name):\n        print(f'Nurse {self.assignment_number} took the vital signs readings of {patient_name}')\n\n\nnurse_tag = Nurse(\n  assignment_number = 10101,\n  division = \"A\",\n  department = \"Cancer Care\",\n  band = \"5\",\n  role = \"Staff Nurse\"\n)\n\nprint(f'Assignment number:  {nurse_tag.assignment_number}')\nprint(f'Division: {nurse_tag.division}')\nprint(f'Department: {nurse_tag.department}')\nprint(f'AfC Band: {nurse_tag.band}')\nprint(f'Role: {nurse_tag.role}')\nprint(f'Daily Capacity: {nurse_tag.daily_capacity}')\n\nnurse_tag.take_readings(\"Roly\")\n\nAssignment number:  10101\nDivision: A\nDepartment: Cancer Care\nAfC Band: 5\nRole: Staff Nurse\nDaily Capacity: 7.5\nNurse 10101 took the vital signs readings of Roly\n\n\n\n\nThe isinstance() function\nThere is also a function that can check whether an object belongs to a specified class. (This is particularly useful for checking data types when a specific type needs to be enforced, given that every variable is treated as an object belonging to a particular class. This data type enforcement doesn’t happen by default in Python since it is a dynamically-typed language. Leaving this up to Python could lead to a mismatch of data types, which could lead to errors further down the line).\n\nisinstance(nurse_tag,Nurse)\n\nTrue\n\n\n\n# Imagine we wanted to ensure that AfC bands always get stored as strings\n# for consistency since they can include alphanumeric values (e.g. \"8a\") as\n# well as purely numeric values (e.g. \"6\")\n\nmy_input = 7 # this will be interpreted as an integer\n\nif isinstance(my_input,str):\n  print('Valid input')\nelse:\n  print('Invalid input: this field only accepts text strings')\n\nInvalid input: this field only accepts text strings\n\n\nThose are the fundamentals of how to create classes and objects. Let’s have a look at something a bit more advanced to give you an idea of something you might want to create for a team of analysts.",
    "crumbs": [
      "Core Concepts",
      "9. Object-Oriented Programming"
    ]
  },
  {
    "objectID": "sessions/09-object-oriented-programming/index.html#a-practical-example",
    "href": "sessions/09-object-oriented-programming/index.html#a-practical-example",
    "title": "An Introduction to Object-Oriented Programming",
    "section": "A practical example",
    "text": "A practical example\nBelow is an example of how we might create an standard SCW chart template for different kinds of charts using a class. The string and Boolean values in the constructor method are default values that can be overwritten by any values passed to the object at instantiation.\n\nimport matplotlib.pyplot as plt\n\nclass SCWPlot:\n  def __init__(self, x_data, y_data, title=\"SCW Plot\", xlabel=\"X-axis\", ylabel=\"Y-axis\", grid=False):\n    self.x_data = x_data\n    self.y_data = y_data\n    self.title = title\n    self.xlabel = xlabel\n    self.ylabel = ylabel\n    self.grid = grid\n    # Colour palette based on SCW colours\n    self.palette = ['#1C355E','#005EB8','#00A9CE','#330072','#78BE20']\n\n  # This part is a method internal to the class. It is not accessed by users, but helps to standardise\n  # the methods relating to each type of chart. By convention, these internal methods begin with an underscore.\n  def _setup_plot(self):\n    plt.figure(figsize=(8, 5))\n    plt.title(self.title)\n    plt.xlabel(self.xlabel)\n    plt.ylabel(self.ylabel)\n    plt.grid(visible=True,which='major',axis='both')\n    plt.tight_layout()\n\n  # The two methods below are each for a different kind of chart that the user can plot. They both make use of\n  # the ._setup_plot() to determine certain shared, consistent characteristics.\n  def plot_line(self, linestyle='-', marker='o'):\n    self._setup_plot()            # use the common structure defined above\n    plt.plot(                     # plot a line chart with the data passed to the object\n        self.x_data, \n        self.y_data, \n        color=self.palette[0],    # use the first value in the palette list\n        linestyle=linestyle, \n        marker=marker\n    )\n    plt.show()                    # display the chart\n\n  def plot_bar(self):\n    self._setup_plot()            # use the common structure defined above\n    plt.bar(                      # plot a bar chart with data passed to the object\n      self.x_data, \n      self.y_data, \n      color=self.palette\n    ) \n    plt.show()                    # display the chart\n\nThen we will create a very simple dataset to use for testing the use of our class.\n\nx = ['A', 'B', 'C', 'D', 'E']\ny = [5, 7, 3, 8, 6]\n\nNow we can instantiate an SCWPlot object and call the two chart type methods to produce charts.\n\nplot = SCWPlot(x, y, title=\"SCW-Branded Plot\", xlabel=\"Category\", ylabel=\"Value\")\n\nplot.plot_line()\n\n\n\n\n\n\n\n\n\nplot.plot_bar()\n\n\n\n\n\n\n\n\nAs you can see, it becomes easy for the user to switch between different kinds of plots while maintaining a standard colour scheme and chart size.\nIn reality, you would not have all of the implementation code (i.e. the code written to create the class) in your script or notebook. This would stored in a separate Python file and the class would be imported into the script. In a future session we will talk about this and creating our own packages.\n\nSimple maintenance and extensibility\nCreating standardised functions as methods of a class helps to simplify maintaining and extending functionality. In the example above, things like the colour palette and figure size are defined once and these get cascaded to the methods relating to individual chart types, so they only need to be edited once. Furthermore, it is easy for a colleague to add another chart type (e.g. horizontal bar) with four lines of code, without necessarily having to understand in detail how the rest of the class has been constructed. This example produces relatively simple charts, but there is the potential to create a class that consistently produces timeseries charts with a preferred x-axis layout (something that can be quite fiddly to perfect!).\nWhile this class can be extended by adding more chart type methods, can you think of a possible reason for creating a child class from it?",
    "crumbs": [
      "Core Concepts",
      "9. Object-Oriented Programming"
    ]
  },
  {
    "objectID": "sessions/09-object-oriented-programming/index.html#exercises",
    "href": "sessions/09-object-oriented-programming/index.html#exercises",
    "title": "An Introduction to Object-Oriented Programming",
    "section": "Exercises",
    "text": "Exercises\n\nHow do you check what class a variable is?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nmy_variable = 5\n\ntype(my_variable)\n\nint\n\n\n\n\n\n\nHow would you check whether the value of the variable in the previous solution is an integer?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nisinstance(my_variable,int)\n\nTrue\n\n\n\n\n\n\nWhich function can be used to display all of the methods and attributes belonging to an object?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\ndir(my_variable)\n\n['__abs__',\n '__add__',\n '__and__',\n '__bool__',\n '__ceil__',\n '__class__',\n '__delattr__',\n '__dir__',\n '__divmod__',\n '__doc__',\n '__eq__',\n '__float__',\n '__floor__',\n '__floordiv__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getnewargs__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__index__',\n '__init__',\n '__init_subclass__',\n '__int__',\n '__invert__',\n '__le__',\n '__lshift__',\n '__lt__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__neg__',\n '__new__',\n '__or__',\n '__pos__',\n '__pow__',\n '__radd__',\n '__rand__',\n '__rdivmod__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__rfloordiv__',\n '__rlshift__',\n '__rmod__',\n '__rmul__',\n '__ror__',\n '__round__',\n '__rpow__',\n '__rrshift__',\n '__rshift__',\n '__rsub__',\n '__rtruediv__',\n '__rxor__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__sub__',\n '__subclasshook__',\n '__truediv__',\n '__trunc__',\n '__xor__',\n 'as_integer_ratio',\n 'bit_count',\n 'bit_length',\n 'conjugate',\n 'denominator',\n 'from_bytes',\n 'imag',\n 'is_integer',\n 'numerator',\n 'real',\n 'to_bytes']\n\n\n\n\n\n\nHow do you access an object’s attributes? What is the syntax?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nobject.attribute\n\nmy_variable.numerator\n\n5\n\n\n\n\n\n\nHow do you call an object’s methods? What is the syntax?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nobject.method()\n\nmy_variable.is_integer()\n\nTrue\n\n\n\n\n\n\nCreate a class called “Patient” with the characteristics below. You may wish to refer back to previous sessions on core programming concepts for guidance.\n\n\nA class attribute called “bed_space” and assign it the value 1\nObject attributes for “patient_number” and “diagnosis”, values for which will be assigned at instantiation, and “cured” which has the default value of False as well as “admitted” with the default value of True\nA method called “get_well” which updates the “cured” attribute to True and prints a statement that returns the “patient_number” and the “diagnosis” that the patient has been cured of. (Hint: you will need to use an f-string with the attributes embedded in the statement).\nA method called “get_discharged” which checks whether the patient has been cured and if this is True, prints a statement to say that the patient has been discharged and sets “admitted” to False. Othwerwise, it prints a statement to say that the patient is not ready to be discharged.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nclass Patient:\n\n  bed_space = 1\n\n  def __init__(self,patient_number,diagnosis):\n    self.patient_number = patient_number\n    self.diagnosis = diagnosis\n    self.cured = False\n    self.admitted = True\n\n  def get_well(self):\n    self.cured = True\n    print(f'Patient {self.patient_number} has been cured of {self.diagnosis}')\n  \n  def get_discharged(self):\n    if self.cured == True:\n      print(f'Patient {self.patient_number} has been discharged')\n      self.admitted = False\n    else:\n      print(f'Patient {self.patient_number} is not ready to be discharged')\n\n\n\n\n\nInstantiate a Patient object with a “patient_number” and “diagnosis”. Try discharging the patient before they have been cured, then call the get_well method and try discharging the patient again.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nmy_patient = Patient(patient_number='12345',diagnosis='acute tummy ache')\n\nmy_patient.get_discharged()\n\nmy_patient.get_well()\n\nmy_patient.get_discharged()\n\nPatient 12345 is not ready to be discharged\nPatient 12345 has been cured of acute tummy ache\nPatient 12345 has been discharged\n\n\n\n\n\n\nWith the Patient object you created, how would you access the “admitted” attribute to check whether they are still an inpatient?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nmy_patient.admitted\n\nFalse\n\n\n\n\n\n\nCreate a child class called “SurgeryPatient” that inherits the methods and attributes from Patient. Add an attribute called “theatre” to store the name of the operating theatre that gets assigned when the object is created and a method that checks whether the patient has been cured or not. When False, the method prints a message requesting that the patient be transferred to the theatre, when True the method prints a message to say that the patient can proceed to discharge.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nclass SurgeryPatient(Patient):\n  def __init__(self, patient_number, diagnosis, theatre):\n    self.theatre = theatre\n    super().__init__(patient_number, diagnosis)\n\n  def theatre_call(self):\n    if self.cured == False:\n      print(f'Please transfer patient {self.patient_number} to theatre {self.theatre}')\n    else:\n      print(f'Patient {self.patient_number} is now ready for discharge')\n\n\n\n\n\nInstantiate a SurgeryPatient object, test whether the attributes and methods have been inherited from the Patient class, and test the .theatre_call() method. Your solution doesn’t have to match exactly. Just be sure that you are able to test each element of the class.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nmy_surgery_patient = SurgeryPatient(patient_number=54321,diagnosis='a broken leg',theatre='11F')\n\nprint(f'This patient occupies {my_surgery_patient.bed_space} bed')\n\nmy_surgery_patient.get_discharged()\n\nmy_surgery_patient.theatre_call()\n\nmy_surgery_patient.get_well()\n\nmy_surgery_patient.theatre_call()\n\nmy_surgery_patient.get_discharged()\n\nprint(f'Is this patient still on our inpatient ward? {my_surgery_patient.admitted}')\n\nThis patient occupies 1 bed\nPatient 54321 is not ready to be discharged\nPlease transfer patient 54321 to theatre 11F\nPatient 54321 has been cured of a broken leg\nPatient 54321 is now ready for discharge\nPatient 54321 has been discharged\nIs this patient still on our inpatient ward? False",
    "crumbs": [
      "Core Concepts",
      "9. Object-Oriented Programming"
    ]
  },
  {
    "objectID": "sessions/09-object-oriented-programming/index.html#bonus-challenge",
    "href": "sessions/09-object-oriented-programming/index.html#bonus-challenge",
    "title": "An Introduction to Object-Oriented Programming",
    "section": "Bonus challenge",
    "text": "Bonus challenge\n\n\n\n\n\n\nWe haven’t provided a suggested solution for this one, but do feel free to discuss potential answers in the Code Club channels.\n\n\n\nInstantiate a smallish number of Patient objects (no need to spend too much time on this, but you do need more than one!). Can you work out a way to total the amount of bed space occupied by your patients by summing together the bed_space class attribute values? Try using a loop.",
    "crumbs": [
      "Core Concepts",
      "9. Object-Oriented Programming"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html",
    "href": "sessions/06-data-types/index.html",
    "title": "An Introduction to Data Types",
    "section": "",
    "text": "This session is the first in a series of programming fundamentals. We recognise that this content might be a bit more dry and abstract, but it is important background to know when you start to actually use Python in your day to day work.\nIf you’ve used Excel and changed the data format for a cell, you’ve already come across data types! It is important to understand how Python stores values in variables and the pitfalls, gotchas and errors you may come across when working with data. The slide deck below gives a (little) bit of history before giving an overview of how data types work in Python. On the last slides are some links to useful resources on the web, which you may want to make note of for the future. Below the slides is a live notebook that demonstrates this, with some exercises at the end to check your understanding.",
    "crumbs": [
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#automatically",
    "href": "sessions/06-data-types/index.html#automatically",
    "title": "An Introduction to Data Types",
    "section": "Automatically",
    "text": "Automatically\nPython automatically assigns a type to a variable based on the value we put into it when we use the = assignment operator.\n\nour_integer = 1\nour_float = 2.2\nour_integer_turned_into_a_float = float(our_integer)\nour_string=\"Hello SCW!\"",
    "crumbs": [
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#manually",
    "href": "sessions/06-data-types/index.html#manually",
    "title": "An Introduction to Data Types",
    "section": "Manually",
    "text": "Manually\nIf we need to, we can use a constructor function named after the data type, like int() or str() to force a variable to be the specific type we need it to be.\n\na = str(\"123\") # a will contain the string 123 rather than the numeric value\nb = float(2) # b will contain the decimal value 2.0\nc = int(1.9) # just throws away the .9; not rounded!",
    "crumbs": [
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#finding-out-what-type-a-variable-is",
    "href": "sessions/06-data-types/index.html#finding-out-what-type-a-variable-is",
    "title": "An Introduction to Data Types",
    "section": "Finding out what type a variable is",
    "text": "Finding out what type a variable is\n\nprint (type(a)) # output: &lt;class 'str'&gt;\nprint (type(b)) # output: &lt;class 'float'&gt;\nprint (type(c)) # output: &lt;class 'int'&gt;",
    "crumbs": [
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#booleans",
    "href": "sessions/06-data-types/index.html#booleans",
    "title": "An Introduction to Data Types",
    "section": "Booleans",
    "text": "Booleans\nBools are often an intermediate - they are an output of evaluations like 1 == 2. Booleans may sound very basic, but they are crucial in understanding control flow, which we’ll be covering in a future session!\n\nz = True            # you'll rarely ever assign a boolean directly like this, but do note they are\n                    # case sensitive; z = true wouldn't have worked here.\nprint(type(z))      # output: &lt;class 'bool'&gt;\nprint (10&gt;9)        # output: True\nprint (1 == 2)      # output: False\n \nprint(bool(123))    # output: True\nprint(bool(\"abc\"))  # output: True\nprint(bool(None))   # output: False\nprint(bool(0))      # output: False",
    "crumbs": [
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#numeric-types",
    "href": "sessions/06-data-types/index.html#numeric-types",
    "title": "An Introduction to Data Types",
    "section": "Numeric types",
    "text": "Numeric types\nPython supports different kinds of numbers, including integers (int), floating point numbers (float). You can do basic arithmetic (+, -, *, /), exponentiation (**), and use built-in functions like round(), abs(), and pow().\n\na = 10              # int\nb = 3               # int\nc = 2.5             # float\nd = -2              # int\n\nprint(a+b)          # output: 13, an int\nprint(a+c)          # output: 12.5, a float\nprint(a ** (1/2))   # taking the square root of an int returns a float\n \nprint(float(a))     # output: 10.0\nprint(int(2.88))    # output: 2; just throws away the decimal part\n \nprint(round(2.88))  # output: 3\nprint(round(2.88,1))# output: 2.9",
    "crumbs": [
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#strings",
    "href": "sessions/06-data-types/index.html#strings",
    "title": "An Introduction to Data Types",
    "section": "Strings",
    "text": "Strings\nStrings are sequences of characters enclosed in quotes. They support indexing, slicing, and a range of methods like .lower(), .replace(), .split(), and .join().\n\nstr_a = \"Hello\"              # string\nstr_b = \"SCW!\"               # string\n\nstr_ab = str_a + \" \" + str_b # python repurposes the \"+\" to mean string concatenation as well as addition\nprint(str_ab)                # output: Hello SCW!\n \nprint(str_ab.find(\"SCW\"))    # output:6 (the location in the string of the substring \"SCW\". Starts from 0!)\n \nstr_repeated = str_ab * 3 \nprint(str_repeated)          # output: Hello SCW!Hello SCW!Hello SCW!\n \nprint(len(str_a))            # output: 5\nprint(str_a[0])              # output: H\nprint(str_a[0:3])            # output: Hel (give me 3 characters starting at 0)\nprint(str_a[3:])             # output: lo (give me everything starting at 3)\nprint(str_a[:5])             # output: Hello (give me the first 5 characters)",
    "crumbs": [
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#lists",
    "href": "sessions/06-data-types/index.html#lists",
    "title": "An Introduction to Data Types",
    "section": "Lists",
    "text": "Lists\nLists are ordered, mutable (changeable) collections. They can hold any type of data and support operations like appending (.append()), removing (.remove()), and slicing (our_list[1:4]).\n\nfruits = [\"banana\", \"lychee\", \"raspberry\", \"apple\"]\nprint(fruits[0])          # output: banana (string)\nprint(fruits[0:2])        # output: ['banana','lychee'] (list!)\nprint(fruits[-1])         # output: apple (string)\n \nfruits.append(\"orange\") \nprint(fruits)             # output: ['banana', 'lychee', 'raspberry', 'apple', 'orange']\n \nprint(\"orange\" in fruits) # output: True\nprint(\"tomato\" in fruits) # output: False\n \nfruits.sort() \nprint(fruits)             # output: ['apple', 'banana', 'lychee', 'orange', 'raspberry']\n\nLists can contain any combination of other data types.\n\nmixed_list = [\"blue\", \"green\", False, 2, 2.55]\nfor item in mixed_list: # we're using a loop here; don't worry if you don't recognise this syntax\n    print(type(item))   # output:&lt;class 'str'&gt; &lt;class 'str'&gt; &lt;class 'bool'&gt; &lt;class 'int'&gt; &lt;class 'float'&gt;",
    "crumbs": [
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/06-data-types/index.html#dicts",
    "href": "sessions/06-data-types/index.html#dicts",
    "title": "An Introduction to Data Types",
    "section": "Dicts",
    "text": "Dicts\nDictionaries store key-value pairs and are optimized for lookups. Keys must be unique and are immutable, but values are mutable. You can add, update, or delete items using dict[key] = value, dict.get(key), or del dict[key].\n\nSCW_basic_info={\n    \"org_code\"      : \"0DF\",\n    \"short_name\"    : \"SCW CSU\",\n    \"long_name\"     : \"NHS South, Central and West Commissioning Support Unit\",\n    \"year_opened\"   : 2014,\n    \"active\"        : True,\n    \"postcode\"      : \"SO50 5PB\"\n}\n\nprint(type(SCW_basic_info[\"active\"]))       # output: &lt;class 'bool'&gt;\nprint(type(SCW_basic_info[\"year_opened\"]))  # output: &lt;class 'int'&gt;\n \nprint(SCW_basic_info[\"org_code\"])           # output: \"0DF\"\nprint(len(SCW_basic_info))                  # output: 6\n \nSCW_basic_info[\"number_of_staff\"] = 1000    # we can easily add a new key and value at the same time\n \nprint(len(SCW_basic_info))                  # output: 7\n \nSCW_basic_info[\"number_of_staff\"] += 1      # we hired a new member of staff\nprint(SCW_basic_info[\"number_of_staff\"])    # output: 1001",
    "crumbs": [
      "Core Concepts",
      "6. Data Types"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html",
    "href": "sessions/03-eda-pandas/index.html",
    "title": "Exploring Data Using Pandas",
    "section": "",
    "text": "This is the first of four sessions looking at how to explore data in Python. This session will focus on introducing the Python library, pandas. We will use pandas to import, inspect, summarise, and transform the data, illustrating a typical exploratory data analysis workflow.\nWe are using Australian weather data, taken from Kaggle. This dataset is used to build machine learning models that predict whether it will rain tomorrow, using data about the weather every day from 2007 to 2017. To download the data, click here.",
    "crumbs": [
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#prep-work",
    "href": "sessions/03-eda-pandas/index.html#prep-work",
    "title": "Exploring Data Using Pandas",
    "section": "Prep work",
    "text": "Prep work\nPlace the weatherAUS.csv file you downloaded in a folder called data within your project folder.\nThere’s also one more package we need, called skimpy. In your terminal (PowerShell), from within your project folder:\nuv add skimpy\nThen, in your JuPyter notebook in VS Code, create a code cell and start by importing the packages we’ll need.\n\n# Importing packages. This won't work if we didn't add them to our project using uv earlier.\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom skimpy import skim\n\nThe bits after as (plt, np, pd, etc.) are called aliases. These are just for convenience and aren’t essential, but it’s easier to type plt every time rather than matplotlib.pyplot, for example. We can choose whatever alias we want but it’s good to stick with common conventions like pd for pandas - this is especially useful for colleagues who might need to read your code further down the line (and who will likely understand what pd is without having to look back to where you imported your packages).\nNext, we’re going to import the data using read_csv from pandas. We’re putting our data into a dataframe which we’re giving the name df - again, we could have chosen whatever name we liked.\n\ndf = pd.read_csv('data/weatherAUS.csv')",
    "crumbs": [
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#setting-the-scene",
    "href": "sessions/03-eda-pandas/index.html#setting-the-scene",
    "title": "Exploring Data Using Pandas",
    "section": "Setting the Scene",
    "text": "Setting the Scene\nBefore we start to explore any dataset, we need to establish what we are looking to do with the data. This should inform our decisions with any exploration, and any analysis that follows.\nQuestions:\n\nWhat are we trying to achieve?\nHow do our goals impact our analysis?\nWhat should we take into consideration before we write any code?\nWhat sort of questions might we be interested in with this dataset?\n\n\nWhat Our Data Can Tell Us (And What it Can’t)\nWe also need to consider what the data is and where it came from.\nQuestions:\n\nHow was the data collected?\nWhat is it missing?\nWhat do the variables in our dataset actually mean, and are they a good approximation of the concepts we are interested in?",
    "crumbs": [
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#exploring-the-dataset",
    "href": "sessions/03-eda-pandas/index.html#exploring-the-dataset",
    "title": "Exploring Data Using Pandas",
    "section": "Exploring the Dataset",
    "text": "Exploring the Dataset\nFirst, we should start with dataset-wide operations.\nQuestions:\n\nWhat do we want to know about a dataset when we first encounter it?\nHow do we get a quick overview of the data that can help us in our next steps?\nWe need to get a “feel” for the data before we can really make any decisions about how to analyse it. How do we get there with a new dataset?\n\nWe can start by getting a quick glance at the data. The starting point when you have just imported a new dataset is usually the pandas method .head(n), which shows the top \\(n\\) rows of the dataset (by default, if we don’t specify a number, it shows the top five rows).\n\n# view the top five rows\ndf.head()\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n0\n2008-12-01\nAlbury\n13.4\n22.9\n0.6\nNaN\nNaN\nW\n44.0\nW\n...\n71.0\n22.0\n1007.7\n1007.1\n8.0\nNaN\n16.9\n21.8\nNo\nNo\n\n\n1\n2008-12-02\nAlbury\n7.4\n25.1\n0.0\nNaN\nNaN\nWNW\n44.0\nNNW\n...\n44.0\n25.0\n1010.6\n1007.8\nNaN\nNaN\n17.2\n24.3\nNo\nNo\n\n\n2\n2008-12-03\nAlbury\n12.9\n25.7\n0.0\nNaN\nNaN\nWSW\n46.0\nW\n...\n38.0\n30.0\n1007.6\n1008.7\nNaN\n2.0\n21.0\n23.2\nNo\nNo\n\n\n3\n2008-12-04\nAlbury\n9.2\n28.0\n0.0\nNaN\nNaN\nNE\n24.0\nSE\n...\n45.0\n16.0\n1017.6\n1012.8\nNaN\nNaN\n18.1\n26.5\nNo\nNo\n\n\n4\n2008-12-05\nAlbury\n17.5\n32.3\n1.0\nNaN\nNaN\nW\n41.0\nENE\n...\n82.0\n33.0\n1010.8\n1006.0\n7.0\n8.0\n17.8\n29.7\nNo\nNo\n\n\n\n\n5 rows × 23 columns\n\n\n\nYou can also look at the bottom rows of the dataset, using .tail(n). This might be useful if you are dealing with time-series data. Below, we specify that we want to look at the bottom ten rows.\n\n# view the bottom ten rows\ndf.tail(10)\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n145450\n2017-06-16\nUluru\n5.2\n24.3\n0.0\nNaN\nNaN\nE\n24.0\nSE\n...\n53.0\n24.0\n1023.8\n1020.0\nNaN\nNaN\n12.3\n23.3\nNo\nNo\n\n\n145451\n2017-06-17\nUluru\n6.4\n23.4\n0.0\nNaN\nNaN\nESE\n31.0\nS\n...\n53.0\n25.0\n1025.8\n1023.0\nNaN\nNaN\n11.2\n23.1\nNo\nNo\n\n\n145452\n2017-06-18\nUluru\n8.0\n20.7\n0.0\nNaN\nNaN\nESE\n41.0\nSE\n...\n56.0\n32.0\n1028.1\n1024.3\nNaN\n7.0\n11.6\n20.0\nNo\nNo\n\n\n145453\n2017-06-19\nUluru\n7.4\n20.6\n0.0\nNaN\nNaN\nE\n35.0\nESE\n...\n63.0\n33.0\n1027.2\n1023.3\nNaN\nNaN\n11.0\n20.3\nNo\nNo\n\n\n145454\n2017-06-20\nUluru\n3.5\n21.8\n0.0\nNaN\nNaN\nE\n31.0\nESE\n...\n59.0\n27.0\n1024.7\n1021.2\nNaN\nNaN\n9.4\n20.9\nNo\nNo\n\n\n145455\n2017-06-21\nUluru\n2.8\n23.4\n0.0\nNaN\nNaN\nE\n31.0\nSE\n...\n51.0\n24.0\n1024.6\n1020.3\nNaN\nNaN\n10.1\n22.4\nNo\nNo\n\n\n145456\n2017-06-22\nUluru\n3.6\n25.3\n0.0\nNaN\nNaN\nNNW\n22.0\nSE\n...\n56.0\n21.0\n1023.5\n1019.1\nNaN\nNaN\n10.9\n24.5\nNo\nNo\n\n\n145457\n2017-06-23\nUluru\n5.4\n26.9\n0.0\nNaN\nNaN\nN\n37.0\nSE\n...\n53.0\n24.0\n1021.0\n1016.8\nNaN\nNaN\n12.5\n26.1\nNo\nNo\n\n\n145458\n2017-06-24\nUluru\n7.8\n27.0\n0.0\nNaN\nNaN\nSE\n28.0\nSSE\n...\n51.0\n24.0\n1019.4\n1016.5\n3.0\n2.0\n15.1\n26.0\nNo\nNo\n\n\n145459\n2017-06-25\nUluru\n14.9\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nESE\n...\n62.0\n36.0\n1020.2\n1017.9\n8.0\n8.0\n15.0\n20.9\nNo\nNaN\n\n\n\n\n10 rows × 23 columns\n\n\n\nA quick glimpse at the data is useful, but we may also want to get quick descriptions of several aspects of the data. Such as the length of the dataset (len(), which can also be used to get the length of various Python objects), which tells us how many observations we have.\n\n# get the object length\nlen(df)\n\n145460\n\n\nAnother option is pd.DataFrame.shape(), which shows the length (number of rows) and width (number of columns).\n\n# get the object shape (number of rows, number of columns)\ndf.shape\n\n(145460, 23)\n\n\nSpeaking of columns, if we want a quick list of the column names, we can get this using pd.DataFrame.columns().\n\n# get all column names\ndf.columns\n\nIndex(['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation',\n       'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm',\n       'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',\n       'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am',\n       'Temp3pm', 'RainToday', 'RainTomorrow'],\n      dtype='object')\n\n\nA quick and easy way to get some valuable information about the dataset is pd.DataFrame.info(), including the total non-null observations and data type1 of each column.\n\n# get dataframe info (column indices, non-null counts, data types)\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 145460 entries, 0 to 145459\nData columns (total 23 columns):\n #   Column         Non-Null Count   Dtype  \n---  ------         --------------   -----  \n 0   Date           145460 non-null  object \n 1   Location       145460 non-null  object \n 2   MinTemp        143975 non-null  float64\n 3   MaxTemp        144199 non-null  float64\n 4   Rainfall       142199 non-null  float64\n 5   Evaporation    82670 non-null   float64\n 6   Sunshine       75625 non-null   float64\n 7   WindGustDir    135134 non-null  object \n 8   WindGustSpeed  135197 non-null  float64\n 9   WindDir9am     134894 non-null  object \n 10  WindDir3pm     141232 non-null  object \n 11  WindSpeed9am   143693 non-null  float64\n 12  WindSpeed3pm   142398 non-null  float64\n 13  Humidity9am    142806 non-null  float64\n 14  Humidity3pm    140953 non-null  float64\n 15  Pressure9am    130395 non-null  float64\n 16  Pressure3pm    130432 non-null  float64\n 17  Cloud9am       89572 non-null   float64\n 18  Cloud3pm       86102 non-null   float64\n 19  Temp9am        143693 non-null  float64\n 20  Temp3pm        141851 non-null  float64\n 21  RainToday      142199 non-null  object \n 22  RainTomorrow   142193 non-null  object \ndtypes: float64(16), object(7)\nmemory usage: 25.5+ MB\n\n\nIf we wanted to get a better sense of the null values in each column, we could calculate the percentage of null values by capturing whether each row of each column is null (pd.DataFrame.isnull()), summing the total null values in each column (pd.DataFrame.sum()), and then dividing by the length of the dataframe (/len()).\n\n# calculate the percentage of null values in each column\ndf.isnull().sum()/len(df)\n\nDate             0.000000\nLocation         0.000000\nMinTemp          0.010209\nMaxTemp          0.008669\nRainfall         0.022419\nEvaporation      0.431665\nSunshine         0.480098\nWindGustDir      0.070989\nWindGustSpeed    0.070555\nWindDir9am       0.072639\nWindDir3pm       0.029066\nWindSpeed9am     0.012148\nWindSpeed3pm     0.021050\nHumidity9am      0.018246\nHumidity3pm      0.030984\nPressure9am      0.103568\nPressure3pm      0.103314\nCloud9am         0.384216\nCloud3pm         0.408071\nTemp9am          0.012148\nTemp3pm          0.024811\nRainToday        0.022419\nRainTomorrow     0.022460\ndtype: float64\n\n\nIf we want a quick summary of all the numeric columns in the dataset, we can use pd.DataFrame.describe().\n\n# quick summary of numeric variables\ndf.describe()\n\n\n\n\n\n\n\n\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustSpeed\nWindSpeed9am\nWindSpeed3pm\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\n\n\n\n\ncount\n143975.000000\n144199.000000\n142199.000000\n82670.000000\n75625.000000\n135197.000000\n143693.000000\n142398.000000\n142806.000000\n140953.000000\n130395.00000\n130432.000000\n89572.000000\n86102.000000\n143693.000000\n141851.00000\n\n\nmean\n12.194034\n23.221348\n2.360918\n5.468232\n7.611178\n40.035230\n14.043426\n18.662657\n68.880831\n51.539116\n1017.64994\n1015.255889\n4.447461\n4.509930\n16.990631\n21.68339\n\n\nstd\n6.398495\n7.119049\n8.478060\n4.193704\n3.785483\n13.607062\n8.915375\n8.809800\n19.029164\n20.795902\n7.10653\n7.037414\n2.887159\n2.720357\n6.488753\n6.93665\n\n\nmin\n-8.500000\n-4.800000\n0.000000\n0.000000\n0.000000\n6.000000\n0.000000\n0.000000\n0.000000\n0.000000\n980.50000\n977.100000\n0.000000\n0.000000\n-7.200000\n-5.40000\n\n\n25%\n7.600000\n17.900000\n0.000000\n2.600000\n4.800000\n31.000000\n7.000000\n13.000000\n57.000000\n37.000000\n1012.90000\n1010.400000\n1.000000\n2.000000\n12.300000\n16.60000\n\n\n50%\n12.000000\n22.600000\n0.000000\n4.800000\n8.400000\n39.000000\n13.000000\n19.000000\n70.000000\n52.000000\n1017.60000\n1015.200000\n5.000000\n5.000000\n16.700000\n21.10000\n\n\n75%\n16.900000\n28.200000\n0.800000\n7.400000\n10.600000\n48.000000\n19.000000\n24.000000\n83.000000\n66.000000\n1022.40000\n1020.000000\n7.000000\n7.000000\n21.600000\n26.40000\n\n\nmax\n33.900000\n48.100000\n371.000000\n145.000000\n14.500000\n135.000000\n130.000000\n87.000000\n100.000000\n100.000000\n1041.00000\n1039.600000\n9.000000\n9.000000\n40.200000\n46.70000\n\n\n\n\n\n\n\nHowever, I prefer to bring in another package, skimpy, that does all of this very quickly and cleanly. We can get a detailed description of the entire dataset using skim().\n\n# a more informative summary function from the skimpy package\nskim(df)\n\n╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n│          Data Summary                Data Types                                                                 │\n│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                          │\n│ ┃ Dataframe         ┃ Values ┃ ┃ Column Type ┃ Count ┃                                                          │\n│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                          │\n│ │ Number of rows    │ 145460 │ │ float64     │ 16    │                                                          │\n│ │ Number of columns │ 23     │ │ string      │ 7     │                                                          │\n│ └───────────────────┴────────┘ └─────────────┴───────┘                                                          │\n│                                                     number                                                      │\n│ ┏━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━┳━━━━━━━━┓  │\n│ ┃ column         ┃ NA     ┃ NA %                ┃ mean  ┃ sd    ┃ p0    ┃ p25  ┃ p50  ┃ p75  ┃ p100 ┃ hist   ┃  │\n│ ┡━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━╇━━━━━━━━┩  │\n│ │ MinTemp        │   1485 │  1.0208992162793895 │ 12.19 │ 6.398 │  -8.5 │  7.6 │   12 │ 16.9 │ 33.9 │  ▃▇▇▃  │  │\n│ │ MaxTemp        │   1261 │  0.8669049910628351 │ 23.22 │ 7.119 │  -4.8 │ 17.9 │ 22.6 │ 28.2 │ 48.1 │  ▁▇▇▃  │  │\n│ │ Rainfall       │   3261 │   2.241853430496356 │ 2.361 │ 8.478 │     0 │    0 │    0 │  0.8 │  371 │   ▇    │  │\n│ │ Evaporation    │  62790 │    43.1665062560154 │ 5.468 │ 4.194 │     0 │  2.6 │  4.8 │  7.4 │  145 │   ▇    │  │\n│ │ Sunshine       │  69835 │   48.00976213391998 │ 7.611 │ 3.785 │     0 │  4.8 │  8.4 │ 10.6 │ 14.5 │ ▃▃▅▆▇▃ │  │\n│ │ WindGustSpeed  │  10263 │   7.055547916953114 │ 40.04 │ 13.61 │     6 │   31 │   39 │   48 │  135 │  ▂▇▂   │  │\n│ │ WindSpeed9am   │   1767 │   1.214766946239516 │ 14.04 │ 8.915 │     0 │    7 │   13 │   19 │  130 │   ▇▂   │  │\n│ │ WindSpeed3pm   │   3062 │   2.105046060772721 │ 18.66 │  8.81 │     0 │   13 │   19 │   24 │   87 │  ▅▇▂   │  │\n│ │ Humidity9am    │   2654 │  1.8245565791282827 │ 68.88 │ 19.03 │     0 │   57 │   70 │   83 │  100 │  ▁▂▇▇▆ │  │\n│ │ Humidity3pm    │   4507 │    3.09844630826344 │ 51.54 │  20.8 │     0 │   37 │   52 │   66 │  100 │ ▁▅▆▇▅▂ │  │\n│ │ Pressure9am    │  15065 │     10.356799120033 │  1018 │ 7.107 │ 980.5 │ 1013 │ 1018 │ 1022 │ 1041 │   ▂▇▅  │  │\n│ │ Pressure3pm    │  15028 │  10.331362573903478 │  1015 │ 7.037 │ 977.1 │ 1010 │ 1015 │ 1020 │ 1040 │   ▂▇▅  │  │\n│ │ Cloud9am       │  55888 │   38.42155919153032 │ 4.447 │ 2.887 │     0 │    1 │    5 │    7 │    9 │ ▇▂▃▂▇▅ │  │\n│ │ Cloud3pm       │  59358 │   40.80709473394748 │  4.51 │  2.72 │     0 │    2 │    5 │    7 │    9 │ ▆▂▃▂▇▃ │  │\n│ │ Temp9am        │   1767 │   1.214766946239516 │ 16.99 │ 6.489 │  -7.2 │ 12.3 │ 16.7 │ 21.6 │ 40.2 │  ▂▇▇▃  │  │\n│ │ Temp3pm        │   3609 │  2.4810944589577892 │ 21.68 │ 6.937 │  -5.4 │ 16.6 │ 21.1 │ 26.4 │ 46.7 │  ▁▇▇▃  │  │\n│ └────────────────┴────────┴─────────────────────┴───────┴───────┴───────┴──────┴──────┴──────┴──────┴────────┘  │\n│                                                     string                                                      │\n│ ┏━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┓  │\n│ ┃          ┃       ┃          ┃          ┃          ┃          ┃          ┃ chars per ┃ words    ┃ total     ┃  │\n│ ┃ column   ┃ NA    ┃ NA %     ┃ shortest ┃ longest  ┃ min      ┃ max      ┃ row       ┃ per row  ┃ words     ┃  │\n│ ┡━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━┩  │\n│ │ Date     │     0 │        0 │ 2008-12- │ 2008-12- │ 2007-11- │ 2017-06- │        10 │        1 │    145460 │  │\n│ │          │       │          │ 01       │ 01       │ 01       │ 25       │           │          │           │  │\n│ │ Location │     0 │        0 │ Sale     │ Melbourn │ Adelaide │ Woomera  │      8.71 │        1 │    145460 │  │\n│ │          │       │          │          │ eAirport │          │          │           │          │           │  │\n│ │ WindGust │ 10326 │ 7.098858 │ W        │ WNW      │ E        │ WSW      │      2.19 │     0.93 │    135134 │  │\n│ │ Dir      │       │ 79279527 │          │          │          │          │           │          │           │  │\n│ │ WindDir9 │ 10566 │ 7.263852 │ W        │ NNW      │ E        │ WSW      │      2.18 │     0.93 │    134894 │  │\n│ │ am       │       │ 60552729 │          │          │          │          │           │          │           │  │\n│ │          │       │        2 │          │          │          │          │           │          │           │  │\n│ │ WindDir3 │  4228 │ 2.906641 │ E        │ WNW      │ E        │ WSW      │      2.21 │     0.97 │    141232 │  │\n│ │ pm       │       │ 00096246 │          │          │          │          │           │          │           │  │\n│ │          │       │        4 │          │          │          │          │           │          │           │  │\n│ │ RainToda │  3261 │ 2.241853 │ No       │ Yes      │ No       │ Yes      │      2.22 │     0.98 │    142199 │  │\n│ │ y        │       │ 43049635 │          │          │          │          │           │          │           │  │\n│ │          │       │        6 │          │          │          │          │           │          │           │  │\n│ │ RainTomo │  3267 │ 2.245978 │ No       │ Yes      │ No       │ Yes      │      2.22 │     0.98 │    142193 │  │\n│ │ rrow     │       │ 27581465 │          │          │          │          │           │          │           │  │\n│ │          │       │        7 │          │          │          │          │           │          │           │  │\n│ └──────────┴───────┴──────────┴──────────┴──────────┴──────────┴──────────┴───────────┴──────────┴───────────┘  │\n╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯",
    "crumbs": [
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#exploring-variables-columns-observations-rows",
    "href": "sessions/03-eda-pandas/index.html#exploring-variables-columns-observations-rows",
    "title": "Exploring Data Using Pandas",
    "section": "Exploring Variables (Columns) & Observations (Rows)",
    "text": "Exploring Variables (Columns) & Observations (Rows)\nIf we are going to narrow our focus to specific variables or groups of observations, we need to know how to select columns, filter values, and group the data. There are lots of different ways we can slice up the data. We won’t cover all of them here2, but we will try to cover a range that helps illustrate how pandas works and will help you build the intuition for working with data in pandas.\nWe can select columns in a variety of ways, but the “correct” way to select columns in most circumstances is using selection brackets (the square brackets []), also known as the indexing operator.\n\n# selecting a single column by name\ndf['Date']\n\n# alternative ways to select columns\n# df.loc[:, 'Date']\n# df.Date\n\n0         2008-12-01\n1         2008-12-02\n2         2008-12-03\n3         2008-12-04\n4         2008-12-05\n             ...    \n145455    2017-06-21\n145456    2017-06-22\n145457    2017-06-23\n145458    2017-06-24\n145459    2017-06-25\nName: Date, Length: 145460, dtype: object\n\n\nIf we want to select multiple columns, we can use double squared brackets ([[ ]]). This is the same process as before, but the inner brackets define a list, and the outer are the selection brackets. Lists are an example of a data (variable) type - for more detail on data types, refer to session 6.\n\n# selecting multiple columns (and all rows) by name\ndf[['Date', 'Location', 'Rainfall']]\n# df.loc[:, ['Date', 'Location', 'Rainfall']]\n\n\n\n\n\n\n\n\nDate\nLocation\nRainfall\n\n\n\n\n0\n2008-12-01\nAlbury\n0.6\n\n\n1\n2008-12-02\nAlbury\n0.0\n\n\n2\n2008-12-03\nAlbury\n0.0\n\n\n3\n2008-12-04\nAlbury\n0.0\n\n\n4\n2008-12-05\nAlbury\n1.0\n\n\n...\n...\n...\n...\n\n\n145455\n2017-06-21\nUluru\n0.0\n\n\n145456\n2017-06-22\nUluru\n0.0\n\n\n145457\n2017-06-23\nUluru\n0.0\n\n\n145458\n2017-06-24\nUluru\n0.0\n\n\n145459\n2017-06-25\nUluru\n0.0\n\n\n\n\n145460 rows × 3 columns\n\n\n\nWhile selection brackets are a quick and easy solution if we want to grab a subset of variables in the dataset, it is realy only intended to be used for simple operations using only column selection.\nFor row selection, we should use pd.DataFrame.iloc[]. The iloc function is used for “integer position” selection, which means you can select rows or columns using their integer position. For rows 10-15, you can select them using the following:\n\n# slicing by rows\ndf.iloc[10:16]\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n10\n2008-12-11\nAlbury\n13.4\n30.4\n0.0\nNaN\nNaN\nN\n30.0\nSSE\n...\n48.0\n22.0\n1011.8\n1008.7\nNaN\nNaN\n20.4\n28.8\nNo\nYes\n\n\n11\n2008-12-12\nAlbury\n15.9\n21.7\n2.2\nNaN\nNaN\nNNE\n31.0\nNE\n...\n89.0\n91.0\n1010.5\n1004.2\n8.0\n8.0\n15.9\n17.0\nYes\nYes\n\n\n12\n2008-12-13\nAlbury\n15.9\n18.6\n15.6\nNaN\nNaN\nW\n61.0\nNNW\n...\n76.0\n93.0\n994.3\n993.0\n8.0\n8.0\n17.4\n15.8\nYes\nYes\n\n\n13\n2008-12-14\nAlbury\n12.6\n21.0\n3.6\nNaN\nNaN\nSW\n44.0\nW\n...\n65.0\n43.0\n1001.2\n1001.8\nNaN\n7.0\n15.8\n19.8\nYes\nNo\n\n\n14\n2008-12-15\nAlbury\n8.4\n24.6\n0.0\nNaN\nNaN\nNaN\nNaN\nS\n...\n57.0\n32.0\n1009.7\n1008.7\nNaN\nNaN\n15.9\n23.5\nNo\nNaN\n\n\n15\n2008-12-16\nAlbury\n9.8\n27.7\nNaN\nNaN\nNaN\nWNW\n50.0\nNaN\n...\n50.0\n28.0\n1013.4\n1010.3\n0.0\nNaN\n17.3\n26.2\nNaN\nNo\n\n\n\n\n6 rows × 23 columns\n\n\n\nWe can do similar using a column’s integer position, but we have to select all rows (:) first:\n\n# using iloc with columns\ndf.iloc[:, 20]\n\n0         21.8\n1         24.3\n2         23.2\n3         26.5\n4         29.7\n          ... \n145455    22.4\n145456    24.5\n145457    26.1\n145458    26.0\n145459    20.9\nName: Temp3pm, Length: 145460, dtype: float64\n\n\nFinally, we can put both together to take a subset of both rows and columns:\n\n# using iloc with rows and columns\ndf.iloc[10:16, 20]\n\n10    28.8\n11    17.0\n12    15.8\n13    19.8\n14    23.5\n15    26.2\nName: Temp3pm, dtype: float64\n\n\nHowever, selecting by integer position is relatively limited. It is more likely we would want to subset the data based on the values of certain columns. We can filter rows by condition using pd.DataFrame.loc[]. The loc function slices by label, instead of integer position.\nFor example, we might want to look at a subset of the data based on location.\n\n# select all observations in Perth\ndf.loc[df['Location'] == 'Perth'] # we use double equals to compare the value in Location to the string Perth \n                                  # (so the effect is to output the rows that match that condition.)\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n120638\n2008-07-01\nPerth\n2.7\n18.8\n0.0\n0.8\n9.1\nENE\n20.0\nNaN\n...\n97.0\n53.0\n1027.6\n1024.5\n2.0\n3.0\n8.5\n18.1\nNo\nNo\n\n\n120639\n2008-07-02\nPerth\n6.4\n20.7\n0.0\n1.8\n7.0\nNE\n22.0\nESE\n...\n80.0\n39.0\n1024.1\n1019.0\n0.0\n6.0\n11.1\n19.7\nNo\nNo\n\n\n120640\n2008-07-03\nPerth\n6.5\n19.9\n0.4\n2.2\n7.3\nNE\n31.0\nNaN\n...\n84.0\n71.0\n1016.8\n1015.6\n1.0\n3.0\n12.1\n17.7\nNo\nYes\n\n\n120641\n2008-07-04\nPerth\n9.5\n19.2\n1.8\n1.2\n4.7\nW\n26.0\nNNE\n...\n93.0\n73.0\n1019.3\n1018.4\n6.0\n6.0\n13.2\n17.7\nYes\nYes\n\n\n120642\n2008-07-05\nPerth\n9.5\n16.4\n1.8\n1.4\n4.9\nWSW\n44.0\nW\n...\n69.0\n57.0\n1020.4\n1022.1\n7.0\n5.0\n15.9\n16.0\nYes\nYes\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n123826\n2017-06-21\nPerth\n10.3\n19.9\n0.2\n1.8\n7.5\nNW\n37.0\nNNE\n...\n89.0\n60.0\n1017.1\n1013.8\n5.0\n6.0\n13.0\n18.5\nNo\nYes\n\n\n123827\n2017-06-22\nPerth\n13.0\n16.8\n61.2\n3.6\n0.0\nSSW\n46.0\nW\n...\n90.0\n75.0\n1005.6\n1008.9\n7.0\n7.0\n16.4\n15.6\nYes\nNo\n\n\n123828\n2017-06-23\nPerth\n13.3\n18.9\n0.4\n1.8\n6.5\nSE\n37.0\nSE\n...\n85.0\n65.0\n1019.2\n1019.4\n6.0\n6.0\n15.1\n18.0\nNo\nNo\n\n\n123829\n2017-06-24\nPerth\n11.5\n18.2\n0.0\n3.8\n9.3\nSE\n30.0\nESE\n...\n62.0\n47.0\n1025.9\n1023.4\n1.0\n3.0\n14.0\n17.6\nNo\nNo\n\n\n123830\n2017-06-25\nPerth\n6.3\n17.0\n0.0\n1.6\n7.9\nE\n26.0\nSE\n...\n75.0\n49.0\n1028.6\n1026.0\n1.0\n3.0\n11.5\n15.6\nNo\nNo\n\n\n\n\n3193 rows × 23 columns\n\n\n\nWe can also filter by multiple values, such as location and rainfall.\n\ndf.loc[(df['Rainfall'] == 0) & (df['Location'] == 'Perth')]\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n120638\n2008-07-01\nPerth\n2.7\n18.8\n0.0\n0.8\n9.1\nENE\n20.0\nNaN\n...\n97.0\n53.0\n1027.6\n1024.5\n2.0\n3.0\n8.5\n18.1\nNo\nNo\n\n\n120639\n2008-07-02\nPerth\n6.4\n20.7\n0.0\n1.8\n7.0\nNE\n22.0\nESE\n...\n80.0\n39.0\n1024.1\n1019.0\n0.0\n6.0\n11.1\n19.7\nNo\nNo\n\n\n120644\n2008-07-07\nPerth\n0.7\n18.3\n0.0\n0.8\n9.3\nN\n37.0\nNE\n...\n72.0\n36.0\n1028.9\n1024.2\n1.0\n5.0\n8.7\n17.9\nNo\nNo\n\n\n120645\n2008-07-08\nPerth\n3.2\n20.4\n0.0\n1.4\n6.9\nNNW\n24.0\nNE\n...\n58.0\n42.0\n1023.9\n1021.1\n6.0\n5.0\n10.2\n19.3\nNo\nYes\n\n\n120651\n2008-07-14\nPerth\n7.9\n19.7\n0.0\n0.2\n6.5\nNE\n31.0\nNE\n...\n86.0\n41.0\n1026.0\n1021.9\n6.0\n5.0\n11.7\n18.7\nNo\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n123823\n2017-06-18\nPerth\n7.5\n23.4\n0.0\n1.8\n9.2\nNNE\n28.0\nENE\n...\n67.0\n41.0\n1026.9\n1022.9\n0.0\n0.0\n14.2\n22.2\nNo\nNo\n\n\n123824\n2017-06-19\nPerth\n5.5\n23.0\n0.0\n3.0\n9.1\nSW\n19.0\nENE\n...\n84.0\n55.0\n1023.0\n1020.3\n1.0\n2.0\n11.5\n22.0\nNo\nNo\n\n\n123825\n2017-06-20\nPerth\n7.8\n22.5\n0.0\n2.8\n9.1\nNW\n26.0\nW\n...\n98.0\n59.0\n1019.3\n1015.9\n1.0\n1.0\n13.5\n21.6\nNo\nNo\n\n\n123829\n2017-06-24\nPerth\n11.5\n18.2\n0.0\n3.8\n9.3\nSE\n30.0\nESE\n...\n62.0\n47.0\n1025.9\n1023.4\n1.0\n3.0\n14.0\n17.6\nNo\nNo\n\n\n123830\n2017-06-25\nPerth\n6.3\n17.0\n0.0\n1.6\n7.9\nE\n26.0\nSE\n...\n75.0\n49.0\n1028.6\n1026.0\n1.0\n3.0\n11.5\n15.6\nNo\nNo\n\n\n\n\n2293 rows × 23 columns\n\n\n\nFor any complex process for subsetting the data, including multiple conditions, pd.DataFrame.loc[] is the best bet.\n\nSummarising Data\nNow that we know how to select the variables or observations we are interested in, we can start doing some descriptive analysis. The operations we use will depend on the questions we are trying to answer, and the possibilities will be almost endless.\nQuestions:\n\nWhat “functions” might we need to carry out on our data when we are exploring it?\n\nWe know that the weather data includes observations from all over the country, but we might want to check exactly how many different locations there are. We can use pd.DataFrame.nunique() to do this.\n\n# count unique values\ndf['Location'].nunique()\n\n49\n\n\nWe may also be interested in the locations themselves, which may tell us more about the spatial distribution of our data. In this case, we can use pd.DataFrame.unique().\n\n# get unique values\ndf['Location'].unique()\n\narray(['Albury', 'BadgerysCreek', 'Cobar', 'CoffsHarbour', 'Moree',\n       'Newcastle', 'NorahHead', 'NorfolkIsland', 'Penrith', 'Richmond',\n       'Sydney', 'SydneyAirport', 'WaggaWagga', 'Williamtown',\n       'Wollongong', 'Canberra', 'Tuggeranong', 'MountGinini', 'Ballarat',\n       'Bendigo', 'Sale', 'MelbourneAirport', 'Melbourne', 'Mildura',\n       'Nhil', 'Portland', 'Watsonia', 'Dartmoor', 'Brisbane', 'Cairns',\n       'GoldCoast', 'Townsville', 'Adelaide', 'MountGambier', 'Nuriootpa',\n       'Woomera', 'Albany', 'Witchcliffe', 'PearceRAAF', 'PerthAirport',\n       'Perth', 'SalmonGums', 'Walpole', 'Hobart', 'Launceston',\n       'AliceSprings', 'Darwin', 'Katherine', 'Uluru'], dtype=object)\n\n\nAnother common operation we might look to do is calculating the mean value (pd.DataFrame.mean()) of a certain variable. What is the average value of sunshine across the entire dataset?\n\n# calculate variable mean\ndf['Sunshine'].mean()\n\nnp.float64(7.611177520661157)\n\n\nThis gives us the mean to many decimal places, and we probably don’t need to know the average sunshine hours to this level of precision. We can use the pd.DataFrame.round() function to round to two decimal places.\n\n# round mean value\ndf['Sunshine'].mean().round(2)\n\nnp.float64(7.61)\n\n\nMany operations will return the value with information about the object’s type included. The above values are wrapped in np.float64() because pd.DataFrame.mean() uses numpy to calculate the mean value. However, if you want to strip this information out so you only see the value itself, you can use print().\n\n# print mean value\nprint(df['Sunshine'].mean().round(2))\n\n7.61\n\n\nWhile we are often interested in the mean value when we talk about averages, we might want to know the median instead (pd.DataFrame.median()).\n\n# calculate other summary statistics\nprint(df['Sunshine'].median())\n\n8.4\n\n\nAnother common calculation is summing values (pd.DataFrame.sum()). We can use sum() to see the total hours of sunshine in our dataset, and we can use int() to convert this value to an integer (which also means we don’t need to use print()3).\n\n# calculate sum value and return an integer\nint(df['Sunshine'].sum())\n\n575595\n\n\nWe can also apply these summary operations on multiple variables, using the same selection logic as before (using double squared brackets).\n\nprint(df[['Sunshine', 'Rainfall']].mean())\n\nSunshine    7.611178\nRainfall    2.360918\ndtype: float64\n\n\nAnd we can apply multiple functions, using pd.DataFrame.agg().\n\ndf['Sunshine'].agg(['mean', 'median', 'sum']).round(1)\n\nmean           7.6\nmedian         8.4\nsum       575595.3\nName: Sunshine, dtype: float64\n\n\nThe next step when exploring specific variables will often be group-level summaries. The average amount of sunshine across the whole dataset has limited utility, but the average hours of sunshine in each location allows us to compare between locations and start to understand how different variables are related to each other. If we want to do a group-level operation, we have to use pd.DataFrame.groupby().\n\n# calculate group means\ndf.groupby(by='Location')['Sunshine'].mean().round(1)\n\nLocation\nAdelaide            7.7\nAlbany              6.7\nAlbury              NaN\nAliceSprings        9.6\nBadgerysCreek       NaN\nBallarat            NaN\nBendigo             NaN\nBrisbane            8.1\nCairns              7.6\nCanberra            7.4\nCobar               8.7\nCoffsHarbour        7.4\nDartmoor            6.5\nDarwin              8.5\nGoldCoast           NaN\nHobart              6.6\nKatherine           NaN\nLaunceston          NaN\nMelbourne           6.4\nMelbourneAirport    6.4\nMildura             8.5\nMoree               8.9\nMountGambier        6.5\nMountGinini         NaN\nNewcastle           NaN\nNhil                NaN\nNorahHead           NaN\nNorfolkIsland       7.0\nNuriootpa           7.7\nPearceRAAF          8.8\nPenrith             NaN\nPerth               8.8\nPerthAirport        8.8\nPortland            6.5\nRichmond            NaN\nSale                6.7\nSalmonGums          NaN\nSydney              7.2\nSydneyAirport       7.2\nTownsville          8.5\nTuggeranong         NaN\nUluru               NaN\nWaggaWagga          8.2\nWalpole             NaN\nWatsonia            6.4\nWilliamtown         7.2\nWitchcliffe         NaN\nWollongong          NaN\nWoomera             9.0\nName: Sunshine, dtype: float64\n\n\nThe groupby(by='Location') function tells us the grouping variable (location), then we select the variable we want to summarise by location (sunshine), and then we specify the operation (mean).\nThere are multiple locations that return NaN (Not a Number). This indicates that numpy was unable to calculate a mean value for those locations. This is likely to be because all sunshine values for those locations are null.\nWe can check this using pd.DataFrame.count(), which counts the total non-null values (whereas pd.DataFrame.size() counts the total values).\n\n# group by location and count non-null sunshine values\ndf.groupby('Location')['Sunshine'].count()\n\nLocation\nAdelaide            1769\nAlbany              2520\nAlbury                 0\nAliceSprings        2520\nBadgerysCreek          0\nBallarat               0\nBendigo                0\nBrisbane            3144\nCairns              2564\nCanberra            1521\nCobar                550\nCoffsHarbour        1494\nDartmoor            2566\nDarwin              3189\nGoldCoast              0\nHobart              3179\nKatherine              0\nLaunceston             0\nMelbourne           3192\nMelbourneAirport    3008\nMildura             2876\nMoree               2055\nMountGambier        2597\nMountGinini            0\nNewcastle              0\nNhil                   0\nNorahHead              0\nNorfolkIsland       2570\nNuriootpa           2848\nPearceRAAF          3004\nPenrith                0\nPerth               3188\nPerthAirport        3004\nPortland            2566\nRichmond               0\nSale                1818\nSalmonGums             0\nSydney              3328\nSydneyAirport       2993\nTownsville          2617\nTuggeranong            0\nUluru                  0\nWaggaWagga          2575\nWalpole                0\nWatsonia            3008\nWilliamtown         1355\nWitchcliffe            0\nWollongong             0\nWoomera             2007\nName: Sunshine, dtype: int64\n\n\nThe results show that all the locations that return NaN in our group mean calculation have zero non-null values.",
    "crumbs": [
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#transforming-data",
    "href": "sessions/03-eda-pandas/index.html#transforming-data",
    "title": "Exploring Data Using Pandas",
    "section": "Transforming Data",
    "text": "Transforming Data\nDatasets are rarely perfectly clean and tidy. We often need to transform the data before we can get the most out of it.\nQuestions:\n\nWhat sort of transformations would help us get the most out of the analysis of the Australian weather data?\n\nThe first step with any analysis is often converting columns to the correct types. With a longitudinal (time-series) dataset,the date column is a good place to start. We can use pd.DataFrame.dtypes to check the data type, either of a single column (using the selector brackets) or all columns in the dataset.\n\nprint(df.dtypes)\n\nDate              object\nLocation          object\nMinTemp          float64\nMaxTemp          float64\nRainfall         float64\nEvaporation      float64\nSunshine         float64\nWindGustDir       object\nWindGustSpeed    float64\nWindDir9am        object\nWindDir3pm        object\nWindSpeed9am     float64\nWindSpeed3pm     float64\nHumidity9am      float64\nHumidity3pm      float64\nPressure9am      float64\nPressure3pm      float64\nCloud9am         float64\nCloud3pm         float64\nTemp9am          float64\nTemp3pm          float64\nRainToday         object\nRainTomorrow      object\ndtype: object\n\n\nAll columns are either stored as object or float64. The object data type is for generic non-numeric data, but from the columns that are stored as objects, we can tell this is mostly categorical variables where the categories are represented as text. The float64 data type refers to data that is numeric and includes decimals (float64 = 64-bit floating point number).\nThe date column is stored as an object, but pandas can store dates as datetime64. We can convert dates using pd.to_datetime(). When transforming data, if we want to keep those transformations, we have to store those changes, using =. In this case, we want to convert the date column but we don’t want to create an entirely new dataframe to handle this change, so we can overwrite the current date column by using the selection brackets to identify the column we want to apply this change to.\n\n# convert date column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\nThe remaining object columns can be converted to categorical, which makes them easier to work with in subsequent analyses. We can use pd.DataFrame.astype() to convert column data types.\n\n# create a list of all object columns\nobject_cols = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\n\n# convert object columns to category\ndf[object_cols] = df[object_cols].astype('category')\n\nA more efficient, though synactically more complex, way of doing this is using lamda functions. We won’t cover lambda functons in this session (they will be discussed in detail in a future session), but below is how we can use them to convert objects to categories.\n\n# convert object columns to category data type\ndf = df.apply(lambda x: x.astype('category') if x.dtype == 'object' else x)\n\nAnother choice we might make is to remove missing values, using pd.DataFrame.dropna() to filter the null values and keep only the non-null values. We can use this to drop all null values across the entire dataset, or we can apply it to a subset of columns, using the subset argument.\n\n# filter observations where sunshine is NA\ndf.dropna(subset='Sunshine')\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n6049\n2009-01-01\nCobar\n17.9\n35.2\n0.0\n12.0\n12.3\nSSW\n48.0\nENE\n...\n20.0\n13.0\n1006.3\n1004.4\n2.0\n5.0\n26.6\n33.4\nNo\nNo\n\n\n6050\n2009-01-02\nCobar\n18.4\n28.9\n0.0\n14.8\n13.0\nS\n37.0\nSSE\n...\n30.0\n8.0\n1012.9\n1012.1\n1.0\n1.0\n20.3\n27.0\nNo\nNo\n\n\n6051\n2009-01-03\nCobar\n15.5\n34.1\n0.0\n12.6\n13.3\nSE\n30.0\nNaN\n...\nNaN\n7.0\nNaN\n1011.6\nNaN\n1.0\nNaN\n32.7\nNo\nNo\n\n\n6052\n2009-01-04\nCobar\n19.4\n37.6\n0.0\n10.8\n10.6\nNNE\n46.0\nNNE\n...\n42.0\n22.0\n1012.3\n1009.2\n1.0\n6.0\n28.7\n34.9\nNo\nNo\n\n\n6053\n2009-01-05\nCobar\n21.9\n38.4\n0.0\n11.4\n12.2\nWNW\n31.0\nWNW\n...\n37.0\n22.0\n1012.7\n1009.1\n1.0\n5.0\n29.1\n35.6\nNo\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n142298\n2017-06-20\nDarwin\n19.3\n33.4\n0.0\n6.0\n11.0\nENE\n35.0\nSE\n...\n63.0\n32.0\n1013.9\n1010.5\n0.0\n1.0\n24.5\n32.3\nNo\nNo\n\n\n142299\n2017-06-21\nDarwin\n21.2\n32.6\n0.0\n7.6\n8.6\nE\n37.0\nSE\n...\n56.0\n28.0\n1014.6\n1011.2\n7.0\n0.0\n24.8\n32.0\nNo\nNo\n\n\n142300\n2017-06-22\nDarwin\n20.7\n32.8\n0.0\n5.6\n11.0\nE\n33.0\nE\n...\n46.0\n23.0\n1015.3\n1011.8\n0.0\n0.0\n24.8\n32.1\nNo\nNo\n\n\n142301\n2017-06-23\nDarwin\n19.5\n31.8\n0.0\n6.2\n10.6\nESE\n26.0\nSE\n...\n62.0\n58.0\n1014.9\n1010.7\n1.0\n1.0\n24.8\n29.2\nNo\nNo\n\n\n142302\n2017-06-24\nDarwin\n20.2\n31.7\n0.0\n5.6\n10.7\nENE\n30.0\nENE\n...\n73.0\n32.0\n1013.9\n1009.7\n6.0\n5.0\n25.4\n31.0\nNo\nNo\n\n\n\n\n75625 rows × 23 columns\n\n\n\nWe haven’t stored this transformation, because filtering nulls without careful consideration is a bad idea, but it’s useful to know, nonetheless.\nThere are lots of ways we could transform the data, but the final example we will consider here is reshaping the data using pd.DataFrame.pivot(), which transforms the data from long to wide format data, and pd.DataFrame.melt(), which transforms it from wide to long format.\nPerhaps we want to focus on the maximum temperature per day in each location in 2015. We can use pd.Series.dt.year to get the year from the date column, and filter for the year 2015, before reshaping the data.\n\ndf2015 = df.loc[df['Date'].dt.year == 2015]\ndf_wide = df2015.pivot(index='Date', columns='Location', values='MaxTemp')\n\ndf_wide.head()\n\n\n\n\n\n\n\nLocation\nAdelaide\nAlbany\nAlbury\nAliceSprings\nBadgerysCreek\nBallarat\nBendigo\nBrisbane\nCairns\nCanberra\n...\nTownsville\nTuggeranong\nUluru\nWaggaWagga\nWalpole\nWatsonia\nWilliamtown\nWitchcliffe\nWollongong\nWoomera\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2015-01-01\n37.0\n21.9\n33.5\n40.3\n34.7\n27.4\n31.2\n31.3\n33.7\n32.6\n...\n32.6\n32.1\n42.0\n35.2\n23.6\n28.3\n33.7\n25.0\n25.3\n39.2\n\n\n2015-01-02\n44.1\n21.2\n39.6\n41.4\n30.5\n38.2\n39.8\n30.5\n33.7\n35.2\n...\n33.0\n34.1\n42.4\n38.9\n21.1\n40.6\n29.3\n23.6\n24.6\n43.3\n\n\n2015-01-03\n38.2\n21.5\n38.3\n36.4\n34.3\n37.5\n40.3\n28.9\n33.6\n34.7\n...\n28.1\n33.7\n39.8\n37.5\n21.8\n39.5\n32.8\n23.0\n25.7\n44.7\n\n\n2015-01-04\n30.5\n23.3\n33.1\n29.0\n34.8\n23.5\n29.0\n30.2\n29.4\n32.5\n...\n31.6\n32.8\n36.1\n33.8\n24.4\n25.1\n34.5\n29.8\n25.3\n37.6\n\n\n2015-01-05\n34.9\n24.9\n35.2\n27.1\n27.2\n26.6\n33.6\n28.1\n31.4\n29.6\n...\n31.6\n28.9\n38.8\n34.9\n29.5\n25.7\n27.0\n31.7\n23.1\n38.3\n\n\n\n\n5 rows × 49 columns\n\n\n\nPerhaps we want to look at the maximum and minimum temperatures in each location, together. We can reshape the data to support this4.\n\ndf_long = df2015.melt(\n    id_vars=['Date', 'Location'],\n    value_vars=['MaxTemp', 'MinTemp'],\n    var_name='Variable',\n    value_name='Value'\n)\n\ndf_long.head()\n\n\n\n\n\n\n\n\nDate\nLocation\nVariable\nValue\n\n\n\n\n0\n2015-01-01\nAlbury\nMaxTemp\n33.5\n\n\n1\n2015-01-02\nAlbury\nMaxTemp\n39.6\n\n\n2\n2015-01-03\nAlbury\nMaxTemp\n38.3\n\n\n3\n2015-01-04\nAlbury\nMaxTemp\n33.1\n\n\n4\n2015-01-05\nAlbury\nMaxTemp\n35.2",
    "crumbs": [
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#exercises",
    "href": "sessions/03-eda-pandas/index.html#exercises",
    "title": "Exploring Data Using Pandas",
    "section": "Exercises",
    "text": "Exercises\nSome of these questions are easily answered by scrolling up and finding the answer in the output of the above code, however, the goal is to find the answer using code. No one actually cares what the answer to any of these questions is, it’s the process that matters!\nRemember, if you don’t know the answer, it’s okay to Google it (or speak to others, including me, for help)!\n\n\nImport Data (to Reset)\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')\n\n\n\nWhat is the ‘Sunshine’ column’s data type?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n# What is the 'Sunshine' column's data type?\nprint(df['Sunshine'].dtypes)\n\nfloat64\n\n\n\n\n\n\nIdentify all the columns that are of dtype ‘object’.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n# Identify all the columns that are of dtype 'object'\nprint(list(df.select_dtypes(include=['object'])))\n\n['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\n\n\n\n\n\n\nHow many of the dataframe’s columns are of dtype ‘object’?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n# How many of the dataframe's columns are of dtype 'object'?\nlen(list(df.select_dtypes(include=['object'])))\n\n7\n\n\n\n\n\n\nHow many of the ‘Rainfall’ column values are NAs?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n# How many of the 'Rainfall' column values are NAs?\nprint(df['Rainfall'].isna().sum())\n\n3261\n\n\n\n\n\n\nCreate a new dataframe which only includes the ‘Date’, ‘Location, ’Sunshine’, ‘Rainfall’, and ‘RainTomorrow’ columns.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nnew_df = df[['Date', 'Location', 'Sunshine', 'Rainfall', 'RainTomorrow']]\nnew_df.head()\n\n\n\n\n\n\n\n\nDate\nLocation\nSunshine\nRainfall\nRainTomorrow\n\n\n\n\n0\n2008-12-01\nAlbury\nNaN\n0.6\nNo\n\n\n1\n2008-12-02\nAlbury\nNaN\n0.0\nNo\n\n\n2\n2008-12-03\nAlbury\nNaN\n0.0\nNo\n\n\n3\n2008-12-04\nAlbury\nNaN\n0.0\nNo\n\n\n4\n2008-12-05\nAlbury\nNaN\n1.0\nNo\n\n\n\n\n\n\n\n\n\n\n\nConvert ‘RainTomorrow’ to a numeric variable, where ‘Yes’ = 1 and ‘No’ = 0.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n# df['Location'].astype('category').cat.codes\n# df['RainTomorrow'].astype('category').cat.codes\ndf['RainTomorrow'].map({'Yes': 1, 'No': 0})\n\n0         0.0\n1         0.0\n2         0.0\n3         0.0\n4         0.0\n         ... \n145455    0.0\n145456    0.0\n145457    0.0\n145458    0.0\n145459    NaN\nName: RainTomorrow, Length: 145460, dtype: float64\n\n\n\n\n\n\nWhat is the average amount of rainfall for each location?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n# average rainfall by location, sorted by value\ndf.groupby('Location')['Rainfall'].mean().sort_values(ascending=False)\n\nLocation\nCairns              5.742035\nDarwin              5.092452\nCoffsHarbour        5.061497\nGoldCoast           3.769396\nWollongong          3.594903\nWilliamtown         3.591108\nTownsville          3.485592\nNorahHead           3.387299\nSydney              3.324543\nMountGinini         3.292260\nKatherine           3.201090\nNewcastle           3.183892\nBrisbane            3.144891\nNorfolkIsland       3.127665\nSydneyAirport       3.009917\nWalpole             2.906846\nWitchcliffe         2.895664\nPortland            2.530374\nAlbany              2.263859\nBadgerysCreek       2.193101\nPenrith             2.175304\nTuggeranong         2.164043\nDartmoor            2.146567\nRichmond            2.138462\nMountGambier        2.087562\nLaunceston          2.011988\nAlbury              1.914115\nPerth               1.906295\nMelbourne           1.870062\nWatsonia            1.860820\nPerthAirport        1.761648\nCanberra            1.741720\nBallarat            1.740026\nWaggaWagga          1.709946\nPearceRAAF          1.669080\nMoree               1.630203\nBendigo             1.619380\nHobart              1.601819\nAdelaide            1.566354\nSale                1.510167\nMelbourneAirport    1.451977\nNuriootpa           1.390343\nCobar               1.127309\nSalmonGums          1.034382\nMildura             0.945062\nNhil                0.934863\nAliceSprings        0.882850\nUluru               0.784363\nWoomera             0.490405\nName: Rainfall, dtype: float64\n\n\n\n\n\n\nWhat is the average amount of rainfall for days that it will rain tomorrow?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n# average rainfall depending on whether it will rain tomorrow or not\ndf.groupby('RainTomorrow')['Rainfall'].mean()\n\nRainTomorrow\nNo     1.270290\nYes    6.142104\nName: Rainfall, dtype: float64\n\n\n\n\n\n\nWhat is the average amount of sunshine in Perth when it will not rain tomorrow?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n# average sunshine in Perth when it won't rain tomorrow\ndf.loc[(df['Location'] == 'Perth') & (df['RainTomorrow'] == 'No'), 'Sunshine'].mean()\n# df[(df['Location']=='Perth') & (df['RainTomorrow']=='No')]['Sunshine'].mean()\n\nnp.float64(9.705306603773584)\n\n\n\n\n\n\nWe want to understand the role that time plays in the dataset. Using the original dataframe, carry the following tasks and answer the corresponding questions:\n\nCreate columns representing the year and month from the ‘Date’ column. How many years of data are in the dataset?\nExamine the distribution of the ‘Sunshine’ NAs over time. Is time a component in the ‘Sunshine’ data quality issues?\nCalculate the average rainfall and sunshine by month. How do rainfall and sunshine vary through the year?\nCalculate the average rainfall and sunshine by year. How have rainfall and sunshine changed over time?\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n# get year and month columns\ndf = (\n    df.assign(Date=pd.to_datetime(df['Date']))\n    .assign(\n        Year=lambda x: x['Date'].dt.year,\n        Month=lambda x: x['Date'].dt.month\n    )\n)\n\n# count unique years\ndf['Year'].nunique()\n\n11\n\n\n\n# lambda function counting nulls by year\ndf.groupby('Year')['Sunshine'].apply(lambda x: x.isna().sum())\n\nYear\n2007        0\n2008      323\n2009     6146\n2010     6220\n2011     6053\n2012     6539\n2013     7570\n2014     9157\n2015     9441\n2016    11994\n2017     6392\nName: Sunshine, dtype: int64\n\n\n\n# rainfall and sunshine by month\ndf.groupby('Month')[['Rainfall', 'Sunshine']].mean().round(1)\n\n\n\n\n\n\n\n\nRainfall\nSunshine\n\n\nMonth\n\n\n\n\n\n\n1\n2.7\n9.2\n\n\n2\n3.2\n8.6\n\n\n3\n2.8\n7.6\n\n\n4\n2.3\n7.1\n\n\n5\n2.0\n6.3\n\n\n6\n2.8\n5.6\n\n\n7\n2.2\n6.1\n\n\n8\n2.0\n7.1\n\n\n9\n1.9\n7.7\n\n\n10\n1.6\n8.5\n\n\n11\n2.3\n8.7\n\n\n12\n2.5\n9.0\n\n\n\n\n\n\n\n\n# rainfall and sunshine by year\ndf.groupby('Year')[['Rainfall', 'Sunshine']].mean().round(1)\n\n\n\n\n\n\n\n\nRainfall\nSunshine\n\n\nYear\n\n\n\n\n\n\n2007\n3.2\n8.1\n\n\n2008\n2.3\n7.8\n\n\n2009\n2.2\n7.9\n\n\n2010\n2.7\n7.3\n\n\n2011\n2.8\n7.3\n\n\n2012\n2.4\n7.6\n\n\n2013\n2.3\n7.7\n\n\n2014\n2.0\n7.8\n\n\n2015\n2.2\n7.7\n\n\n2016\n2.4\n7.6\n\n\n2017\n2.5\n7.7",
    "crumbs": [
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#footnotes",
    "href": "sessions/03-eda-pandas/index.html#footnotes",
    "title": "Exploring Data Using Pandas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor more information about pandas data types, check out the pandas documentation on dtypes.↩︎\nFor more information, I’d recommend the pandas documentation, and this pandas tutorial on subsetting data.↩︎\nSome functions should be wrapped in print() in order to return a value that is easy to read, but others won’t. There will be an internal logic for which is which, but it’s not of huge importance to us. You are better off just testing functions out and wrapping them in print() if necessary.↩︎\nThis is often very useful when we need to visualise data, for example plotting the max and min temp for each location, is easier if the values are organised in the same column and differentiated using another column.↩︎",
    "crumbs": [
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#things-that-flow",
    "href": "sessions/07-control-flow/slides.html#things-that-flow",
    "title": "An Introduction to Control Flow",
    "section": "Things That Flow",
    "text": "Things That Flow\n\n\n\n\nA stream flowing towards the sea.\n\n\n\n\n\n\nTraffic flow on a busy road.\n\n\n\n\n\n\nConversation flowing between people."
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#code-flows-too",
    "href": "sessions/07-control-flow/slides.html#code-flows-too",
    "title": "An Introduction to Control Flow",
    "section": "Code Flows Too",
    "text": "Code Flows Too\n\nThe flow of code and the steps that impact that flow are called “control flow”.\nIn general code flows in the way same as a book (with some exceptions).\n\nEach function is completed before considering the next.\n\nFunctions can be nested in other functions - the inner most function is completed first.\nControl structures can be used to change the course of a program.\nRepetition structures can be used to repeat a section of code."
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#sequential-structure",
    "href": "sessions/07-control-flow/slides.html#sequential-structure",
    "title": "An Introduction to Control Flow",
    "section": "Sequential Structure",
    "text": "Sequential Structure\n\nIn general code flows like a book reads:\n\nStatements (like lines of code) run top to bottom line,\nLeft to right in statement,\nEach statement being completed before moving to the next.\n\n\n\nv = \"2\"         #stores \"2\" as a string in 'v'\ni = int(v)      #stores v as an integer 'i'\nt = type(i)     #stores the type of 'i' as 't'\nprint(t)        #prints 't'"
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#nesting-functions-and-operations",
    "href": "sessions/07-control-flow/slides.html#nesting-functions-and-operations",
    "title": "An Introduction to Control Flow",
    "section": "Nesting Functions and Operations",
    "text": "Nesting Functions and Operations\n\nWe are not limited to a single function or operation per row.\nThe previous example could be re-written as:\n\nprint(type(int(\"2\")))\n\nNesting functions can be useful, however care should be taken and it may be easier to separate functions over multiple rows."
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#control-structures",
    "href": "sessions/07-control-flow/slides.html#control-structures",
    "title": "An Introduction to Control Flow",
    "section": "Control Structures",
    "text": "Control Structures\n\nControl structures (also known as decision structures) allow the flow to respond to varying situations.\nA decision is made based on one or more conditions.\nThese control structures are very similar to the IF function in Excel and the CASE statement in SQL (but remember that indentation matters in Python).\n\n\n\n\nPython - Control\nSQL - CASE\nExcel - IF\n\n\n\n\nif x = 2:\nCASE WHEN x &gt; 2\nIF(x &gt; 2,\n\n\ny = 1\nTHEN 1\n1,\n\n\nelse:\nELSE\n\n\n\ny = 0\n0 END\n0)"
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#the-importance-of-being-boolean",
    "href": "sessions/07-control-flow/slides.html#the-importance-of-being-boolean",
    "title": "An Introduction to Control Flow",
    "section": "The Importance of Being Boole(an)",
    "text": "The Importance of Being Boole(an)\n\n\n\n\nGeorge Boole was an English Mathmetician and logician whose work on binary logic has resulted in binary conditions bearing his name\nAny statement that can be evaluated as only either True (1) or False (0) is Boolean."
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#repetition-structures",
    "href": "sessions/07-control-flow/slides.html#repetition-structures",
    "title": "An Introduction to Control Flow",
    "section": "Repetition Structures",
    "text": "Repetition Structures\n\nRepetition structures (commonly referred to as “loops”) allow for us to recycle chunks of code to perform the same or similar operation a specified number of times or until a condition changes.\nFor loops cycle through a series of iterations, until they reach the end performing each series of statements for each iteration.\n\nThis can be used to cycle through a list, dictionary or other iterable as well as working through ranges of numbers\n\nWhile loops continue until a predetermined condition changes from True to False.\n\nThis can be useful for testing conditions but comes with a warning:\n\nMake sure your condition will change at some point or else your loop cannot end."
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#what-is-iterability",
    "href": "sessions/07-control-flow/slides.html#what-is-iterability",
    "title": "An Introduction to Control Flow",
    "section": "What is Iterability?",
    "text": "What is Iterability?\n\nIterability is the ability to split an object into discrete items. The item may be ordered or unordered, each item will be extracted, processed ad set aside.\nIn general if an object can be split into multiple items it can be iterated (integers and floats are not iterable).\nIterable objects include:\n\nStrings (the word “strings” contains 7 iterable items).\nLists eg [1, 2, 3, 4, 4, 4, 5, 6]\nTuples eg (12, ‘Red’, ‘Apples’)\nSets eg {1, 2, 3, 4, 5, 6}\nDict eg {ICB_Code: ‘QSL’, Metric_Code: E.M.10}"
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#sequential-structure-example",
    "href": "sessions/07-control-flow/slides.html#sequential-structure-example",
    "title": "An Introduction to Control Flow",
    "section": "Sequential Structure Example",
    "text": "Sequential Structure Example\n\nThe following sequential code will create a variable called ‘var’ which is a string, it converts this string to an integer and conducts a series of mathematical operators before printing the result:\n\n\nvar = '22' # set \"var\" to '22'\nvar = int(var) # convert \"var\" to int\nvar = var  / 2 - 3 # apply math operators\nprint (var) # print result"
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#control-structure-example",
    "href": "sessions/07-control-flow/slides.html#control-structure-example",
    "title": "An Introduction to Control Flow",
    "section": "Control Structure Example",
    "text": "Control Structure Example\n\nThis code checks if a variable called ‘provider’ in this list is equal to a selection of values and prints out an associated string.\n\n\nif provider == ‘ryr’:\n  print(‘SUSSEX’)\nelif provider == ‘rhu’:\n  print(‘HIOW’)\nelif provider == ‘rxq’:\n  print(‘BOB’)\nelse:\n  print(‘Unknown’)"
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#control-structure-example-1",
    "href": "sessions/07-control-flow/slides.html#control-structure-example-1",
    "title": "An Introduction to Control Flow",
    "section": "Control Structure Example",
    "text": "Control Structure Example\nFor comparison, this is the equivalent SQL CASE statement.\nCASE\n  WHEN provider = 'ryr'\n    THEN 'Sussex'\n  WHEN provider = 'rhu'\n    THEN 'HIOW'\n  WHEN provider = 'rhu'\n    THEN 'BOB'\n  ELSE 'Unknown'\n  END"
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#watch-out-for-unintended-consequences",
    "href": "sessions/07-control-flow/slides.html#watch-out-for-unintended-consequences",
    "title": "An Introduction to Control Flow",
    "section": "Watch Out for Unintended Consequences",
    "text": "Watch Out for Unintended Consequences\n\n\nNot taking care over your coding can cause big issues. Consider the corner cases and unintended consequences?\nEmpty variables, incorrect data types, and misunderstood flow in the structure can affect your program.\nClose the loop! Make sure you know how your loops are being switched off and that it’s possible.\nA cautionary tale: The Virtual Plague That Nearly Wiped Out The World of Warcraft\n\n\n\n\n\nThe “Corrupted Blood” Incident - a fairly famous coding error."
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#final-thoughts",
    "href": "sessions/07-control-flow/slides.html#final-thoughts",
    "title": "An Introduction to Control Flow",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n\nDon’t worry about memorising any of this!\nThe aim of this session is to give a basic understanding of the logic needed to implement control flow in your program."
  },
  {
    "objectID": "sessions/07-control-flow/slides.html#further-reading",
    "href": "sessions/07-control-flow/slides.html#further-reading",
    "title": "An Introduction to Control Flow",
    "section": "Further Reading",
    "text": "Further Reading\n\n\nPython Tutorial - Control Flow\nGeeks for Geeks - Control Structures\nW3 Schools - if/elif/else Logic\nList Comprehension in Python"
  },
  {
    "objectID": "intermediate-skills-sessions/16-git-github/index.html",
    "href": "intermediate-skills-sessions/16-git-github/index.html",
    "title": "Git and Github",
    "section": "",
    "text": "Git is a version control system (or source control system, referring to source code). It’s software that helps you track changes to your files over time. You can sort of think of it like Track Changes in Word, but applied to an entire folder worth of files, much more powerful, and primarily designed to work with plaintext files - such as Python code, for example.\nThe basic concept in Git is a repository, which you can think of as a project that can contain an unlimited number of files and folders.\nUsing Git for a project lets us:\n\nAutomatically create and keep snapshots of your work at different points in time\nClearly see exactly what changed between versions\nGo back to earlier versions if something goes wrong\nCreate branches that let individuals or subgroups work on parts of a project without affecting the rest of it\nUnderstand who made which changes and when\nRequest reviews from our colleagues, add comments to changes, and resolve issues before committing to an update\nAvoid the classic issue of people working on their own copies of files, and having to figure out whether “Copy of Board Report Script_FINAL_V8-USE THIS.py” is the correct version\n\nGit was created in 2005 by Linus Torvalds (who also created Linux) and has become the standard tool for version control, particularly in software development. However, it’s useful for any project where you’re working with text files - reports, data analysis scripts, documentation, and more.\n\n\n\nGithub is a website that hosts Git repositories online. While Git works on your own computer and can be hosted on an organisation’s server, Github provides a place to store your repositories in the cloud where others can access them.\nGithub adds several features on top of Git:\n\nA backup of your work stored safely online\nEasy collaboration - multiple people can work on the same project using a web-based interface\nA web interface to browse your files and history without using Git commands\nGithub Desktop, an application that runs on your computer and lets you sync a Github repository with your local filesystem\nTools for managing projects, discussing changes, and reviewing code\nA way to share your work publicly or keep it private within your organisation, with detailed user-based permissions\n\nMany organisations use Github to manage their projects because it makes collaboration straightforward and keeps everything in one accessible place. It’s free for most use cases, including unlimited private repositories for teams.\nAlternatives to Github do exist - one example is Gitlab.\n\n\n\n\n\n\nWarningAn important note regarding IG\n\n\n\nGithub is a public platform ultimately operated by Microsoft with servers in the USA. Public repositories are visible to anyone, even those without a github account! Be absolutely sure you understand what you’re pushing to Github if you are working on a project that interacts with non-public NHS data in any way. Even if you’re working with a private repository, this still is not suitable for anything sensitive as this will not be GDPR compliant.\n\n\n\n\n\nGit and Github use their own terminology and it’s important to keep this straight if we want to understand how to work with it. The essentials are below. This is non-exhaustive, but covers the basics needed to get started.\n\nRepository (“repo”) - A project folder that Git is tracking. Contains all your files plus the complete history of changes.\nCommit - A saved snapshot of your work at a particular point in time. Each commit records what changed, who made the change, and when.\nClone - Making a copy of a repository from Github onto your computer so you can work on it locally.\nPush - Sending your commits from your computer up to Github so others can see them.\nPull - Downloading commits from Github to your computer to get the latest changes others have made.\nPull request - A process for merging changes made on a branch back into the main branch.\nBranch - A separate line of development. Lets you work on new features without affecting the main version of your project.\nMerge - Combining changes from one branch into another, typically bringing your work back into the main branch.\nStaging - Marking files to be included in your next commit. You stage changes before committing them.\nConflict - When Git can’t automatically merge changes because two people edited the same part of a file differently. A merge conflict requires manual intervention to resolve, but thankfully Github gives us nice tools to deal with this.",
    "crumbs": [
      "Tools",
      "16. Git and GitHub"
    ]
  },
  {
    "objectID": "intermediate-skills-sessions/16-git-github/index.html#introduction",
    "href": "intermediate-skills-sessions/16-git-github/index.html#introduction",
    "title": "Git and Github",
    "section": "",
    "text": "Git is a version control system (or source control system, referring to source code). It’s software that helps you track changes to your files over time. You can sort of think of it like Track Changes in Word, but applied to an entire folder worth of files, much more powerful, and primarily designed to work with plaintext files - such as Python code, for example.\nThe basic concept in Git is a repository, which you can think of as a project that can contain an unlimited number of files and folders.\nUsing Git for a project lets us:\n\nAutomatically create and keep snapshots of your work at different points in time\nClearly see exactly what changed between versions\nGo back to earlier versions if something goes wrong\nCreate branches that let individuals or subgroups work on parts of a project without affecting the rest of it\nUnderstand who made which changes and when\nRequest reviews from our colleagues, add comments to changes, and resolve issues before committing to an update\nAvoid the classic issue of people working on their own copies of files, and having to figure out whether “Copy of Board Report Script_FINAL_V8-USE THIS.py” is the correct version\n\nGit was created in 2005 by Linus Torvalds (who also created Linux) and has become the standard tool for version control, particularly in software development. However, it’s useful for any project where you’re working with text files - reports, data analysis scripts, documentation, and more.\n\n\n\nGithub is a website that hosts Git repositories online. While Git works on your own computer and can be hosted on an organisation’s server, Github provides a place to store your repositories in the cloud where others can access them.\nGithub adds several features on top of Git:\n\nA backup of your work stored safely online\nEasy collaboration - multiple people can work on the same project using a web-based interface\nA web interface to browse your files and history without using Git commands\nGithub Desktop, an application that runs on your computer and lets you sync a Github repository with your local filesystem\nTools for managing projects, discussing changes, and reviewing code\nA way to share your work publicly or keep it private within your organisation, with detailed user-based permissions\n\nMany organisations use Github to manage their projects because it makes collaboration straightforward and keeps everything in one accessible place. It’s free for most use cases, including unlimited private repositories for teams.\nAlternatives to Github do exist - one example is Gitlab.\n\n\n\n\n\n\nWarningAn important note regarding IG\n\n\n\nGithub is a public platform ultimately operated by Microsoft with servers in the USA. Public repositories are visible to anyone, even those without a github account! Be absolutely sure you understand what you’re pushing to Github if you are working on a project that interacts with non-public NHS data in any way. Even if you’re working with a private repository, this still is not suitable for anything sensitive as this will not be GDPR compliant.\n\n\n\n\n\nGit and Github use their own terminology and it’s important to keep this straight if we want to understand how to work with it. The essentials are below. This is non-exhaustive, but covers the basics needed to get started.\n\nRepository (“repo”) - A project folder that Git is tracking. Contains all your files plus the complete history of changes.\nCommit - A saved snapshot of your work at a particular point in time. Each commit records what changed, who made the change, and when.\nClone - Making a copy of a repository from Github onto your computer so you can work on it locally.\nPush - Sending your commits from your computer up to Github so others can see them.\nPull - Downloading commits from Github to your computer to get the latest changes others have made.\nPull request - A process for merging changes made on a branch back into the main branch.\nBranch - A separate line of development. Lets you work on new features without affecting the main version of your project.\nMerge - Combining changes from one branch into another, typically bringing your work back into the main branch.\nStaging - Marking files to be included in your next commit. You stage changes before committing them.\nConflict - When Git can’t automatically merge changes because two people edited the same part of a file differently. A merge conflict requires manual intervention to resolve, but thankfully Github gives us nice tools to deal with this.",
    "crumbs": [
      "Tools",
      "16. Git and GitHub"
    ]
  },
  {
    "objectID": "intermediate-skills-sessions/16-git-github/index.html#working-with-github",
    "href": "intermediate-skills-sessions/16-git-github/index.html#working-with-github",
    "title": "Git and Github",
    "section": "Working with Github",
    "text": "Working with Github\n\nCreating a repository\nOnce you’ve got a Github account, creating a repository is very simple. On github.com/new, there’s a single screen that lets you create a blank repository. The key thing is choosing whether the repo is going to sit under your personal account or under an organisation, and deciding whether it is going to be public or private.\n\n\n\n\n\n\n\nCloning a repository\nCloning just means downloading the current state of a repository from Github to our local machine. However, we normally do this so that we can make local changes and potentially upload (push) those back to the source repository!\nIn Github Desktop, assuming you’re logged into your Github account that has access to the repo you want to clone, you should see it in the list when you click File &gt; Clone Repository:\n\n\n\n\n\n\n\nA brief note on gitignore\nThere are often cases where we want certain parts of the project not to be tracked by git. For example, if we’re using uv to maintain a venv with our python libraries, it would be a waste for these files to be uploaded and downloaded every time we interact with our repo. uv actually takes care of this for us! But in other cases we need to do this ourselves - for example, we might be writing a project that uses csv files for data, and we don’t want these to be uploaded.\nGithub Desktop makes this very easy. When we’ve added some files or folders that we want git to ignore, we just right click within the changes list and add either the individual files or entire folders to gitignore - once we’ve done this, git will disregard these files and folders, refrain from pushing them to the remote repo, and not track any changes made to them.\n\n\n\n\n\nWe can also go to Repository &gt; Repository Settings… and see everything currently in our gitignore file.\n\n\n\n\n\n\n\nMaking changes\n\nEditing files\nOnce we’ve set up our repo in Github Desktop, any changes to the local folder (with the exception of anything covered by gitignore) will automatically be detected. The app will helpfully show us which files have been added, removed, or changed, and for the latter it will directly show which lines have been altered:\n\n\n\n\n\n\n\nStaging changes and commit messages\nIn Git, staging is the process of selecting which changes you want to include in your next commit.\nIn Github Desktop, staging is handled automatically for you. When you make changes to files, they’ll appear in the left panel. By default, all changed files are staged (they have a tick next to them). If you want to commit only some files, you can untick the others - only the ticked files will be included in your commit.\nEvery commit needs a commit message - a short description of what changed and why. Good commit messages are crucial for understanding your project’s history later.\nIn Github Desktop, you’ll see a commit message box at the bottom left. The first line is the summary (required) - keep this short and clear, like “Fix bug in data import function” or “Add analysis for Q3 data”. The larger text box below is for an optional description where you can add more detail if needed.\n\n\n\n\n\nOnce you’re happy with your message, click “Commit to [branch name]”. Remember that if you’re working on a public repo, these comments will be visible publicly on Github.\n\n\n\nPushing and pulling\n\nPushing your changes\nOnce you’ve made one or more commits locally, they exist only on your computer. To share them with others (or just to back them up to Github), you need to push them.\nIn Github Desktop, after you’ve made a commit, you’ll see a button at the top that says “Push origin” with a number indicating how many commits are waiting to be pushed. Simply click this button and your changes will be uploaded to Github.\n\n\n\n\n\nIt’s good practice to push regularly, and certainly when you want other people to see the changes you’ve made in order to comment on them (or base their own changes on them).\n\n\n\n\n\n\nIf you’re using branches, make sure you’re on the correct branch before pushing your changes! Do also note that you can make changes on one branch and switch to another branch without pushing your changes yet; Github Desktop will offer to stash your changes on that branch locally so that you can go back to them later.\n\n\n\n\n\nPulling changes from others\nIf you’re working with others on the same repository, they’ll be making changes too. To get their latest changes onto your computer, you need to pull from Github.\nIn Github Desktop, when there are new commits on Github that you don’t have locally, you’ll see a “Pull origin” button at the top. Click this to download the latest changes.\n\n\n\n\n\nMake sure you’ve pulled the latest state of the branch before making your own changes - not doing so can result in conflicts.\n\n\n\nViewing history\nOne of the most powerful features of git is the ability to see your project’s complete history. In Github Desktop, click on the “History” tab (next to “Changes”) to see a list of all commits that have been made to the repository.\n\n\n\n\n\nClicking on any commit shows you exactly what changed in that commit - which files were modified, and what the specific changes were. This is incredibly useful for understanding how your project evolved, or for tracking down when a particular change was made.\nYou can also view this history on the Github website, which provides additional features like searching through commits and viewing the state of files at any point in time.\n\n\nBranching basics\nBranches are one of Git’s most powerful features, but they can seem confusing at first. Think of them as parallel versions of your project where you can experiment or work on new features (like a specific subset of a report) without affecting the main version.\nBy default, every repository has a main branch. This is typically the “official” version of your project. When you want to add a new feature or make significant changes, you can create a new branch, make your changes there, and then merge them back into main when you’re happy with them.\n\n\n\nFrom https://blog.while-true-do.io/git-next-steps/. © 2021, Daniel Schier, CC BY-SA 4.0\n\n\nIn Github Desktop, you can create a new branch by clicking on the “Current Branch” dropdown at the top and selecting “New Branch”. Give it a descriptive name like “add-data-validation” or “update-documentation”.\n\n\n\n\n\nOnce you’re on a branch, any commits you make will only affect that branch. You can switch back to main (or any other branch) at any time using the same dropdown. When your work on the branch is complete, you’ll typically merge it back into main using a pull request (covered in the Collaboration section).",
    "crumbs": [
      "Tools",
      "16. Git and GitHub"
    ]
  },
  {
    "objectID": "intermediate-skills-sessions/16-git-github/index.html#collaboration",
    "href": "intermediate-skills-sessions/16-git-github/index.html#collaboration",
    "title": "Git and Github",
    "section": "Collaboration",
    "text": "Collaboration\n\nWorking with others\nOne of Github’s greatest strengths is making collaboration straightforward. When multiple people are working on the same repository, the basic workflow is:\n\nPull the latest changes before you start work\nMake your changes and commit them locally\nPull again to get any changes others have made while you were working\nPush your changes.\n\nFor larger teams or more complex projects, it’s common to use branches for each feature or task. This means different people can work on different aspects of the project simultaneously without interfering with each other’s work. However, this is optional, and if you’re just starting out with Github it’s absolutely fine to just keep everything on a main branch while you get the hang of things.\n\n\nHandling conflicts\nSometimes, Git can’t automatically merge changes because two people have edited the same part of the same file in different ways. This is called a merge conflict, and it requires manual intervention to resolve.\nWhen you pull changes and a conflict occurs, Github Desktop will show you which files are conflicted. You can click on a conflicted file and Github Desktop will show you both versions of the conflicting sections, letting you choose which to keep (or manually edit to combine them).\n\n\n\n\n\n\n\nPull requests\nA pull request (often abbreviated to PR) is how you propose and discuss changes before they’re merged into the main branch. Despite the name, it’s not actually about “pulling” - think of it as a request for your changes to be merged.\nThe typical workflow is:\n\nCreate a branch for your work\nMake commits on that branch\nPush the branch to Github\nOpen a pull request on Github’s website (Github Desktop will also offer to do this for you)\nOthers can review your changes, leave comments, and suggest modifications\nOnce approved, the pull request is merged into main\n\nPull requests are particularly useful because they provide a space for code review and discussion, and they create a clear record of what changed and why. They’re central to how many development teams work.",
    "crumbs": [
      "Tools",
      "16. Git and GitHub"
    ]
  },
  {
    "objectID": "intermediate-skills-sessions/16-git-github/index.html#resources",
    "href": "intermediate-skills-sessions/16-git-github/index.html#resources",
    "title": "Git and Github",
    "section": "Resources",
    "text": "Resources\n\nW3schools git and Github tutorials\nGithub documentation",
    "crumbs": [
      "Tools",
      "16. Git and GitHub"
    ]
  },
  {
    "objectID": "intermediate-skills-sessions/17-quarto-intro/index.html",
    "href": "intermediate-skills-sessions/17-quarto-intro/index.html",
    "title": "An Introduction to Quarto",
    "section": "",
    "text": "This session is a standalone introduction to Quarto, a tool which can be used to turn notebooks into sharable documents and build presentations and websites from live code segments.\nWhilst this session is not part of a specific series it will be more accessible once you have an understanding of key concepts as covered in existing code club sessions.\n\nSlides\nUse the left ⬅️ and right ➡️ arrow keys to navigate through the slides below. To view in a separate tab/window, follow this link.\n\n\n\n\nCode along requirements\nIn addition to the slideshow above there will also be some elements which learners may wish to code along with. The demonstration has been run in VS Code enabled with the Quarto extension.\nThe code base for the demonstration is stored in an SCW repository as it connects to SCW servers, but the information downloaded is a replica of information released by NHS England.\nOnce downloaded you will need to run the “uv sync” command in a powershell terminal from your local repository folder.\n\n\nWhat is Quarto?\n“Quarto” is a system for publishing scientific and technical work. The name is a reference to the “Quarto” format of print publishing, where a sheet is printed into 4 sections which can all be printed double sided.\nThe Quarto system (code not print) allows for multiple code languages to be combined with markdown to allow for users to format their output for readability before converting into one of a number of formats from Word documents and PDFs to Presentations and Websites (For example, this website has been built in Quarto).\nIn its simplest form Quarto is a standalone command-line interface (CLI) but is easily integrated with integrated development environments (IDEs) such as VS Code, Positron or R-Studio. Given that we have used VS Code throughout the rest of the session.\n\n\nHow do I use Quarto in VS Code?\nThe easiest way to use Quarto is to install the relevant extension. Go to the extensions tab in VS Code and search for “Quarto”.\n\n\n\nFind the Quarto Extension\n\n\nInstalling this will enable the quarto extension meaning you can now access commands via the terminal or via the command pallette and search “Quarto” which will bring you to the Quarto commands.\n\n\n\nViewing Quarto commands in the Command Palette\n\n\nThere may be further items required depending on your output requirements but this should enable output into a number of different formats including Word and HTML.\nA helpful command line tip: entering the command quarto help will provide a list of common commands that you can use to interact with your quarto documents.\nThe most commonly used of these will likely be “quarto render –to [format]” (where format is something like “HTML”, “Word”, or “PDF”).\n\n\nFurther reading\nRead more about publishing your work using Quarto",
    "crumbs": [
      "Tools",
      "17. Quarto"
    ]
  },
  {
    "objectID": "intermediate-skills-sessions/intermediate_skills_schedule.html",
    "href": "intermediate-skills-sessions/intermediate_skills_schedule.html",
    "title": "Code Club Intermediate Skills Schedule",
    "section": "",
    "text": "This is the schedule and contents page for Code Club’s series on Intermediate Skills for FY25/26.\nThe series covers aspects of Python programming that appear in the Intermediate and Advanced columns of the National Competency Framework, as well as an introduction to Machine Learning and some other coding-related skills to have up your sleeve, such as source control with Git.\nThe Demonstration, Presentation, and Notebook columns indicate the content to be expected for each session:\n\nDemonstration: A live show-and-tell relating to the discussion topic.\nPresentation: A slide deck covering the discussion topic.\nNotebook: A Jupyter Notebook containing code-along elements or examples for people to work through after the session.\n\nWe have mapped the contents of the syllabus to competencies in the National Competency Framework for Data Professionals in Health and Care so that you can see which sessions will help you on your way towards them. For full details of the skills in the framework, the self-assessment tool can be found on FutureNHS.\nPlease note that this is a first draft of the mapping of NCF competencies to our syllabus and it is awaiting review.\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  Session Date\n  Module\n  Session Name\n  Description\n  Demonstration\n  Presentation\n  Notebook\n  NCF Competency\n\n\n\n  \n    04/12/2025\n    Tools\n    Git and GitHub\n    Source control and collaboration\n    -\n    -\n    -\n    DEC6 / DSC6: Programming\n  \n  \n    18/12/2025\n    Tools\n    Quarto\n    Reproducible reports, documents and websites\n    -\n    -\n    -\n    SA13: Data Automation\n  \n  \n    08/01/2026\n    Intermediate Python\n    Webscraping and APIs\n    How to use these techniques to import data in an automatic, programmatic way from the web\n    -\n    -\n    -\n    SA13: Data Automation\n  \n  \n    22/01/2026\n    Machine Learning\n    Introduction to Machine Learning\n    Demystifying the concept and introducing different kinds of models\n    -\n    -\n    -\n    SA12: Machine Learning\n  \n  \n    05/02/2026\n    Machine Learning\n    Linear and Logistic Regression Machine Learning Models\n    A closer look at these models in a Machine Learning context\n    -\n    -\n    -\n    SA12: Machine Learning\n  \n  \n    19/02/2026\n    Machine Learning\n    Machine Learning Workflows\n    What a typical end-to-end machine learning project workflow looks like\n    -\n    -\n    -\n    SA12: Machine Learning\n  \n  \n    05/03/2026\n    Machine Learning\n    Unsupervised Machine Learning\n    When can this technique be useful and what are its shortcomings?\n    -\n    -\n    -\n    SA12: Machine Learning\n  \n  \n    19/03/2026\n    Intermediate Python\n    Generators\n    Pros, cons, practical applications\n    -\n    -\n    -\n    SA21 : Python Proficiency (Intermediate)\n  \n  \n    02/04/2026\n    Intermediate Python\n    Decorators\n    Modify or extend functions with simple syntax\n    -\n    -\n    -\n    SA21 : Python Proficiency (Intermediate)\n  \n  \n    16/04/2026\n    Intermediate Python\n    Asynchronous programming and callbacks\n    How to use and the differences between asynchronous and synchronous\n    -\n    -\n    -\n    SA21 : Python Proficiency (Intermediate)\n  \n  \n    30/04/2026\n    Intermediate Python\n    Higher order functions\n    How to use functions within functions, and why\n    -\n    -\n    -\n    SA21 : Python Proficiency (Intermediate)\n  \n  \n    14/05/2026\n    Intermediate Python\n    Map, filter, reduce\n    Extend your functional programming capabilities with these in-built higher order functions\n    -\n    -\n    -\n    SA21 : Python Proficiency (Advanced)\n  \n  \n    28/05/2026\n    Intermediate Python\n    Logging, debugging, exception handling\n    How to handle exceptions and how to eliminate errors in your programs\n    -\n    -\n    -\n    SA21 : Python Proficiency (Advanced)\n  \n  \n    11/06/2026\n    Intermediate Python\n    Packages\n    How to build your own package\n    -\n    -\n    -\n    -\n  \n  \n    25/06/2026\n    Intermediate Python\n    Design patterns\n    Using iterators, decorators, proxies and factories\n    -\n    -\n    -\n    SA21 : Python Proficiency (Advanced)",
    "crumbs": [
      "Code Club Intermediate Skills Schedule"
    ]
  },
  {
    "objectID": "intermediate-skills-sessions/18-webscraping-apis/index.html",
    "href": "intermediate-skills-sessions/18-webscraping-apis/index.html",
    "title": "Webscraping and APIs",
    "section": "",
    "text": "Ever wanted to automate the downloading of data from external websites? Ever had a request where you need historic data from NHS publications where each .csv file is on its own web page and you are faced with negotiating dozens of clicks and dialogue boxes? Ever wanted to pipe public data directly into a report to provide wider context entirely fuss-free? Then you have come to the right tutorial. We are going to cover two methods for doing these very things, things that you can take away and apply today: no highfalutin concepts to digest, no need to hold out hope for a juicy data science project to get your teeth into. What’s more, we are going to give you some examples of public datasets that you can try these out on, giving you data with which to exercise your other Python skills.\nThe first method we are going to cover is webscraping, which is basically a way to retrieve elements on web pages by accessing them via the HTML tags (stay with us, there’s no need to become an expert in HTML, the Python package is going to do the hard work).\nThe second method is to access data made available via APIs (Application Programming Interfaces). We will specifically be looking at APIs that provide the data in JSON format, something which only requires minimal manipulation to put it into a dataframe.\nHowever, before we get to the good stuff, there are a few things to touch on that will prove essential when using these two methods. They relate to accessing URLs (web links) dynamically and making requests to content via the web. We will give you a brief introduction to prime you for when they come up later.",
    "crumbs": [
      "Intermediate Python",
      "18. Webscraping and APIs"
    ]
  },
  {
    "objectID": "intermediate-skills-sessions/18-webscraping-apis/index.html#regular-expressions",
    "href": "intermediate-skills-sessions/18-webscraping-apis/index.html#regular-expressions",
    "title": "Webscraping and APIs",
    "section": "Regular Expressions",
    "text": "Regular Expressions\nThis is a very important part of webscraping and making API calls since you are often interested in accessing web pages, hosted files and API endpoints based on a URL pattern. Where it becomes useful is when you want to programmatically access any URLs or file names that match a pattern, but contain an element that can vary: for example, when there are monthly editions of a publication, where the file name or URL contains the name of the month. Take the example below:\n\ndynamic_section = r'^england-[a-z]+-202[0-9]$'\n\nThis regular expression is intended to represent the changing part of a URL that points to the specific pages where monthly publications are hosted. Let’s look at the elements:\n\nThe “r” in front of the string tells Python that it should handle whatever comes between the quotation marks as a raw string, which is to say that it should ignore any of Python’s conventions around special characters, such as backslash being an escape character, and that it should pass the regex string to the re functions without manipulating it in any way.\nThe “^” states that the following characters must come at the beginning of the string that you are searching, i.e. nothing should come before it.\nThe “$” denotes the end of the string, i.e. nothing should come after it.\nThe string must start with the sequence “england-”.\nIt must end with the sequence “-202x”, where x is any digit from 0 to 9.\nAnd the “[a-z]+” means that the string will contain one or more (the “+”) lower case alphabetical characters (“[a-z]”, i.e. any lower case letter in the range between the square brackets).\n\nTaking the following as the base URL…\n'https://digital.nhs.uk/data-and-information/publications/statistical/learning-disabilities-health-check-scheme'\nThe following would be valid URLs matching the pattern of dynamic_section:\nhttps://digital.nhs.uk/data-and-information/publications/statistical/learning-disabilities-health-check-scheme/england-january-2024\nhttps://digital.nhs.uk/data-and-information/publications/statistical/learning-disabilities-health-check-scheme/england-may-2021\nBut the following would be ignored:\nhttps://digital.nhs.uk/data-and-information/publications/statistical/learning-disabilities-health-check-scheme/england-quarter-4-2020-21\nIf you want a hand with creating a regular expression that does the job for your situation, https://pythex.org has an interface that can be used to test regex patterns against text springs, as well as a regex cheatsheet. The Geeks for Geeks tutorial has a good set of examples of what each regex character sequence could be used to target.\n\n\n\n\n\n\nNoteChoose the right library\n\n\n\nThe Python library to use when constructing regular expressions is re. There is also a library called regex, but it is an older library with less functionality. You do not need to explicitly install re since it comes with Python when that is installed, but you still need to import it.",
    "crumbs": [
      "Intermediate Python",
      "18. Webscraping and APIs"
    ]
  },
  {
    "objectID": "intermediate-skills-sessions/18-webscraping-apis/index.html#the-rest-standard-and-get-requests",
    "href": "intermediate-skills-sessions/18-webscraping-apis/index.html#the-rest-standard-and-get-requests",
    "title": "Webscraping and APIs",
    "section": "The REST standard and GET requests",
    "text": "The REST standard and GET requests\nWhen it comes to making requests for data online, it is important to understand a little about how they are made and the standard that underpins them. The requests that we are concerned with follow the REST (REpresentational State Transfer) standard. REST guides the design of processes, standardising and simplifying the communication of requests for data hosted on web servers. As a result, operations are made using a standard set of terms. The most common ones are listed below:\n\nGET: is used when you want to read data on the server.\nPOST: is used to create data.\nPATCH (or PUT): is used to update data.\nDELETE: no surprise, is used to delete data.\n\nSince this tutorial is teaching you to be a consumer of this data, we are really only interested in GET requests. Whether you are webscraping or making a request to an API endpoint, you will be making a GET request. The first step of each is to make a GET request using the request library and checking which response is returned:\n\nA response of 200 is positive, i.e. there is data to be had via the supplied URL.\n400 is a negative response.\n304 is a “not modified” or “no new data” reponse, which will come up again later when we cover API call etiquette.\nThese codes are often built into try/except or if/else blocks to govern what happens when there is / isn’t any available data.\nThere are many other codes that you may wish to handle, particularly if you want to generate informative error messages, but the three listed above should be enough to get you started.\n\n\nimport request as req\n\nurl = [...] # the target URL\n\nresponse = req.get(url)\n\nif response.status_code == 200:\n  # do something with the data\nelse:\n  print(f'Failed to fetch webpage: {response.status_code}')\n\nNow that we have introduced those concepts, we can start having a look at the things you were promised.",
    "crumbs": [
      "Intermediate Python",
      "18. Webscraping and APIs"
    ]
  },
  {
    "objectID": "intermediate-skills-sessions/18-webscraping-apis/index.html#webscraping",
    "href": "intermediate-skills-sessions/18-webscraping-apis/index.html#webscraping",
    "title": "Webscraping and APIs",
    "section": "Webscraping",
    "text": "Webscraping\nThis is a really handy tool for automating the extraction from a web page of anything that is encoded with HTML tags. It can be used to:\n\nDownload files hosted on the web server and made available via hyperlinks; for example, monthly / annual data publications.\nCopy down data tables appearing on a web page and converting them to a Pandas dataframe; for example, data collection deadlines / data dissemination dates.\nCopy text from the page title, headers or the body of the page.\nCopy images, hyperlinks, mailto links, iframes…\n\n\nHTML tags\nThese are important since they encode the structure of a web page. Understanding these gives you an idea of what is possible when it comes to accessing elements of the structure of a web page.\nThere’s a nice compact HTML cheatsheet from Stanford University, but if you are not into the whole brevity thing, Geeks for Geeks has got you covered again with a nicely laid-out explanation of each.\n\n\nBeautiful Soup\nOne of the most commonly used libraries for webscraping is Beautiful Soup. It parses the HTML and allows the user to access the elements using familiar Python syntax. The documentation is very comprehensive, and the Quick Start section provides plenty of useful, simple examples.\nTo install it in your Python environment, enter uv add beautifulsoup4 into your terminal.\nNote that when you import it into your script, you write: from bs4 import BeautifulSoup.\n\n\nExamples\nLet’s see how this is used in practise. First of all, we will install the packages that we are going to be using.\n\nfrom bs4 import BeautifulSoup # the webscraping library\n\nimport requests as req # the web request library\n\nimport re # the regular expressions library that comes with the Python installation.\n\n# this will help us construct dynamic URLs from different elements joined together.\n# it can also be used to make HTTP requests.\n# it is installed by entering \"uv add urllib3\" into your terminal\nfrom urllib.parse import urljoin \n\n# used for converting files into binary data, which can then be converted into \n# other formats.\n# it comes as part of the Python installation.\nfrom io import StringIO \n\nimport pandas as pd # so that we can store our data in a dataframe\n\nimport os # operating system functions, such as accessing file directories\n\n\nA simple request to retrieve a web page title\n\n\n\n\n\n\nNoteInstantiation\n\n\n\nNote that it is conventional to instantiate a BeautifulSoup parser object as “soup”.\n\n\n\nurl = 'https://www.scwcsu.nhs.uk/about/our-values' # define the url in question\n\nresponse = req.get(url) # define the response as a GET request to the URL\n\n# if there is a positive reponse to the request, create a BeautifulSoup parser object\n# that collects the parsed content of the response.\n# Then print the web page \"title\" element.\n# The parser library being used is Python's in-built \"html.parser\"\nif response.status_code == 200:\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    print('Webpage title:', soup.title.string)\n\n# otherwise, return a helpful error message\nelse:\n    print(f'Failed to fetch webpage: {response.status_code}')\n\nWebpage title: Our values - NHS SCW Support and Transformation for Health and Care\n\n\nFor a list of alternative parser libraries, see this section of the BeautifulSoup documentation.\n\n\nDisplay the full HTML of a web page.\nIf you want to inspect all of the HTML code for a given page, so that you can get an idea of what is available, you can use the prettify() method. This re-uses the same soup object defined above. The output of the code will not be produced since it has been deactivated so that it does not take up too much space on our website. We recommend that you run it in a downloaded copy of the accompanying Jupyter Notebook.\n\nprint(soup.prettify())\n\n\n\nScrape information from a table on a web page.\n\nurl = ('https://digital.nhs.uk/data-and-information/data-collections-and-data-sets/data-sets/mental-health-services-data-set/submit-data')\n\nresponse = req.get(url)\n\nif response.status_code == 200:\n    soup = BeautifulSoup(response.content, \"html.parser\")\n\ntables = soup.find_all('table') # if you are sure there is only one, use \"soup.find()\"\nprint(type(tables))\n\n# get the first item in the BeautifulSoup ResultSet, convert it into a string, \n# and read the html into a pandas DataFrame\ntable_df = pd.read_html(str(tables))[0]     # if you are sure there is just one, no need to select by index \"[0]\"\n\ntable_df\n\n&lt;class 'bs4.element.ResultSet'&gt;\n\n\n\n\n\n\n\n\n\nPerformance month\nUpdates/Resubmissions\nCut of data taken at 11:59:59pm\n\n\n\n\n0\nOctober 2025\nApril, May, June, July, August September\n25 November 2025\n\n\n1\nNovember 2025\nApril, May, June, July, August, September, Oct...\n22 December 2025\n\n\n2\nDecember 2025\nApril, May, June, July, August, September, Oct...\n26 January 2025\n\n\n3\nJanuary 2026\nApril, May, June, July, August, September, Oct...\n24 February 2026\n\n\n4\nFebruary 2026\nApril, May, June, July, August, September, Oct...\n24 March 2026\n\n\n5\nMarch 2026\nApril, May, June, July, August, September, Oct...\n27 April 2026\n\n\n6\nApril 2026\nApril, May, June, July, August, September, Oct...\n27 May 2026 (end of year for resubmissions)\n\n\n7\nMay 2026\nApril\n24 June 2026\n\n\n8\nJune 2026\nApril, May\n23 July 2026\n\n\n9\nJuly 2026\nApril, May, June\n25 August 2026\n\n\n10\nAugust 2026\nApril, May, June, July\n23 September 2026\n\n\n11\nSeptember 2026\nApril, May, June, July, August\n26 October 2026\n\n\n12\nOctober 2026\nApril, May, June, July, August, September\n24 November 2026\n\n\n\n\n\n\n\n\n\nLocate a .csv file on a webpage.\nMost of the code is the same as the “title” example, but this time we are looking for a hyperlink on the page that points to a .csv file (that is to say, the URL ends with the .csv file extension).\n\nurl = ('https://digital.nhs.uk/data-and-information/publications/statistical/out-of-area-placements-in-mental-health-services/march-2024') \n\nresponse = req.get(url)\n\nif response.status_code == 200:\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n    # find a hyperlink element (has the tag \"a\") where the hyperlink element\n    # ends with \"csv\".\n    csv_link = soup.find(\"a\", href=lambda href: href and href.endswith('csv'))\n\n    file_url = csv_link[\"href\"] # just return the URL from within the HTML statement\n\n    print(\"Found .csv file:\", file_url)\n\nelse:\n    print(f'Failed to fetch webpage: {response.status_code}')\n\nFound .csv file: https://files.digital.nhs.uk/32/0B358C/oaps-open-data-mar-2024.csv\n\n\n\n\nDownload the file via the discovered hyperlink.\nFirst of all, we need to check whether the file is available for us to download, so we also need to check the reponse here, too.\n\nfile_name = file_url.split(\"/\")[-1]  # extract the file name from the URL i.e. the bit after the last \"/\"\nfile_response = req.get(file_url)\n\nif file_response.status_code == 200:\n\n    # save the file to the current directory\n    with open(f'{file_name}', \"wb\") as file:\n        file.write(file_response.content)\n    print(f\"Downloaded: {file_name}\")\nelse:\n    print(f\"Failed to download: {file_url}\")\n\nDownloaded: oaps-open-data-mar-2024.csv\n\n\n\n\nRead .csv data directly into a Pandas dataframe\nUsing the StringIO() class from the io library, create an in-memory stream of the data that can be operated on like a file, without having first saved it down as one. The .csv data is treated like a long string of text where fields are separated by delimiters (commas by default) and rows are separated by newline characters (typically \\n). The .read_csv() method in Pandas converts this string into a DataFrame.\n\nfrom io import StringIO \n\ncsv_content = StringIO(file_response.text)\n\ndf = pd.read_csv(csv_content)\n\ndf.head(3)\n\n\n\n\n\n\n\n\nGrouping\nPublicationPeriod\nPublicationDate\nQuestion\nBreakdown1\nBreakdown1Code\nBreakdown1Description\nBreakdown2\nBreakdown2Code\nBreakdown2Description\nValue\n\n\n\n\n0\nMonth\n01/03/2024-31/03/2024\n2024/06\nAverage recorded daily cost over the period\nBedType\n10\nAcute adult mental health care\nNaN\nNaN\nNaN\n695\n\n\n1\nMonth\n01/03/2024-31/03/2024\n2024/06\nLower quartile daily cost over the period\nBedType\n10\nAcute adult mental health care\nNaN\nNaN\nNaN\n576\n\n\n2\nMonth\n01/03/2024-31/03/2024\n2024/06\nNumber of OAPs active during the period with a...\nBedType\n10\nAcute adult mental health care\nNaN\nNaN\nNaN\n280\n\n\n\n\n\n\n\n\n\nUsing a regular expression and urljoin to locate files on multiple web pages.\nThis is a hefty bit of code with multiple for loops and if statements. Hopefully, the inline comments explain what is going on at each stage. The great thing about using Python code is that you can easily re-use this code, simply replacing the base URL and the dynamic section. You can also specify the file type, in case you want to use it to download .xlsx files, for example.\nWhen defining the “dynamic_section”, you need to make sure that you have identified a regular expression that matches the pattern of all the target URLs that you are interested in.\nWhile the regular expression in the example above ended with 202[0-9]$, it has been set to 2024$ here so that it doesn’t download too many files in one go.\n\nurl = 'https://digital.nhs.uk/data-and-information/publications/statistical/learning-disabilities-health-check-scheme'\n\ntarget_urls = []                           # empty list that will later get filled with target URLs in a for loop.\n\ndynamic_section = r'^england-[a-z]+-2024$' # the regular expression for the URLs we are interested in. note that the $ implies that you don't want anything else to follow.\n\nresponse = req.get(url)                    # get the response from the base URL\n\next = '.csv'                               # specify the file type\n\nif response.status_code == 200:\n    soup = BeautifulSoup(response.content, \"html.parser\")     # if there is a successful response, create a BeautifulSoup object.\n\n    for link in soup.find_all('a', href = True):                # for each of the instances of the pattern we are looking for...\n        sublink = link[\"href\"]\n        if re.match(dynamic_section,sublink.split('/')[-1]):\n            full_url = urljoin(url, sublink)                   \n            target_urls.append(full_url)                        # ... add the constructed full URL to a list of target URLs\n        \n    for link in target_urls:                                    # check for a successful response (code 200) from each URL...\n        response = req.get(link)                                \n        if response.status_code == 200:\n            soup = BeautifulSoup(response.content, \"html.parser\") # ... and create a BeautifulSoup object for each.\n\n            for link in soup.find_all(\"a\", href=True):          # for each URL found on each of the pages in target_urls...\n                file_url = link['href']                         \n\n                if file_url.endswith((ext)):                    # ... check for .csv file extensions\n                    print(\"Found .csv file:\", file_url)\n\n                    file_name = file_url.split(\"/\")[-1]         # extract the file name from the URL i.e. everything after the last /\n                    file_response = req.get(file_url)           # check the response for each file\n            \n                    if file_response.status_code == 200:        # if there's a successful response...\n                        \n                        with open(file_name, \"wb\") as file:     # ... save the file to the current directory\n                            file.write(file_response.content)\n                        print(f\"Downloaded: {file_name}\")\n                    else:\n                        print(f\"Failed to download: {file_url}\")\n\nelse:\n    print(f'Failed to fetch webpage: {response.status_code}')   # this else statement pairs with the original response code check for the base URL\n                                                                # (see the first \"if\" in this code block)",
    "crumbs": [
      "Intermediate Python",
      "18. Webscraping and APIs"
    ]
  },
  {
    "objectID": "intermediate-skills-sessions/18-webscraping-apis/index.html#apis",
    "href": "intermediate-skills-sessions/18-webscraping-apis/index.html#apis",
    "title": "Webscraping and APIs",
    "section": "APIs",
    "text": "APIs\nThe second method we are going to cover in this tutorial is making requests to data that has been made available via a web API endpoint. API stands for Application Programming Interface, and these are used to extend the functionality of an application allowing it to communicate with another.\nWe will specifically be looking at web APIs that return their data in JSON format1. For a very simple explanation of what JSON data is, have a look at the W3 Schools page. In short, it is a format that is typically used for sending data from a server to a web page. The key thing to understand about them is that each record takes the form of a list of key : value pairs, looking just like a Python dictionary, and multiple records are contained in an array denoted by square brackets. You will see this structure reflected in the Python code below.\nFor this demonstration, we are going to pull in flood alerts with at least a severity score of 3 from the Environment Agency’s live flood alert data2. If you want to see the data that we will be extracting in its original JSON format, follow this URL. It is the same URL as will be used in the the Python-based request.\n\n\n\n\n\n\nThe options available for querying the data is determined by the way in which the API endpoint has been constructed by the developers. For example, the data may be aggregated by default, meaning that an unfiltered request returns the data aggregated to the highest level. This may well be intended to stop people from requesting masses of granular data by default, but it can be difficult to get an idea of what the available categories are in the data breakdown, if there isn’t a dedicated part of the API that allows for the breakdown categories to be returned in a list. It is important to read the API documentation thoroughly to get an undestanding of what your options are.\n\n\n\nNow for the code itself. As was mentioned in the section above on REST and GET requests, we need to import the requests library to make the GET request. We also need to import the json library, which decodes the JSON format, converting the JSON data types into Python data types. A table of these conversions can be found here.\nThe example below has been placed inside a function. This isn’t essential, but it can prove useful if you want to expand the function to take inputs that make it reusable and apply it to different extracts of the data. You will also see that placing the code inside a function becomes necessary when using Python generators, which is another topic covered later on in our Intermediate Skills curriculum.\n\nimport json\n\nimport requests as req\n\ndef get_flood_alerts():\n    request_url = 'https://environment.data.gov.uk/flood-monitoring/id/floods?min-severity=3' # Step 1\n    response = req.get(request_url) # Step 2\n    if response.status_code == 200: # Step 3\n        results_json = response.json()['items'] # Step 4\n        alerts = [{   # Step 5\n            'id' : json['@id'], # Step 6\n            'description' : json['description'],\n            'area_name' : json['eaAreaName'],\n            'flood_area_id' : json['floodAreaID'],\n            'is_tidal' : json['isTidal'],\n            'severity' : json['severity'],\n            'severity_level' : json['severityLevel'],\n            'time_message_changed' : json['timeMessageChanged'],\n            'time_raised' : json['timeRaised'],\n            'time_severity_changed' : json['timeSeverityChanged']\n        } for json in results_json] # Step 5 continued\n        return alerts # Step 7\n    else:\n        print(f'Failed to fetch data. Response status: {response.status_code}') # Step 3 continued\n\nThe steps below are labelled in the code above:\n\nThe first step is to define the request URL, which points to the API endpoint. If you follow the link in a web browser, you will see all of the relevant records laid out in JSON format. Note that in the example below there is the section \"min-severity=3\" which comes after a question mark ?. The question mark indicates that everything after it relates to an optional filter, which is to say that you can get a cut of the data based on specific criteria. You can add multiple filters by joining them together with “&”.\nThen a GET request is made and assigned to the variable “response”.\nThen, as with the webscraping, we want to handle the response from the webserver gracefully, in case the data is not available. This means placing details of what we are requesting in an if/else conditional statement. If we get a positive response of 200, proceed with retrieving the data; otherwise, return the error code.\nThe results of the JSON data request are returned as a dictionary (remember that data in JSON format is very similar to Python dictionaries). In the flood data API, 'items' is the key in the dictionary, and the values are all of the data records3. In the line results_json = response.json()['items'], we are accessing all of the records stored against “items”.\nThe values corresponding to the key “items” are held in an array (in square brackets). We give the results that we want to return the variable name alerts. We then create a for loop to create a Python list of all of the records, and for each record there is a key : value pair for each field in the data, replicating the array / JSON record object structure. Think of each key in the dictionary as being a column name, each value as the record value and each item in the alerts list as being a row.\nIn the dictionary, we give the key a name of our choosing. It is what we want the column name to be. On the value side of the dictionary, we are accessing the value corresponding to each key in the JSON reponse. In the line for json in results_json, each json is it’s own dictionary containing a record, for example \"@id\" : \"http://environment.data.gov.uk/flood-monitoring/id/floods/112WAFTUBA\", \"description\" : \"Upper Bristol Avon area\", \"eaAreaName\" : \"Wessex\".... We want to access the value corresponding to each key in the JSON data and store it against our key.\nThe list of records is returned in a format that can be used by other Python packages.\n\nLet’s put the results of the query into a Pandas DataFrame and view the results. For this we call the function we defined above and have Pandas convert that list into a DataFrame, with column names that we have given it and rows that correspond to each record.\n\nalerts = pd.DataFrame(get_flood_alerts())\n\nprint(f'Number of rows and columns in the dataset: {alerts.shape}')\n\nalerts.head()\n\nNumber of rows and columns in the dataset: (216, 10)\n\n\n\n\n\n\n\n\n\nid\ndescription\narea_name\nflood_area_id\nis_tidal\nseverity\nseverity_level\ntime_message_changed\ntime_raised\ntime_severity_changed\n\n\n\n\n0\nhttp://environment.data.gov.uk/flood-monitorin...\nUpper Bristol Avon area\nWessex\n112WAFTUBA\nFalse\nFlood alert\n3\n2026-01-28T11:53:00\n2026-01-28T11:53:35\n2026-01-15T18:20:00\n\n\n1\nhttp://environment.data.gov.uk/flood-monitorin...\nRiver Trent from Cromwell Weir to Gainsborough\nEast Midlands\n034WAB424\nFalse\nFlood alert\n3\n2026-01-29T09:20:00\n2026-01-29T09:20:43\n2026-01-27T16:50:00\n\n\n2\nhttp://environment.data.gov.uk/flood-monitorin...\nLower River Soar in Leicestershire\nEast Midlands\n034WAF428\nFalse\nFlood alert\n3\n2026-01-29T10:38:00\n2026-01-29T10:38:52\n2026-01-08T16:20:00\n\n\n3\nhttp://environment.data.gov.uk/flood-monitorin...\nRiver Churn and its tributaries\nThames\n061WAF02Churn\nFalse\nFlood alert\n3\n2026-01-29T09:55:00\n2026-01-29T09:55:50\n2026-01-15T21:04:00\n\n\n4\nhttp://environment.data.gov.uk/flood-monitorin...\nRiver Cherwell from Lower Heyford down to and ...\nThames\n061WAF14LChrwell\nFalse\nFlood alert\n3\n2026-01-29T10:52:00\n2026-01-29T10:52:46\n2026-01-11T08:41:00",
    "crumbs": [
      "Intermediate Python",
      "18. Webscraping and APIs"
    ]
  },
  {
    "objectID": "intermediate-skills-sessions/18-webscraping-apis/index.html#api-request-etiquette",
    "href": "intermediate-skills-sessions/18-webscraping-apis/index.html#api-request-etiquette",
    "title": "Webscraping and APIs",
    "section": "API request etiquette",
    "text": "API request etiquette\nThese are particularly important if you are making requests to free, public APIs, such as those provided by the government and NHS. These considerations are a little more advanced, and are probably only required if you are intending to develop something that supplies data with frequent updates, but it is worth being aware of them: You may start with a project that is small and simple, but it might then develop into something more data-hungry\n\nRespect rate limits\nAPIs typically define:\n\nRequests per second/minute\nDaily/monthly quotas\nBurst versus sustained limits\n\nThey may not be stated explicitly, but you should assume that these limits exist and throttle your calls appropriately. For more information on throttling techniques, have a look at this Medium post.\n\n\nUse caching\nIf the API isn’t subject to frequent changes, cache results locally so that you do not need to make repeated requests for the same data. Here’s a Geeks for Geeks tutorial on different types of caching in Python to get you started.\nIf you intend to create your own web app that is pulling in data from other sources, it is likely that you will be using a particular web app framework, and these will provide their own tools for web caching. For example, we have introduced Steamlit in another tutorial and their overview of caching can be found here. Another framework for creating web apps is Flask and a simple introduction to web caching using that framework can be found on PyQuestHub.\n\n\nAvoid polling too aggressively\n\nMake requests at a reasonable rate: do you really need to check for updates every second?. You can find out more about polling in this Medium blog post.\nMake use of webhooks: these allow for automatic communication between systems, eliminating the need for one system to constantly check another for updates. Data is pushed automatically whenever an event occurs. You can find another trusty Geek’s for Geek’s tutorial on webhooks here.\n\n\n\nUse conditional requests\nIf supported, you can use ETag and If-Modified-Since, which return a 304 Not Modfied reponse instead of a full JSON payload. In essence, this response is saying that no changes have been made to the content made available via the endpoint. You could build in some logic that handles the error without crashing the program, and also notifies the end user that there is no new data since the last update.\nExamples of each, plus a combined approach, can be found in this Python Lore tutorial.\n\n\nSelect only what you need\nJust as you would with a SQL query, try to select only what you need. Some APIs support field selection in the optional filters part of the URL (e.g ?fields=metric,value). Similarly, try not to request all of the records made available via the API. Would having a rolling 12 months’ data be sufficient? Could you store historic data locally? Try to use any filters available in the API to exclude any data that you do not need.\n\n\nIdentify yourself\nSome APIs require users to provide a User-Agent string. Failure to do so could mean that your request gets blocked by the web server. Web servers use the information to serve appropriate content, implement rate-limiting or block automated requests4. You can even add some kind of contact information or a link to the repository for your application so that you can be contacted if there is an issue (only include contact information you are willing to share publicly!). Below is an example of some Python that could be used to generate a User-Agent string:\n\nimport platform     # to get operating system information\nimport sys          # to get Python version information\n\nAPP_NAME = \"MyApiProject\"\nAPP_VERSION = \"1.0\"\nGITHUB_PAGE = \"https://github.com/NHS-South-Central-and-West/code-club\"\n\ndef build_user_agent():\n    python_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n    os_info = platform.system()\n    return f\"{APP_NAME}/{APP_VERSION} (Python {python_version}; {os_info}); {GITHUB_PAGE}\"\n\nheaders = {\n    \"User-Agent\": build_user_agent()\n}\n\n# let's see what that looks like:\n\nprint(headers)\n\n{'User-Agent': 'MyApiProject/1.0 (Python 3.12; Windows); https://github.com/NHS-South-Central-and-West/code-club'}\n\n\nThen, when you make the request, you pass the User-Agent string to the “headers” keyword argument:\n\nresponse = requests.get(\"https://environment.data.gov.uk/flood-monitoring/id/floods\", headers=headers)\n\n\n\n\n\n\n\nCautionRead the smallprint\n\n\n\nIt is advisable that you read any Terms of Service applied to the use of an API. Providers of free APIs may forbid commercial use (and you need to be sure what is meant by this), redistribution of the data and automated, high-frequency usage.",
    "crumbs": [
      "Intermediate Python",
      "18. Webscraping and APIs"
    ]
  },
  {
    "objectID": "intermediate-skills-sessions/18-webscraping-apis/index.html#the-fingertips_py-package",
    "href": "intermediate-skills-sessions/18-webscraping-apis/index.html#the-fingertips_py-package",
    "title": "Webscraping and APIs",
    "section": "The fingertips_py package",
    "text": "The fingertips_py package\nThis is a package that was originally developed by Public Health England to make it easy to import data via the Fingertips API endpoint. It’s an example of what the possibilities are, hopefully serving as inspiration for your own Python projects. It’s also pretty useful, if you want to make use of Fingertips data yourself!\nWe have created a walkthrough of using the fingertips_py package here.",
    "crumbs": [
      "Intermediate Python",
      "18. Webscraping and APIs"
    ]
  },
  {
    "objectID": "intermediate-skills-sessions/18-webscraping-apis/index.html#exercises",
    "href": "intermediate-skills-sessions/18-webscraping-apis/index.html#exercises",
    "title": "Webscraping and APIs",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a regular expression that could be used to identify all of the Excel files on the following web page:Mental Health Services Data Set Submission Reports.\n\nNote: The displayed document titles may not reflect the actual file URLs.\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\npattern = r'.*mswm-submission-tracker.*.xlsm'\n\n# . matches any character; * means any number of those.\n# That pattern can occur before or after \"mswm-submission-tracker\".\n\n\n\n\n\nWhich REST API response code means a positive result, i.e. that data is available?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n200\n\n\n\n\nWhich REST API response code means that no new data is available?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n304\n\n\n\n\nWrite some Python code to return the planned outages table on the SUS Service Announcements and Outages page to a Pandas DataFrame and then print the result. Make sure that you handle any error raised due to the web page being unavailable.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\nurl = 'https://digital.nhs.uk/services/secondary-uses-service-sus/secondary-uses-service-sus-what-s-new/service-announcements-and-outages'\n\nresponse = req.get(url)\n\nif response.status_code == 200:\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    table = soup.find('table')\n    table_df = pd.read_html(str(table))\n    print(table_df)\nelse:\n    print(f'Outages page currently unavailable: {response.status_code}')\n\n[                         Date         Time                  Type\n0   Wednesday 4 February 2026  6pm to 10pm  SUS+/DLP maintenance\n1  Wednesday 11 February 2026  6pm to 10pm  SUS+/DLP maintenance\n2  Wednesday 25 February 2026  6pm to 10pm  SUS+/DLP maintenance]\n\n\n\n\n\n\nWhich BeautifulSoup method can you use to return the HTML in a nicely laid out format?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nprint(soup.prettify())\n\n\n\n\nUsing the example under “Using a regular expression and urljoin to locate files on multiple web pages.” as a template, write some Python code that will download all of the NHS Talking Therapies Data Quality Reports for 2025 accessible via the official statistics page.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport re\n\nurl = 'https://digital.nhs.uk/data-and-information/publications/statistical/nhs-talking-therapies-monthly-statistics-including-employment-advisors'\n\ntarget_urls = []                           \n\ndynamic_section = r'^performance-[a-z]+-2025$' \n\nresponse = req.get(url)                    \n\next = '.csv'                               \n\nif response.status_code == 200:\n    soup = BeautifulSoup(response.content, \"html.parser\")     \n\n    for link in soup.find_all('a', href = True):                \n        sublink = link[\"href\"]\n        if re.match(dynamic_section,sublink.split('/')[-1]):\n            full_url = urljoin(url, sublink)                   \n            target_urls.append(full_url)                        \n        \n    for link in target_urls:                                    \n        response = req.get(link)                                \n        if response.status_code == 200:\n            soup = BeautifulSoup(response.content, \"html.parser\") \n\n            for link in soup.find_all(\"a\", href=True):          \n                file_url = link['href']                         \n\n                if file_url.endswith((ext)):                   \n                    print(\"Found .csv file:\", file_url)\n\n                    file_name = file_url.split(\"/\")[-1]        \n                    file_response = req.get(file_url)          \n            \n                    if file_response.status_code == 200:       \n                        \n                        with open(file_name, \"wb\") as file:     \n                            file.write(file_response.content)\n                        print(f\"Downloaded: {file_name}\")\n                    else:\n                        print(f\"Failed to download: {file_url}\")\n\nelse:\n    print(f'Failed to fetch webpage: {response.status_code}')   \n\n\n\n\n\nWhich character designates the beginning of the filter section of a URL when filtering a JSON API request? Which character is used to join multiple filters together?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nDesignates the beginning of the filter section: ?\nJoins multiple filters together: &\n\n\n\n\nWrite a Python function that will return a dataframe of the daily number of patients admitted to hospital with COVID-19 in 2025 via the UKHSA data dashboard API. You will need to read the API documentation, making use of the examples. The UKHSA data dashboard page.\n\n\nFilter the data to just 2025.\nThe geography_type should be Nation.\nThe geography should be England.\nReturn the following columns:\n\ntheme\nsub_theme\ntopic\ngeography\nmetric\nyear\ndate\nmetric_value\n\n\nHINT: Instead of “items” (as in the flood alerts example), the records are contained in a list called “results”.\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nimport json\nimport pandas as pd\nimport requests as req\n\ndef get_covid_admissions():\n    request_url = 'https://api.ukhsa-dashboard.data.gov.uk/themes/infectious_disease/sub_themes/respiratory/topics/COVID-19/geography_types/Nation/geographies/England/metrics/COVID-19_healthcare_admissionByDay?year=2025'\n    response = req.get(request_url)\n    if response.status_code == 200:\n        results_json = response.json()[\"results\"]\n        records = [{   \n            'theme': json['theme'],\n            'sub_theme': json['sub_theme'],\n            'topic': json['topic'],\n            'geography': json['geography'],\n            'metric': json['metric'],\n            'year': json['year'],\n            'date': json['date'],\n            'metric_value': json['metric_value'],\n        } for json in results_json\n        ]\n        return records\n    else:\n        print(f'Failed to fetch data. Response status: {response.status_code}')\n\nrecords = pd.DataFrame(get_covid_admissions())\n\nprint(f'Number of rows and columns in the dataset: {records.shape}')\n\nrecords.head()\n\nNumber of rows and columns in the dataset: (5, 8)\n\n\n\n\n\n\n\n\n\ntheme\nsub_theme\ntopic\ngeography\nmetric\nyear\ndate\nmetric_value\n\n\n\n\n0\ninfectious_disease\nrespiratory\nCOVID-19\nEngland\nCOVID-19_healthcare_admissionByDay\n2025\n2025-01-01\n144.0\n\n\n1\ninfectious_disease\nrespiratory\nCOVID-19\nEngland\nCOVID-19_healthcare_admissionByDay\n2025\n2025-01-02\n132.0\n\n\n2\ninfectious_disease\nrespiratory\nCOVID-19\nEngland\nCOVID-19_healthcare_admissionByDay\n2025\n2025-01-03\n119.0\n\n\n3\ninfectious_disease\nrespiratory\nCOVID-19\nEngland\nCOVID-19_healthcare_admissionByDay\n2025\n2025-01-04\n120.0\n\n\n4\ninfectious_disease\nrespiratory\nCOVID-19\nEngland\nCOVID-19_healthcare_admissionByDay\n2025\n2025-01-05\n121.0",
    "crumbs": [
      "Intermediate Python",
      "18. Webscraping and APIs"
    ]
  },
  {
    "objectID": "intermediate-skills-sessions/18-webscraping-apis/index.html#footnotes",
    "href": "intermediate-skills-sessions/18-webscraping-apis/index.html#footnotes",
    "title": "Webscraping and APIs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nXML is another common format for data made available via an API endpoint.↩︎\nEnvironment Agency real-time flood-monitoring API documentation↩︎\nIn actual fact, the format that the flood alert data is provided in is a little more complex and is more like a nested dictionary, where you have a top-level set of key-value pairs and one of those keys has values that are themselves dictionaries. In the case of the flood alert data, there are some metadata fields that are defined in the top level alongside “items”, and then the value against the “items” key is then itself an array of key-value pairs.↩︎\nSee (https://webscraping.ai/faq/requests/how-do-i-set-a-user-agent-string-for-requests)↩︎",
    "crumbs": [
      "Intermediate Python",
      "18. Webscraping and APIs"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Code Club!",
    "section": "",
    "text": "Code Club aims to support everyone at SCW in developing technical and analytical skills through interpretive dance code. We believe these skills are indispensable to the NHS today and in the future, enabling the delivery of high-quality insights through data science and advanced analytics, and the automation of day-to-day tasks with programming. We want to foster an environment that welcomes everybody, sparks ideas, and nurtures collaboration.\nThe Code Club syllabus has been designed to help people with little to no coding experience develop their skills in Python and extend their analytical skills through code. Sessions will be an hour long and held once per fortnight at 2:00 PM on Thursdays. Code Club has come a long way since the reboot in May 2025 and we are now offering tutorials on Intermediate Skills, in line with the National Competency Framework (plus a few things we think you will find useful and enjoyable!). If you are just joining us and are new to Python (or you just want to go back and refresh your memory), you can find materials to get you started plus some coding and data science essentials on our Essential Skills page."
  },
  {
    "objectID": "intermediate-skills-sessions/18-webscraping-apis/fingertips_py.html",
    "href": "intermediate-skills-sessions/18-webscraping-apis/fingertips_py.html",
    "title": "Walkthrough of fingertips_py",
    "section": "",
    "text": "In conjunction with the tutorial on webscraping and APIs, below is a walkthrough of the fingertips_py package maintained by the Department of Health and Social Care, which simplifies importing data via the Fingertips API."
  },
  {
    "objectID": "intermediate-skills-sessions/18-webscraping-apis/fingertips_py.html#documentation",
    "href": "intermediate-skills-sessions/18-webscraping-apis/fingertips_py.html#documentation",
    "title": "Walkthrough of fingertips_py",
    "section": "Documentation",
    "text": "Documentation\nFingertips Public Health Profiles (public-facing source of the data)\nFingertips Python API package (fingertips_py) documentation\nPackage repository"
  },
  {
    "objectID": "intermediate-skills-sessions/18-webscraping-apis/fingertips_py.html#installation",
    "href": "intermediate-skills-sessions/18-webscraping-apis/fingertips_py.html#installation",
    "title": "Walkthrough of fingertips_py",
    "section": "Installation",
    "text": "Installation\nFor more information, see the fingertips_py documentation.\nCode Club recommends using the uv package management software, but if you do need to use pip, the relevant installation instructions are below.\n\nuv\nTo install using uv, enter the following in the terminal:\nuv add fingertips_py\n\n\npip\nTo install using pip, activate your environment and enter the following in the terminal:\npython -m pip install fingertips_py"
  },
  {
    "objectID": "intermediate-skills-sessions/18-webscraping-apis/fingertips_py.html#exploring-package-functionality",
    "href": "intermediate-skills-sessions/18-webscraping-apis/fingertips_py.html#exploring-package-functionality",
    "title": "Walkthrough of fingertips_py",
    "section": "Exploring package functionality",
    "text": "Exploring package functionality\nLet’s have a look at the various methods in the package that allow us to import different levels of the dataset and the metadata.\n\nImporting the package\nWe will also use pandas for presenting the imported data in a dataframe.\n\nimport fingertips_py as ftp\n\nimport pandas as pd\n\n\n\nGet profile IDs by profile name\nThe profile IDs can be used to refer to different “profiles” on Fingertips. That is to say, different aspects of public health such as levels of diabetes, cardiovascular disease, or dementia in the country.\n\n# Cancer profile ID\ncancer_profile_md = ftp.metadata.get_profile_by_name('cancer services') # creates a dictionary of metadata. Profile names are not case sensitive.\n\ncancer_profile_id = cancer_profile_md['Id'] # access the profile ID in the metadata dictionary\n\n# Wider Determinants of Health profile ID\n\nwdoh_profile_md = ftp.metadata.get_profile_by_name('wider determinants of health')\n\nwdoh_profile_id = wdoh_profile_md['Id']\n\n# Return the IDs\n\nprint(f'Cancer profile ID: {cancer_profile_id}')\nprint(f'Wider Determinants of Health profile ID: {wdoh_profile_id}')\n\nCancer profile ID: 92\nWider Determinants of Health profile ID: 130\n\n\n\n\nGet profile metadata as a dataframe\nInspecting the metadata gives you a lot of information about a given profile, such as metric sources, definition, and polarity.\n\nwdoh_md_df = ftp.metadata.get_metadata_for_profile_as_dataframe(wdoh_profile_id)\n\nwdoh_md_df.head()\n\n\n\n\n\n\n\n\nIndicator ID\nIndicator\nIndicator number\nRationale\nSpecific rationale\nDefinition\nData source\nIndicator source\nDefinition of numerator\nSource of numerator\n...\nData re-use\nLinks\nIndicator Content\nSimple Name\nSimple Definition\nUnit\nValue type\nYear type\nPolarity\nDate updated\n\n\n\n\n0\n11401\nThe rate of complaints about noise\nB14a\nThe Government's policy on noise is set out in...\nNaN\nNumber of complaints per year per local author...\nOHID, in collaboration with Chartered Institut...\nData on noise complaints provided by Chartered...\nNumber of complaints about noise.\nChartered Institute of Environmental Health (C...\n...\nNaN\nhttp://www.cieh.org/\nNaN\nNaN\nNaN\nper 1,000\nCrude rate\nFinancial\nRAG - Low is good\n11/04/2025\n\n\n1\n93754\nKilled and seriously injured casualties on Eng...\nB10\nMotor vehicle traffic accidents are a major ca...\nNaN\nNumber of people reported killed or seriously ...\nOHID, based on Department for Transport data\nNaN\nThe number of people of all ages killed or ser...\nDepartment for Transport (DfT), Road accidents...\n...\nKilled and seriously injured data are National...\n&lt;span style=\"color: #0b0c0c; font-family: GDS ...\nNaN\nPeople killed or seriously injured on roads\nRate of people reported killed or seriously in...\nper billion vehicle miles\nCrude rate\nCalendar\nRAG - Low is good\n22/10/2025\n\n\n2\n93074\nAccess to Healthy Assets & Hazards Index\nNaN\nThe Access to Healthy Assets and Hazards (AHAH...\nNaN\nPercentage of the population who live in LSOAs...\nConsumer Data Research Centre\nThe overall index collates data from a variety...\nTotal population residing in Lower Super Outpu...\nConsumer Data Research Centre (CDRC), Access t...\n...\nNaN\nhttps://data.cdrc.ac.uk/dataset/access-healthy...\nNaN\nNaN\nNaN\n%\nProportion\nCalendar\nRAG - Low is good\n03/09/2024\n\n\n3\n90357\nThe percentage of the population exposed to ro...\nB14b\nThere are a number of direct and indirect link...\nNaN\nNoise exposure determined by strategic noise m...\nOHID, based on Department for Environment, Foo...\nNaN\nNoise exposure determined by strategic noise m...\nDepartment for Environment, Food and Rural Aff...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n%\nProportion\nCalendar\nRAG - Low is good\n23/01/2025\n\n\n4\n94124\nFast food outlets per 100,000 population\nNaN\n&lt;p class=\"MsoNormal\" style=\"margin-bottom: 0cm...\nNaN\n&lt;p class=\"MsoNormal\" style=\"margin-bottom: 0cm...\nOHID, based on Food Standards Agency data\nNaN\n&lt;span style=\"line-height: 107%; mso-fareast-fo...\nFood Standards Agency (FSA), Food Hygiene Rati...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nper 100,000\nCrude rate\nCalendar\nRAG - Low is good\n24/01/2025\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n\nGet metadata by indicator ID\n\nfast_food_md = ftp.get_metadata_for_indicator_as_dataframe(94124)\n\nfast_food_md\n\n\n\n\n\n\n\n\nIndicator ID\nIndicator\nIndicator number\nRationale\nSpecific rationale\nDefinition\nData source\nIndicator source\nDefinition of numerator\nSource of numerator\n...\nData re-use\nLinks\nIndicator Content\nSimple Name\nSimple Definition\nUnit\nValue type\nYear type\nPolarity\nDate updated\n\n\n\n\n0\n94124\nFast food outlets per 100,000 population\nNaN\n&lt;p class=\"MsoNormal\" style=\"margin-bottom: 0cm...\nNaN\n&lt;p class=\"MsoNormal\" style=\"margin-bottom: 0cm...\nOHID, based on Food Standards Agency data\nNaN\n&lt;span style=\"line-height: 107%; mso-fareast-fo...\nFood Standards Agency (FSA), Food Hygiene Rati...\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nper 100,000\nCrude rate\nCalendar\nRAG - Low is good\n24/01/2025\n\n\n\n\n1 rows × 32 columns\n\n\n\n\n\nGet all area types\nThis can help you understand which geographical breakdowns are available via the API. The code below just returns the first 10 so that a large table doesn’t get displayed on this page, but there are many more. Worth looking into, if you are planning to make use of these metrics to support the narrative in a report you are working on.\n\n# returns a nested dictionary, where the area ID is the key to a further dictionary of key:value pairs\nall_areas_dict = ftp.metadata.get_all_areas()\n\n# converts the nested dictionary to a DataFrame, where the outer key becomes the index\nall_areas_df = pd.DataFrame.from_dict(all_areas_dict, orient='index') \n\n# reset the DataFrame to have the default indexing\nall_areas_df = all_areas_df.reset_index() \n\n# rename the column labelled 'index' in the dictionary-to-DataFrame conversion\nall_areas_df.rename(columns={'index':'Id'}, inplace=True) \n\nall_areas_df.head(10)\n\n\n\n\n\n\n\n\nId\nName\nShort\nClass\nSequence\nCanBeDisplayedOnMap\n\n\n\n\n0\n3\nMiddle Super Output Area\nMSOA\nNone\n0\nTrue\n\n\n1\n6\nGovernment Office Region (E12)\nRegions (statistical)\nNone\n0\nTrue\n\n\n2\n7\nGeneral Practice\nGPs\ngp\n0\nTrue\n\n\n3\n8\nElectoral Best Fit Wards (2024)\nElectoral Wards (2024)\nNone\n0\nTrue\n\n\n4\n9\nCensus Merged Wards\nMerged Wards\nNone\n0\nFalse\n\n\n5\n14\nAcute Trust\nAcute Trust\nNone\n0\nFalse\n\n\n6\n15\nEngland\nEngland\nNone\n0\nFalse\n\n\n7\n20\nMental Health Trust\nMental Health Trust\nNone\n0\nFalse\n\n\n8\n41\nAmbulance Trust\nAmbulance Trust\nNone\n0\nFalse\n\n\n9\n66\nSub-ICB, former CCGs\nICB sub-locations\nNone\n2022\nTrue\n\n\n\n\n\n\n\n\n\nGet all area types available for a particular profile\n\nwdoh_areas = ftp.metadata.get_area_types_for_profile(wdoh_profile_id)\n\nwdoh_areas_df = pd.DataFrame.from_dict(wdoh_areas, orient='index')\n\nwdoh_areas_df.reset_index(inplace=True)\n\nwdoh_areas_df.drop('index', axis=1,inplace=True) # drop the column called 'index' following the dictionary-to-DataFrame conversion\n\nwdoh_areas_df\n\n\n\n\n\n\n\n\nName\nShort\nClass\nCanBeDisplayedOnMap\nSequence\nId\n\n\n\n\n0\nGovernment Office Region (E12)\nRegions (statistical)\nNone\nTrue\n0\n6\n\n\n1\nEngland\nEngland\nNone\nFalse\n0\n15\n\n\n2\nLower tier local authorities (post 4/23)\nDistricts & UAs (from Apr 2023)\nua-la-composite\nTrue\n2023\n501\n\n\n3\nUpper tier local authorities (post 4/23)\nCounties & UAs (from Apr 2023)\nua-county-composite\nTrue\n2023\n502\n\n\n\n\n\n\n\n\n\nGet all data for indicators\nThere seems to be an error within this function at time of writing since it constructs an invalid URL. It has been kept in this demonstration so that you can be made aware that this method may not work, unless it has been fixed.\nftp.retrieve_data.get_all_data_for_indicators\n\nwdoh_indicators_df = pd.DataFrame(\n    ftp.retrieve_data.get_all_data_for_indicators(\n        [11404,92772],                  # two Wider Determinants of Health indicators\n         502,                           # Area type = Upper tier local authorities\n         15,                            # Parent area type =  England\n         filter_by_area_codes= None,\n         is_test= False\n    )\n)\n\nwdoh_indicators_df.head()\n\n\nLet’s try this one instead\nThis one does much the same thing, though slightly differently, while not causing an error.\nfingertips_py.retrieve_data.get_data_by_indicator_ids\n\nwdoh_indicators_df = pd.DataFrame(\n    ftp.retrieve_data.get_data_by_indicator_ids(\n        [11404,92772],                          # two Wider Determinants of Health indicators\n         502,                                   # Area type = Upper tier local authorities\n         15,                                    # Parent area type =  England\n         include_sortable_time_periods=None,\n         is_test= False\n    )\n)\n\nwdoh_indicators_df.head()\n\n\n\n\n\n\n\n\nIndicator ID\nIndicator Name\nParent Code\nParent Name\nArea Code\nArea Name\nArea Type\nSex\nAge\nCategory Type\n...\nCount\nDenominator\nValue note\nRecent Trend\nCompared to England value or percentiles\nCompared to percentiles\nTime period Sortable\nNew data\nCompared to goal\nTime period range\n\n\n\n\n0\n92772\nPremises licensed to sell alcohol per square k...\nNaN\nNaN\nE92000001\nEngland\nEngland\nNot applicable\nNot applicable\nNaN\n...\n154487.0\n130310.0136\nAggregated from all known lower geography values\nNaN\nNot compared\nNot compared\n20150000\nNaN\nNaN\n1y\n\n\n1\n92772\nPremises licensed to sell alcohol per square k...\nE92000001\nEngland\nE92000001\nEngland\nEngland\nNot applicable\nNot applicable\nNaN\n...\n154487.0\n130310.0136\nAggregated from all known lower geography values\nNaN\nSimilar\nNot compared\n20150000\nNaN\nNaN\n1y\n\n\n2\n92772\nPremises licensed to sell alcohol per square k...\nE92000001\nEngland\nE06000001\nHartlepool\nCounties & UAs (from Apr 2023)\nNot applicable\nNot applicable\nNaN\n...\n312.0\n93.5595\nNaN\nNaN\nWorse\nNot compared\n20150000\nNaN\nNaN\n1y\n\n\n3\n92772\nPremises licensed to sell alcohol per square k...\nE92000001\nEngland\nE06000002\nMiddlesbrough\nCounties & UAs (from Apr 2023)\nNot applicable\nNot applicable\nNaN\n...\n347.0\n53.8888\nNaN\nNaN\nWorse\nNot compared\n20150000\nNaN\nNaN\n1y\n\n\n4\n92772\nPremises licensed to sell alcohol per square k...\nE92000001\nEngland\nE06000003\nRedcar and Cleveland\nCounties & UAs (from Apr 2023)\nNot applicable\nNot applicable\nNaN\n...\n361.0\n244.8202\nNaN\nNaN\nWorse\nNot compared\n20150000\nNaN\nNaN\n1y\n\n\n\n\n5 rows × 27 columns\n\n\n\n\n\n\nGet data at all available geographies for a particular indicator\n\nlicensed_premises_all_geographies = ftp.retrieve_data.get_data_for_indicator_at_all_available_geographies(92772)\n\nlicensed_premises_all_geographies.head()\n\n\n\n\n\n\n\n\nIndicator ID\nIndicator Name\nParent Code\nParent Name\nArea Code\nArea Name\nArea Type\nSex\nAge\nCategory Type\n...\nCount\nDenominator\nValue note\nRecent Trend\nCompared to England value or percentiles\nCompared to percentiles\nTime period Sortable\nNew data\nCompared to goal\nTime period range\n\n\n\n\n0\n92772\nPremises licensed to sell alcohol per square k...\nNaN\nNaN\nE92000001\nEngland\nEngland\nNot applicable\nNot applicable\nNaN\n...\n154487.0\n130310.0136\nAggregated from all known lower geography values\nNaN\nNot compared\nNot compared\n20150000\nNaN\nNaN\n1y\n\n\n1\n92772\nPremises licensed to sell alcohol per square k...\nE92000001\nEngland\nE92000001\nEngland\nEngland\nNot applicable\nNot applicable\nNaN\n...\n154487.0\n130310.0136\nAggregated from all known lower geography values\nNaN\nSimilar\nNot compared\n20150000\nNaN\nNaN\n1y\n\n\n2\n92772\nPremises licensed to sell alcohol per square k...\nE92000001\nEngland\nE06000001\nHartlepool\nCounties & UAs (from Apr 2023)\nNot applicable\nNot applicable\nNaN\n...\n312.0\n93.5595\nNaN\nNaN\nWorse\nNot compared\n20150000\nNaN\nNaN\n1y\n\n\n3\n92772\nPremises licensed to sell alcohol per square k...\nE92000001\nEngland\nE06000002\nMiddlesbrough\nCounties & UAs (from Apr 2023)\nNot applicable\nNot applicable\nNaN\n...\n347.0\n53.8888\nNaN\nNaN\nWorse\nNot compared\n20150000\nNaN\nNaN\n1y\n\n\n4\n92772\nPremises licensed to sell alcohol per square k...\nE92000001\nEngland\nE06000003\nRedcar and Cleveland\nCounties & UAs (from Apr 2023)\nNot applicable\nNot applicable\nNaN\n...\n361.0\n244.8202\nNaN\nNaN\nWorse\nNot compared\n20150000\nNaN\nNaN\n1y\n\n\n\n\n5 rows × 27 columns\n\n\n\n\nLet’s check whether it has included all geographies by getting a list of unique area types\n\nset(licensed_premises_all_geographies['Area Type'])\n\n{'Counties & UAs (2021/22-2022/23)',\n 'Counties & UAs (from Apr 2023)',\n 'Districts & UAs (2021/22-2022/23)',\n 'Districts & UAs (from Apr 2023)',\n 'England',\n 'Regions (statistical)'}\n\n\n\n\n\nLet’s put all this together and visualise it on a map\nWe will need to import some more packages:\n\nGeopandas: supports combining geographical data with pandas DataFrames.\nContextily: used for laying maps over geographical shapes, such as maps provided by OpenStreetMap.\nMatplotlib: user for applying shading to geographical shapes, adding a legend and any other useful chart elements.\n\n\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport contextily as cx\n\nLet’s plot “Child Poverty, Income deprivation affecting children index (IDACI) 2019 Proportion %” in Bracknell Forest Local Authority area by MSOA. Let’s imagine we found this metric on the Fingertips website and want to import the data so that we can create a custom visualisation to go into some background narrative to a report.\nFirstly, we need the profile ID for the Profile “Local health, public health data for small geographic areas”. Then we can get the profile metadata as a dataframe so that we can see how the metric we are interested is referred to within the dataset.\n\nsmall_geographic_areas_md = ftp.metadata.get_profile_by_name('Local health, public health data for small geographic areas')\n\n# access the profile ID in the metadata dictionary\nsmall_geographic_areas_id = small_geographic_areas_md['Id']\n\n# get a metadata dataframe for the profile in question\nsmall_geographic_areas_md_df = ftp.metadata.get_metadata_for_profile_as_dataframe(small_geographic_areas_id)\n\n# get the indicator ID and name where the name contains \"IDACI\"\nidaci = small_geographic_areas_md_df[['Indicator ID','Indicator']][small_geographic_areas_md_df['Indicator'].str.contains('IDACI')]\n\n# inspect the metrics in our \"idaci\" variable\nidaci\n\n\n\n\n\n\n\n\nIndicator ID\nIndicator\n\n\n\n\n6\n93094\nChildren in poverty: Income Deprivation Affect...\n\n\n\n\n\n\n\nThere’s only one indicator with “IDACI” in the name, so we can go ahead and use the indicator ID to get the data for the metric we are interested in at MSOA level.\n\nidaci_id = idaci['Indicator ID'].iloc[0] # gets the entry from the Indicator ID column\n\nidaci_df = pd.DataFrame(\n    ftp.retrieve_data.get_data_by_indicator_ids(\n        idaci_id,\n        3, # MSOA\n        501 # Local Authority\n    )\n)\n\nidaci_df.head()\n\n\n\n\n\n\n\n\nIndicator ID\nIndicator Name\nParent Code\nParent Name\nArea Code\nArea Name\nArea Type\nSex\nAge\nCategory Type\n...\nCount\nDenominator\nValue note\nRecent Trend\nCompared to England value or percentiles\nCompared to Districts & UAs (from Apr 2023) value or percentiles\nTime period Sortable\nNew data\nCompared to goal\nTime period range\n\n\n\n\n0\n93094\nChildren in poverty: Income Deprivation Affect...\nNaN\nNaN\nE92000001\nEngland\nEngland\nPersons\n&lt;16 yrs\nNaN\n...\n1777641.757\n10405050\nNaN\nCannot be calculated\nNot compared\nNot compared\n20190000\nNaN\nNaN\n1y\n\n\n1\n93094\nChildren in poverty: Income Deprivation Affect...\nE92000001\nEngland\nE06000001\nHartlepool\nDistricts & UAs (from Apr 2023)\nPersons\n&lt;16 yrs\nNaN\n...\n4978.719\n17580\nNaN\nCannot be calculated\nWorse\nNot compared\n20190000\nNaN\nNaN\n1y\n\n\n2\n93094\nChildren in poverty: Income Deprivation Affect...\nE92000001\nEngland\nE06000002\nMiddlesbrough\nDistricts & UAs (from Apr 2023)\nPersons\n&lt;16 yrs\nNaN\n...\n9359.504\n28622\nNaN\nCannot be calculated\nWorse\nNot compared\n20190000\nNaN\nNaN\n1y\n\n\n3\n93094\nChildren in poverty: Income Deprivation Affect...\nE92000001\nEngland\nE06000003\nRedcar and Cleveland\nDistricts & UAs (from Apr 2023)\nPersons\n&lt;16 yrs\nNaN\n...\n6195.212\n24210\nNaN\nCannot be calculated\nWorse\nNot compared\n20190000\nNaN\nNaN\n1y\n\n\n4\n93094\nChildren in poverty: Income Deprivation Affect...\nE92000001\nEngland\nE06000004\nStockton-on-Tees\nDistricts & UAs (from Apr 2023)\nPersons\n&lt;16 yrs\nNaN\n...\n7965.449\n38168\nNaN\nCannot be calculated\nWorse\nNot compared\n20190000\nNaN\nNaN\n1y\n\n\n\n\n5 rows × 27 columns\n\n\n\nNow we can import the data for the geographic mapping. For this we will use a .geojson boundary shape file downloaded from the Open Geography Portal.\n\ngdf = gpd.read_file('data/MSOA_Dec_2011_Boundaries_Super_Generalised_Clipped_BSC_EW_V3_2022_-5254045062471510471.geojson')\n\ngdf.head()\n\n\n\n\n\n\n\n\nOBJECTID\nMSOA11CD\nMSOA11NM\nMSOA11NMW\nBNG_E\nBNG_N\nLONG\nLAT\nGlobalID\ngeometry\n\n\n\n\n0\n1\nE02000001\nCity of London 001\nCity of London 001\n532378\n181354\n-0.093570\n51.51560\na9f03568-7a0a-42b8-a23e-1271f76431e1\nPOLYGON ((-0.08519 51.52034, -0.07845 51.52151...\n\n\n1\n2\nE02000002\nBarking and Dagenham 001\nBarking and Dagenham 001\n548267\n189693\n0.138759\n51.58659\nf0ca54f0-1a1e-4c72-8fcb-85e21be6de79\nPOLYGON ((0.14984 51.59701, 0.15111 51.58708, ...\n\n\n2\n3\nE02000003\nBarking and Dagenham 002\nBarking and Dagenham 002\n548259\n188522\n0.138150\n51.57607\n3772a2ec-b052-4000-b62b-2c85ac401a7f\nPOLYGON ((0.14841 51.58075, 0.14978 51.5697, 0...\n\n\n3\n4\nE02000004\nBarking and Dagenham 003\nBarking and Dagenham 003\n551004\n186418\n0.176830\n51.55644\n3388e1f6-e578-4907-b271-168756f05856\nPOLYGON ((0.19021 51.55268, 0.18602 51.54754, ...\n\n\n4\n5\nE02000005\nBarking and Dagenham 004\nBarking and Dagenham 004\n548733\n186827\n0.144269\n51.56071\n1af0aed4-60d0-4fd6-b326-4b868968c12f\nPOLYGON ((0.15441 51.56607, 0.1479 51.56109, 0...\n\n\n\n\n\n\n\nNext, we join it to the Fingertips data so that the metric data and the geographical data are all in one dataframe.\n\nboundary_df = pd.merge(\n    left= gdf,\n    right= idaci_df,\n    left_on= 'MSOA11CD',\n    right_on= 'Area Code',\n    how= 'right' \n)\n\nboundary_df = boundary_df[boundary_df['Parent Name'] == 'Bracknell Forest'] # just get data where the parent area (i.e. local authority) is Bracknell Forest\n\nboundary_df.head()\n\n\n\n\n\n\n\n\nOBJECTID\nMSOA11CD\nMSOA11NM\nMSOA11NMW\nBNG_E\nBNG_N\nLONG\nLAT\nGlobalID\ngeometry\n...\nCount\nDenominator\nValue note\nRecent Trend\nCompared to England value or percentiles\nCompared to Districts & UAs (from Apr 2023) value or percentiles\nTime period Sortable\nNew data\nCompared to goal\nTime period range\n\n\n\n\n3497\n3263.0\nE02003352\nBracknell Forest 001\nBracknell Forest 001\n491853.0\n172163.0\n-0.67980\n51.44102\n3c549766-9728-4db5-b739-7c766b7413c1\nPOLYGON ((-0.65676 51.46149, -0.65822 51.44932...\n...\n34.968\n1018\nNaN\nCannot be calculated\nBetter\nBetter\n20190000\nNaN\nNaN\n1y\n\n\n3498\n3264.0\nE02003353\nBracknell Forest 002\nBracknell Forest 002\n486081.0\n171506.0\n-0.76298\n51.43602\nae2b8239-e38a-4f1c-90fe-f290fbc538e8\nPOLYGON ((-0.75275 51.46223, -0.74107 51.45887...\n...\n64.414\n1752\nNaN\nCannot be calculated\nBetter\nBetter\n20190000\nNaN\nNaN\n1y\n\n\n3499\n3265.0\nE02003354\nBracknell Forest 003\nBracknell Forest 003\n487683.0\n170212.0\n-0.74026\n51.42414\n31f3071a-3138-4059-b9cd-279441b78d10\nPOLYGON ((-0.72071 51.41979, -0.72078 51.41927...\n...\n117.612\n2124\nNaN\nCannot be calculated\nBetter\nBetter\n20190000\nNaN\nNaN\n1y\n\n\n3500\n3266.0\nE02003355\nBracknell Forest 004\nBracknell Forest 004\n486030.0\n169758.0\n-0.76414\n51.42031\n74e3802a-22fa-476d-9574-9987b0adabb0\nPOLYGON ((-0.75417 51.41869, -0.76013 51.41353...\n...\n234.014\n1515\nNaN\nCannot be calculated\nSimilar\nWorse\n20190000\nNaN\nNaN\n1y\n\n\n3501\n3267.0\nE02003356\nBracknell Forest 005\nBracknell Forest 005\n490476.0\n167562.0\n-0.70078\n51.39988\n3905ed89-433e-4322-8e30-e7c16404ebe1\nPOLYGON ((-0.68535 51.40142, -0.66763 51.38457...\n...\n46.210\n1470\nNaN\nCannot be calculated\nBetter\nBetter\n20190000\nNaN\nNaN\n1y\n\n\n\n\n5 rows × 37 columns\n\n\n\nFinally, we can plot the data onto OpenStreetMap using the contextily package.\nmatplotlib.pyplot plots the boundary shapes coloured by percentage using the polygon geometry data from the geopandas dataframe.\n\nax = boundary_df.plot(column= 'Value', legend=True, alpha = 0.6)\n\nax.axis('off')\nax.set_title('Percentage of Children Affected by Poverty in Bracknell Forest')\n\ncx.add_basemap(\n    ax,\n    crs=boundary_df.crs.to_string()\n)"
  },
  {
    "objectID": "intermediate-skills-sessions/17-quarto-intro/slides.html#from-publishing-in-quarto",
    "href": "intermediate-skills-sessions/17-quarto-intro/slides.html#from-publishing-in-quarto",
    "title": "An Introduction to Quarto",
    "section": "From publishing in “Quarto”",
    "text": "From publishing in “Quarto”\nThe word Quarto was originally a print publishing format where each page would have been a quarter the print size (depending on margin and trim point).\n\n\n\n\nHow to print in Quarto.\n\n\n\n\n\n\nQuarto document from the front as it would be printed.\n\n\n\n\n\n\nQuarto sheet as it would be read"
  },
  {
    "objectID": "intermediate-skills-sessions/17-quarto-intro/slides.html#to-publishing-in-quarto",
    "href": "intermediate-skills-sessions/17-quarto-intro/slides.html#to-publishing-in-quarto",
    "title": "An Introduction to Quarto",
    "section": "To publishing in Quarto",
    "text": "To publishing in Quarto\n\nThe Quarto we are referring to today is a system for publishing documents into a number of common digital formats though it can also be optimised to allow for designing print publications.\nBuilt as the evolution of R Markdown, it allows for a multi language system that supports code in Python, R, JavaScript and Julia.\nQuarto allows users to publish reporting from a combination of MarkDown and Code to numerous formats including MS Word, HTML, ePub, Revealjs, MS PowerPoint, and many more"
  },
  {
    "objectID": "intermediate-skills-sessions/17-quarto-intro/slides.html#quarto-websites",
    "href": "intermediate-skills-sessions/17-quarto-intro/slides.html#quarto-websites",
    "title": "An Introduction to Quarto",
    "section": "Quarto Websites",
    "text": "Quarto Websites\nOur very own website is built pretty much entirely using Quarto hosted from GitHub, it can also be used for personal blogs such as that of our very own Paul Johnson.\n\n\n\n\nCode Club\n\n\n\n\n\n\nPaul’s Blog"
  },
  {
    "objectID": "intermediate-skills-sessions/17-quarto-intro/slides.html#quarto-books-and-dashboards",
    "href": "intermediate-skills-sessions/17-quarto-intro/slides.html#quarto-books-and-dashboards",
    "title": "An Introduction to Quarto",
    "section": "Quarto books and Dashboards",
    "text": "Quarto books and Dashboards\nBeyond websites you can also use Quarto to publish to other formats. Here’s two such examples, an e-book and a shiny dashboard about penguins:\n\n\n\n\nA dashboard on Penguins\n\n\n\n\n\n\nTelling Stories With Data"
  },
  {
    "objectID": "intermediate-skills-sessions/17-quarto-intro/slides.html#final-thoughts",
    "href": "intermediate-skills-sessions/17-quarto-intro/slides.html#final-thoughts",
    "title": "An Introduction to Quarto",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n\nDon’t worry about memorising any of this!\nThe chief purpose of this session is to give you an idea of what is possible.\nYou can always come back later or ask for help via the “Code Help” channel or by contacting SAT directly."
  },
  {
    "objectID": "intermediate-skills-sessions/19-introduction-to-machine-learning/index.html",
    "href": "intermediate-skills-sessions/19-introduction-to-machine-learning/index.html",
    "title": "Introduction to Machine Learning",
    "section": "",
    "text": "This is the first session in our machine learning series. This session will focus on introducing machine learning conceptually. We will first discuss what machine learning is and why it is so powerful, then build a simple model to predict passenger survival on the Titanic. The goal is not to come away from this session with a complete, thorough understanding of machine learning. Instead, the goal is to understand what machine learning brings to the table."
  },
  {
    "objectID": "intermediate-skills-sessions/19-introduction-to-machine-learning/index.html#slides",
    "href": "intermediate-skills-sessions/19-introduction-to-machine-learning/index.html#slides",
    "title": "Introduction to Machine Learning",
    "section": "Slides",
    "text": "Slides\nUse the left ⬅️ and right ➡️ arrow keys to navigate through the slides below. To view in a separate tab/window, follow this link."
  },
  {
    "objectID": "intermediate-skills-sessions/19-introduction-to-machine-learning/index.html#what-is-machine-learning",
    "href": "intermediate-skills-sessions/19-introduction-to-machine-learning/index.html#what-is-machine-learning",
    "title": "Introduction to Machine Learning",
    "section": "What is Machine Learning?",
    "text": "What is Machine Learning?\nIn analytics and data science, tasks often focus on explaining why something happened or predicting what will happen in the future. Both of these tasks are complex, but for different reasons. Predicting the future is difficult because data can only tell us about the past. There are a number of ways we can approach this problem, but in the NHS, we often rely on dashboards that describe what has happened in the past to inform decisions about the future. This is inherently limited because many factors will cause the future to differ from the past, and decision-making based on descriptions of the past relies heavily on human judgment.\nWe can improve on human judgment using rules-based systems, which define explicit rules (IF condition THEN action) to generate predictions1. This can be effective in simple contexts where the rules are already known and easily quantified, but struggle with any complexity. Another approach to making decisions about the future more robust is statistical modelling; however, traditional statistical methods can struggle with large numbers of variables, complex and unknown interactions, and predictions that need to be automated and applied at scale.\nThis is where machine learning comes in. Machine learning is an approach to building systems that learn patterns from data to make predictions about the future. Fundamentally, machine learning is a blend of statistics and software development, leveraging statistical methods and applying software development principles to solve the problems of complexity and scale. The result is systems that are highly effective at predicting outcomes, can be applied at scale, and deployed in the real world to have a meaningful impact on our decisions about the future.\nMachine learning can be applied to a wide variety of contexts and can work with all types of data, structured and unstructured2. In healthcare, there are lots of examples where machine learning can be very effective:\n\nReadmission Prediction - Which patients need follow-up care?\nDiagnostic Support - Which patients need further testing?\nResource Allocation - How many beds needed next week?\nTreatment Personalisation - Which intervention works for whom?"
  },
  {
    "objectID": "intermediate-skills-sessions/19-introduction-to-machine-learning/index.html#predicting-survival-on-the-titanic",
    "href": "intermediate-skills-sessions/19-introduction-to-machine-learning/index.html#predicting-survival-on-the-titanic",
    "title": "Introduction to Machine Learning",
    "section": "Predicting Survival on the Titanic",
    "text": "Predicting Survival on the Titanic\nWe will use a classic machine learning dataset, passenger data from the Titanic, to demonstrate what a simple model looks like and how machine learning works. The task is to predict passenger survival. This is a type of task called supervised learning, where you train a model on data where the outcome is already known and labelled, in order to predict answers on new data for which the outcome is not known.\nFor example, predicting future readmissions from patient data that has an outcome variable that tells us whether patients were readmitted or not is a supervised learning task.\nThe basic machine learning workflow, which will be demonstrated below, looks as follows:\n\nGet data with examples and labels\nSplit into training and testing sets\nTrain a model on training data\nPredict on test data\nEvaluate how well it performed\n\n\nSetup\nFirst, let’s import the libraries we need, and the titanic dataset.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n\n# set random seed for reproducibility\nnp.random.seed(42)\n\n\n# load the titanic dataset\ndf = sns.load_dataset('titanic')\n\n\n\nData Exploration\nBefore building our model we should explore the data to help us make decisions about how to transform the data so it is ready for modelling and how to structure our model.\nIn practice, the exploratory phase should be much more detailed, but here we will keep it simple and focus on demonstrating the basic machine learning workflow.\n\n# inspect first few rows\ndf.head()\n\n\n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n\n\n\n\n\n\n# check data shape (rows, columns)\ndf.shape\n\n(891, 15)\n\n\n\n# count missing values\ndf.isnull().sum()\n\nsurvived         0\npclass           0\nsex              0\nage            177\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           688\nembark_town      2\nalive            0\nalone            0\ndtype: int64\n\n\nThere are several columns that have a lot of missing values. If we use any of these variables (often called features in machine learning) in our model, we will need to deal with these nulls.\nIt’s also important to check the distribution of the outcome, to identify significant imbalance.\n\n# count survivors vs non-survivors\ndf['survived'].value_counts()\n\nsurvived\n0    549\n1    342\nName: count, dtype: int64\n\n\nWe can also calculate the rate of survival to build a simple baseline to compare our model against.\n\n# calculate survival rate\nprint(f\"Survival rate: {df['survived'].mean():.1%}\")\n\nSurvival rate: 38.4%\n\n\nIf we always predicted that passengers did not survive, we’d be right about 62% of the time. Our model needs to be able to beat this to add value.\n\n\nData Preparation\nTo keep the model simple, we will use only four features: passenger class, sex, age, and fare.\nWe will first split our data up into these four features (X) and the outcome, passenger survival (y), and then we will split X and y into our training and testing data.\nThis is a critical step in a machine learning workflow because it creates a process for honest evaluation of the model’s performance. The model uses the training data to learn patterns that help it predict outcomes, and the test data is used to check how the model performs on data it hasn’t seen in the training process.\n\nX = df[['pclass', 'sex', 'age', 'fare']]\ny = df['survived']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\nSeveral of the features in our model need to be transformed so that the model can use them. We will write a function that fills the missing values in the age column with the median age3, and converts the sex column to integer type. We then apply this function to both the training and testing features.\n\ndef prepare_features(data):\n\n    # fill missing age values with the median age\n    data['age'] = data['age'].fillna(data['age'].median())\n    # convert sex to integer (male = 0, female = 1) \n    data['sex'] = (data['sex'] == 'female').astype(int)\n\n    return data\n\nX_train = prepare_features(X_train)\nX_test = prepare_features(X_test)\n\n\n\nModel Training\nLogistic regression is a simple, interpretable model which is good for binary classification, so that is what we will use here.\n\n# specify and train the model\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train, y_train)\n\nLogisticRegression(max_iter=1000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l2'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'lbfgs'\n\n\n\nmax_iter \n1000\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n\n            \n        \n    \n\n\nThe process for fitting a machine learning model is actually remarkably simple. It’s just two lines!\n\nPredictions\nHaving fit our model on the training data, we can then predict passenger survival in the test data, to see how our model performs on unseen data.\n\n# predict on test set\ny_pred = clf.predict(X_test)\n\n# look at first ten predictions vs actual\npd.DataFrame({\n    'Actual': y_test.values[:10],\n    'Predicted': y_pred[:10]\n})\n\n\n\n\n\n\n\n\nActual\nPredicted\n\n\n\n\n0\n1\n0\n\n\n1\n0\n0\n\n\n2\n0\n0\n\n\n3\n1\n1\n\n\n4\n1\n1\n\n\n5\n1\n1\n\n\n6\n1\n1\n\n\n7\n0\n0\n\n\n8\n1\n1\n\n\n9\n1\n1\n\n\n\n\n\n\n\nLooking at the first ten predictions, we can already see the model is not perfect, but it seems to be performing pretty well.\n\n\nModel Evaluation\nHaving generated predictions, we need to evaluate how well the model performed. There are lots of different metrics we can use to evaluate model performance. The simplest metric is accuracy4, which tells us what percentage of predictions were correct.\n\n# calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.1%}\")\n\nAccuracy: 79.5%\n\n\nOur model performs pretty well, and it is significantly better than the baseline we calculated earlier. But accuracy alone doesn’t tell the whole story.\nWe also need to consider the types of errors our model makes. False positives (predicting a passenger will survive when they won’t) and false negatives (predicting a passenger won’t survive when they will) have different costs and implications. Understanding which type of error is more costly in a given context helps us evaluate whether our model is fit for purpose.\n\n# generate confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# plot confusion matrix\nplt.figure(figsize=(12,6))\n\ndisp = ConfusionMatrixDisplay(cm)\ndisp.plot(cmap=plt.cm.Blues)\n\nplt.title(\"Confusion Matrix for Predicted Survival on the Titanic\")\nplt.show()\n\n&lt;Figure size 1152x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nThe confusion matrix above shows the breakdown of correct and incorrect predictions. The top-left box shows true negatives (correctly predicted did not survive), the bottom-right shows true positives (correctly predicted survived), and the off-diagonal boxes show our errors: false positives (top-right) and false negatives (bottom-left). This breakdown helps us understand not just how often our model is wrong, but how it’s wrong, which is important for deciding whether the model is good enough for real-world use.\nOur model has a slightly higher number of false negatives, which suggests it is undervaluing the probability of some passengers surviving. Our next steps would be understanding the observations where our model fails (perhaps it struggles with the rows that had missing age values?), and iterating on this workflow to improve our model’s performance.\n\n\n\nSummary\nIn this session, we’ve introduced machine learning conceptually and demonstrated a basic supervised learning workflow. Machine learning allows us to build systems that learn patterns from data to make predictions at scale, handling complexity that would overwhelm traditional rules-based systems or simple statistical approaches.\nThe workflow we followed is the foundation of most supervised learning tasks: get labeled data, split it into training and testing sets, train a model on the training data, make predictions on the test data, and evaluate performance. This split between training and testing is critical because it gives us an honest assessment of how the model performs on data it hasn’t seen before. We also looked at how to evaluate model performance. There are many ways we can build on this workflow to make it more robust and improve model performance.\nWhat we haven’t covered yet is how different models actually learn, why you might choose one model over another, and how to improve performance through feature engineering and hyperparameter tuning. We also haven’t explored other types of machine learning beyond supervised learning, or the full workflow that takes a model from development to deployment. These topics will be covered in the futures, building on the foundations we’ve laid out here."
  },
  {
    "objectID": "intermediate-skills-sessions/19-introduction-to-machine-learning/index.html#footnotes",
    "href": "intermediate-skills-sessions/19-introduction-to-machine-learning/index.html#footnotes",
    "title": "Introduction to Machine Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor example, a rules-based system that classifies patients by their risk-levels might identify a patient over the age of 65 with diabetes and apply a flag to their record that tells clinicians that the patient is high-risk.↩︎\nStructured data, also known as tabular data, is any data that can easily fit into a table, with columns representing variables and rows representing different observations.↩︎\nReplacing missing values with the mean or median, or dropping missing values entirely, is generally a bad strategy that can have a significant negative impact on your model. However, we are using the median here for the sake of simplicity.↩︎\nWe will look at other metrics and when we should use them in the future.↩︎"
  },
  {
    "objectID": "resources/glossary.html",
    "href": "resources/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "This page will have a list of definitions for commonly used (and/or commonly misunderstood) terminology and acronyms relating to python or data analysis and manipulation in general.\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n  Term\n  Definition\n\n\n\n  \n    Dependency\n    A package or library that is required by a Python program (or by another package) in order to function. For example, if your software interacts with a SQL server then it might have sqlalchemy as one of its dependencies.\n  \n  \n    Exploratory Data Analysis (EDA)\n    The process of analysing datasets to summarise their main characteristics, often using visual methods, before formal modeling.\n  \n  \n    Functional programming\n    A programming paradigm that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data.\n  \n  \n    Git\n    A distributed version control system used to track changes in source code during software development.\n  \n  \n    GitHub\n    A cloud-based platform that provides hosting for repositories of files and folders (usually of software code) using git as its backend for version control.\n  \n  \n    Integrated Development Environment (IDE)\n    A software application that provides tools for coding, such as a code editor, debugger, terminal, and build automation, within a single interface. It helps streamline and simplify the software development process. A popular example is VS Code.\n  \n  \n    Jupyter\n    A software package that allows the creation of python notebooks that can include a mixture of markdown-formatted text and live Python code.\n  \n  \n    Markdown (.md)\n    A simple plain-text markup scheme designed to allow for rapid production of formatted text (with headings, links, etc) within a plaintext file. Used by Quarto (as a Quarto-specific flavour with the .qmd extension).\n  \n  \n    matplotlib\n    The baseline Python library for creating static, animated, and interactive visualisations, offering extensive customisation options for plots and charts. Also used as a framework for more advanced or visually pleasing visualisation packages like seaborn.\n  \n  \n    numpy\n    A python library for working with numbers and doing science.\n  \n  \n    Object-oriented programming (OOP)\n    A programming paradigm based on the concept of \"objects,\" which can contain data and code to manipulate that data.\n  \n  \n    pandas\n    A python data analysis and manipulation library for working with dataframes (tabular data).\n  \n  \n    Python\n    A general-purpose programming language.\n  \n  \n    Regression\n    A method for modeling the relationship between one or more explanatory variables and an outcome. It is used to predict outcomes and understand the impact of changes in predictors (explanatory variables) on the response (outcome).\n  \n  \n    Repository (repo)\n    In git and github, a repository is a self-contained \"project\" of files and folders.\n  \n  \n    Reproducible Analytical Pipelines (RAP)\n    A set of processes and tools designed to ensure that data analysis can be consistently repeated and verified by others.\n  \n  \n    seaborn (sns)\n    A Python data visualisation library based on Matplotlib, providing a high-level interface for drawing attractive and informative statistical graphics.\n  \n  \n    skimpy\n    A python library for creating summary statistics from dataframes\n  \n  \n    sqlalchemy\n    A python library for doing SQL queries.\n  \n  \n    TOML\n    Tom's Obvious Minimal Language - simple, human-readable data serialisation format designed for configuration files, emphasizing readability and ease of use. Used by uv to specify its projects.\n  \n  \n    uv\n    A Python package manager, which can manage python projects (folders) and manage the installation and management of the python environment and libraries within that folder.\n  \n  \n    Virtual Environment (venv)\n    An isolated space where you (or an environment manager like uv) can install and manage Python packages in a self-contained way without affecting the system-wide Python setup.\n  \n  \n    YAML\n    Yet Another Markup Language - a human-readable data serialisation format often used for configuration files and data exchange between languages."
  },
  {
    "objectID": "sessions/07-control-flow/index.html",
    "href": "sessions/07-control-flow/index.html",
    "title": "An Introduction to Control Flow",
    "section": "",
    "text": "This session is the second in a series of programming fundamentals. We recognise that this content might be a bit more dry and abstract, but it is important background to know when you start to actually use Python in your day to day work.\nMuch as the flow of a stream describes how it goes from its source to its mouth, control flow describes the logical path a program is expected to take when you run it. Just as you can divert the flow of a stream with structures like dams and bridges, you can change the direction a program flows by the use of control and repetition structures. The below slides aim to provide an introduction to these concepts and the way we can use them.",
    "crumbs": [
      "Core Concepts",
      "7. Control Flow"
    ]
  },
  {
    "objectID": "sessions/07-control-flow/index.html#control-or-decision-structures",
    "href": "sessions/07-control-flow/index.html#control-or-decision-structures",
    "title": "An Introduction to Control Flow",
    "section": "Control (or Decision) Structures",
    "text": "Control (or Decision) Structures\nLike a case statement in SQL, control structures can be used to select different options and actions based on the input variable. These follow the structure:\n\nif &lt;this boolean condition is true&gt;:\n  &lt;do this&gt;\nelif &lt;this boolean condition is true&gt;:\n  &lt;do that&gt;\nelse:\n  &lt;do something else&gt;\n\nIn its most basic form, only an if clause is required. The else clause allows the bucketing of all circumstances not handled previously so that code can be applied in any given circumstance.",
    "crumbs": [
      "Core Concepts",
      "7. Control Flow"
    ]
  },
  {
    "objectID": "sessions/07-control-flow/index.html#repetition-structures-or-loops",
    "href": "sessions/07-control-flow/index.html#repetition-structures-or-loops",
    "title": "An Introduction to Control Flow",
    "section": "Repetition Structures (or Loops)",
    "text": "Repetition Structures (or Loops)\nRepetition structures allow for sections of code to be repeated until a condition is met. for loops repeat code over a set number of iterations based on an iterable condition. while loops repeat code until a predetermined condition is met.\n\nfor Loops\nBelow are two examples of for code loops. The first loops through a list called ‘providers’ and prints each item. The second loops through a range of numbers and prints each.\n\nLogical Structure\nfor &lt;i&gt; in &lt;iterable&gt;:\n    &lt;code_to_iterate&gt;\n\nfor &lt;i&gt; in range(&lt;a&gt; - &lt;b&gt;):\n  print(&lt;i&gt;)\n\n\nPython\nfor provider in providers:\n  print(provider)\n\nfor num in range(0-6):\n  print(num)\n\n\n\nwith Loops\nwhile loops check the state of a boolean condition. In this case the loop runs until a declared variable is over 5 printing each incremental value.\n\nLogical Structure\nwhile &lt;boolean is true&gt;:\n    &lt;code_to_iterate&gt;\n\n\nPython\nvar = 0\nwhile var &lt;= 5:\n  print(var)",
    "crumbs": [
      "Core Concepts",
      "7. Control Flow"
    ]
  },
  {
    "objectID": "sessions/07-control-flow/index.html#exercises",
    "href": "sessions/07-control-flow/index.html#exercises",
    "title": "An Introduction to Control Flow",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a function that prints whether a number is negative, zero, or positive.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\ndef classify(x):\n    if x &lt; 0:\n        print(\"Negative\")\n    elif x == 0:\n        print(\"Zero\")\n    else:\n        print(\"Positive\")\n\n\n\n\n\nLoop through a list of ages and print if each person is a Child (&lt;13), Teenager (13–17), Adult (18–64), or Senior (65+).\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nages = [10, 15, 30, 70]\nfor age in ages:\n    if age &lt; 13:\n        print(\"Child\")\n    elif age &lt; 18:\n        print(\"Teenager\")\n    elif age &lt; 65:\n        print(\"Adult\")\n    else:\n        print(\"Senior\")\n\nChild\nTeenager\nAdult\nSenior\n\n\n\n\n\n\nUse a while loop to count down from 10 to 0.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nx = 10\nwhile x &gt;= 0:\n    print(x)\n    x -= 1\n\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\n\n\n\n\n\n\nLoop from 1 to 20 and print Fizz for multiples of 3, Buzz for 5, FizzBuzz for both.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nfor i in range(1, 21):\n    if i % 3 == 0 and i % 5 == 0:\n        print(\"FizzBuzz\")\n    elif i % 3 == 0:\n        print(\"Fizz\")\n    elif i % 5 == 0:\n        print(\"Buzz\")\n    else:\n        print(i)\n\n1\n2\nFizz\n4\nBuzz\nFizz\n7\n8\nFizz\nBuzz\n11\nFizz\n13\n14\nFizzBuzz\n16\n17\nFizz\n19\nBuzz\n\n\n\n\n\n\nUse random.randint to simulate rolling a die until you get a 6.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nimport random\nrolls = 0\nwhile True:\n    rolls += 1\n    if random.randint(1, 6) == 6:\n        break\nprint(\"Rolled a 6 in\", rolls, \"tries\")\n\nRolled a 6 in 7 tries\n\n\n\n\n\n\nLoop through job titles and print if they contain “analyst” or “manager.”\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\ntitles = [\"Data Analyst\", \"HR Manager\", \"Intern\"]\nfor title in titles:\n    t = title.lower()\n    if \"analyst\" in t:\n        print(\"Analyst role\")\n    elif \"manager\" in t:\n        print(\"Manager role\")\n    else:\n        print(\"Other\")\n\nAnalyst role\nManager role\nOther",
    "crumbs": [
      "Core Concepts",
      "7. Control Flow"
    ]
  },
  {
    "objectID": "sessions/06-data-types/slides.html#a-brief-history-of-data-types",
    "href": "sessions/06-data-types/slides.html#a-brief-history-of-data-types",
    "title": "An Introduction To Data Types",
    "section": "A brief history of data types",
    "text": "A brief history of data types\n\n\nAll1 computers store data in binary (1s and 0s) – example shown on the right, represented as hexadecimal\nVariables add a level of convenience and abstraction by letting us name specific buckets to put data in, and data types give structure to these buckets.\nIn the early days of computing data was stored as raw binary\nThe need for specific data types came from the emergence of structured programming from the 1950s onward\nLanguages like FORTRAN and COBOL introduced the segregation of numeric datatypes and character types\nObject-oriented languages like C++ and Java further expanded on this with user-defined data types\nSpecifying the type of data allows the machine to allocate an appropriate amount of memory to it (was very important in the early days of computing, but still relevant)\nAllows us to prevent errors; setting the expectation on the exact type of data that a specific variable will contain.\n\n\n\n\n\nRaw data in hex format (ASCII representation on right).\n\n\n\n\n\nCore rope memory. More on this on the next slide. (Konstantin Lanzet, Wikimedia Commons)"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#history-lesson---core-rope-memory",
    "href": "sessions/06-data-types/slides.html#history-lesson---core-rope-memory",
    "title": "An Introduction To Data Types",
    "section": "History lesson - core rope memory",
    "text": "History lesson - core rope memory\n\nThe Apollo Guidance Computer for the Apollo programme which eventually landed the first person on the moon made use of core rope memory. The program code and fixed data (such as important physical and astronomical constant) were literally woven into a grid of magnetic round cores using a needle, with the sequence the wire took through the cores deciding the pattern of 0s and 1s. This highly technical work was done in bulk in factories by almost exclusively female workers.\n\n\n\nA closeup of a few cores in a core rope memory module, showing the hundreds of times the sense wire goes through each core2.\n\n\n\n\n\n\nA factory employee working on a core rope module3\n\n\n\n\n\nOne of the memory trays of the AGC - each rectangular module contains a self-contained core rope grid4"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#a-quick-note-on-type-systems",
    "href": "sessions/06-data-types/slides.html#a-quick-note-on-type-systems",
    "title": "An Introduction To Data Types",
    "section": "A quick note on type systems",
    "text": "A quick note on type systems\n\nProgramming languages have different philosophies. They are often referred as being “strong” or “weak” and “static” or “dynamic”.\nStrongly but dynamically-typed languages (e.g. Python)\n\nPython features dynamic typing. There is no need to explicitly declare variables as being a specific data type, and it does allow limited implicit conversions, but not as extensively as e.g. JavaScript.\n\nStatically-typed languages (C++, Rust, SQL)\n\nThe programmer has to specify the data type for a variable or object in the code itself and they are checked at compile time. Safer (catches errors early) and possibly more performant, but more tedious and less flexible\n\nWeakly-typed languages (e.g. Javascript)\n\nAllows extensive type coercion; mixing-and-matching of datatypes freely e.g. 5+“2”=“52”\n\n\n\n\n\nhttps://remotescout24.com/en/blog/806-typed-vs-untyped-programming-languages\n\n\n\n\n\nC++. This code generates a type error; we tried to assign a string value to an int\n\n\n\n\n\nJavaScript. This is valid JS code and ends with z being a string with the content “52”"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#overview",
    "href": "sessions/06-data-types/slides.html#overview",
    "title": "An Introduction To Data Types",
    "section": "Overview",
    "text": "Overview\n\n\n\nA logical overview of the basic data types in python. From https://pynative.com/python-data-types/"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#booleans",
    "href": "sessions/06-data-types/slides.html#booleans",
    "title": "An Introduction To Data Types",
    "section": "Booleans",
    "text": "Booleans\n\nLike most programming languages, python has a bool datatype. In some ways, this is the simplest type available. A Boolean can only be True or False, and is returned when evaluating an expression. For example:\nour_result = 10&gt;9 print(our_result)\nReturns True - we’re asking Python for the result of the comparison 10&gt;9, and to store this result in a variable called our_result. The data type of a true-false comparison result like that is bool, so our variable will also be of this type.\nBooleans will become highly relevant when we talk about conditionals and program flow.\n\n\n\n\nGeorge Boole (1815-1864) - the originator of Boolean logic"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#numeric-types",
    "href": "sessions/06-data-types/slides.html#numeric-types",
    "title": "An Introduction To Data Types",
    "section": "Numeric types",
    "text": "Numeric types\nNumeric types are for variables that will only contain numbers. Other programming languages often have many different numeric types, but Python (mercifully) only has two:\nint can contain any5 whole (no fraction or decimals) number; negative, positive or zero. E.g.\n\na = -4\nb = 3\nc = 9087358292578\n\nfloat can contain any number with a decimal point, to arbitrary6 precision. E,g,\n\nx = -2.2\ny = 3.0\nz = 2452.259259999999999\n\nIf you’re manually assigning a number to a variable, python will always choose an int or float depending on whether you’ve used a decimal point or not - so 2 and 2.0 are not equivalent in this context."
  },
  {
    "objectID": "sessions/06-data-types/slides.html#data-structures",
    "href": "sessions/06-data-types/slides.html#data-structures",
    "title": "An Introduction To Data Types",
    "section": "Data structures",
    "text": "Data structures\nWith data structures, we can address an element or elements by using square bracket notation - more on this below.\nStrings (str)\nThese are similar to a VARCHAR in SQL. They are ordered sequences (strings) of characters7. Enclosed by quotation marks8. E.g.\n\nour_string = \"Hello world\"\n\nLists (list)\nAn ordered sequence of objects, where each object can be another data type (int, float, string, bool, etc). Enclosed by square brackets, and the items separated by commas. E.g.\n\nour_list = [1, 2.3, \"abc\"]\n\nDictionaries (dict)\nDictionaries are key-value pairs, where each entry is a pair of entries. Enclosed by curly braces, the keys and values separated by a colon and each pair separated by a comma. E.g.\n\nour_dict = {\"org_code\":\"0DF\",\"name\":\"SCW CSU\",\"year\": 2013}"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#other-data-types",
    "href": "sessions/06-data-types/slides.html#other-data-types",
    "title": "An Introduction To Data Types",
    "section": "Other data types",
    "text": "Other data types\nBuilt-in\n\nWe’ve skipped over complex numbers and tuples, the latter being like a dict but non-changeable.\n\nOther packages\n\nYou may have heard of other data types such as arrays (which are kind like lists but multi-dimensional).\nArrays are not a built-in Python type but are offered by the numpy package.\npandas also offers additional data types such as timestamp (similar to SQL’s datetime).\ndataframes (from pandas) are an example of a higher-order class that makes use of datatypes within it; remember from previous sessions that a dataframe can contain strings, integers, timestamps etc."
  },
  {
    "objectID": "sessions/06-data-types/slides.html#final-thoughts",
    "href": "sessions/06-data-types/slides.html#final-thoughts",
    "title": "An Introduction To Data Types",
    "section": "Final thoughts",
    "text": "Final thoughts\nDon’t worry about memorising any of this! If you take one thing away from this session, make it the fact that data types exist, that being aware of them will help you understand problems with your code, and that resources and documentation are readily available online.\n\nFurther reading\n\nhttps://docs.python.org/3/tutorial/datastructures.html\nhttps://docs.python.org/3/library/stdtypes.html\nhttps://www.geeksforgeeks.org/python-data-types/\nhttps://www.w3schools.com/python/python_datatypes.asp"
  },
  {
    "objectID": "sessions/06-data-types/slides.html#footnotes",
    "href": "sessions/06-data-types/slides.html#footnotes",
    "title": "An Introduction To Data Types",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nexperimental ternary computers and quantum computing are firmly out of scope of this presentation\nfrom https://www.righto.com/2019/07/software-woven-into-wire-core-rope-and.html\nfrom https://www.righto.com/2019/07/software-woven-into-wire-core-rope-and.html\nfrom https://www.righto.com/2019/07/software-woven-into-wire-core-rope-and.html\nthere is no clearly-defined maximum number for an integer in python; certainly not one you’re likely to ever encounter\nagain, limits exist but aren’t relevant here\nletters, numbers, symbols, etc. - any valid UTF-8 symbols\nin most instances either double quotes (\") or single quotes (') are fine - but it’s a good idea to pick one style and be consistent."
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#learning-objectives",
    "href": "sessions/09-object-oriented-programming/slides.html#learning-objectives",
    "title": "Introduction to Object-Oriented Programming",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand core concepts of object-oriented programming\nUnderstand the benefits of object-oriented programming\nLearn how to create your own object classes (see the accompanying notebook)"
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#object-oriented-programming-what-is-this-tech-geekery-and-why-should-i-care",
    "href": "sessions/09-object-oriented-programming/slides.html#object-oriented-programming-what-is-this-tech-geekery-and-why-should-i-care",
    "title": "Introduction to Object-Oriented Programming",
    "section": "Object-Oriented Programming? What is this tech-geekery and why should I care?",
    "text": "Object-Oriented Programming? What is this tech-geekery and why should I care?\n\nPython is an object-oriented language. Every entity is treated as an object; even single integers are objects of the “int” class.\nAn understanding of object-oriented programming will help give you a better understanding of how the packages you use function.\nYou can use this understanding to create your own programs that harness the strengths of objected-oriented programming:\n\nConvenience\nFlexibility\nExtensibility\nSimpler interfacing"
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#history",
    "href": "sessions/09-object-oriented-programming/slides.html#history",
    "title": "Introduction to Object-Oriented Programming",
    "section": "History",
    "text": "History\n\n\nIn the early days of programming, variables could only be the “primitive” data types containing a single value\n\nInteger, Float, Boolean, Char\n\nLater came Structures (“Structs”), which can contain multiple values of different types.\nStructs were the precursor to objects, but they couldn’t yet contain associated functions within them.\nObjects first appear in the Simula programming language in the 1960s for modelling physical phenomena.\nThose objects influenced Alan Kay, who coined the term “object-oriented programming” to describe architecture where objects pass information to one another.\n\n\n\n\n\nMy. Kay looking very pleased with his coinage."
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#classes-and-objects",
    "href": "sessions/09-object-oriented-programming/slides.html#classes-and-objects",
    "title": "Introduction to Object-Oriented Programming",
    "section": "Classes and Objects",
    "text": "Classes and Objects\n\n\nClasses act as templates for objects\nObjects are referred to as “instances” of classes\n\nWe talk of objects being “instantiated” from a class\nThink of each object as being a copy created using the class template\n\nObjects represent entities with their own data (attributes) and behaviours (methods)\nWe can create lots of instances of an object with their own attribute values and call methods on them separately yet consistently\nObjects are self-contained units that can interact with objects both of the same and of other classes\n\n\n\n\n\nConsistency is the key."
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#inheritance",
    "href": "sessions/09-object-oriented-programming/slides.html#inheritance",
    "title": "Introduction to Object-Oriented Programming",
    "section": "Inheritance",
    "text": "Inheritance\n\n\nChild classes inherit attributes and methods from parent classes\nChild classes can modify / override and add to what they have inherited\nReduces code duplication; increases re-usability\nImproves extensibility: i.e. new classes with the same core behaviours, but new features, can be based on existing classes\n\n\n\n\n\nInheritance is a much less contentious issue in Python."
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#encapsulation-for-convenience",
    "href": "sessions/09-object-oriented-programming/slides.html#encapsulation-for-convenience",
    "title": "Introduction to Object-Oriented Programming",
    "section": "Encapsulation for convenience",
    "text": "Encapsulation for convenience\n\nBundling data (attributes) with functions (methods)\nMethods are tailor-made to work with the data contained in the object\nSaves on having to pass data between multiple functions, which is particularly useful in machine learning models\nPandas DataFrames demonstrate encapsulation. They contain data, but also have methods associated with them\n\ndf = pd.DataFrame(data) &lt;– Instantiating a dataframe object\ndf.head(), df.describe(), df.drop() &lt;– calling methods"
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#polymorphism-for-flexibility",
    "href": "sessions/09-object-oriented-programming/slides.html#polymorphism-for-flexibility",
    "title": "Introduction to Object-Oriented Programming",
    "section": "Polymorphism for flexibility",
    "text": "Polymorphism for flexibility\n\nObjects of different types can be treated in the same way, even if the behaviour differs\n\nWith Pandas DataFrames, .head() will work on both a DataFrame and a Series1\n\n“Duck typing”: If the behaviour of a thing matches that of another thing, they are considered the same. In OOP terms, the presence of certain methods is more important than which class an object comes from2\n\nThe scikit-learn library’s allows the same code to work for different models\n\n\nSeries.head() with return the first few values, while DataFrame.head() returns the first few rows of all columnsThe concept of “duck typing”, found in Python and other languages, comes from the phrase “if it walks like a duck, quacks like a duck and swims like a duck, then it’s a duck.”"
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#abstraction-for-simpler-interfacing",
    "href": "sessions/09-object-oriented-programming/slides.html#abstraction-for-simpler-interfacing",
    "title": "Introduction to Object-Oriented Programming",
    "section": "Abstraction for simpler interfacing",
    "text": "Abstraction for simpler interfacing\n\nSeparating the implementation code from the functionality that users (i.e. other programmers) interact with\nCreates a simple interface for parts of a program pass information between each other\nExamples:\n\nWhen working with machine learning models, users only need to apply simple methods to train the model and make a prediction.\nEntities interacting with each other within a simulation model."
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#when-to-use-oop",
    "href": "sessions/09-object-oriented-programming/slides.html#when-to-use-oop",
    "title": "Introduction to Object-Oriented Programming",
    "section": "When to use OOP",
    "text": "When to use OOP\n\nWhen you want to easily re-use code, to avoid repetition and to extend functionality\nDiscrete Event Simulations for modelling queueing / capacity problems\nCreating custom, branded visualisation packages, for example an NHS-branded SPC chart\n\nCreating a package that can be used to import the latest data from a website without users having to understand API calls or the website’s structure\n\nWhen you want to model real-world entities\n\nDiscrete Event Simulations for modelling queueing / capacity problems\n\nWhen you want to make code modular and easy for others to work with\nWhen you want to simplify end-users’ interaction with Python, fostering a self-service approach to analytics\n\nCreating custom, branded visualisation packages, for example an NHS-branded SPC chart\n\nLess appropriate for: When you want to be certain of the state of your data at each step of a process, for example when cleansing data"
  },
  {
    "objectID": "sessions/09-object-oriented-programming/slides.html#resources",
    "href": "sessions/09-object-oriented-programming/slides.html#resources",
    "title": "Introduction to Object-Oriented Programming",
    "section": "Resources",
    "text": "Resources\nRealPython: Object-Oriented Programming (OOP) in Python\n\nOOP produces code that is easy to read, extend and maintain\n\nHSMA’s Guide to Object-Oriented Programming\n\nHSMA’s Discrete Event Simulation Module"
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html",
    "href": "sessions/05-eda-seaborn/index.html",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "",
    "text": "This session will build on the previous session that introduced the Seaborn library, using it to visualise data and do some exploratory analysis.\nWe are using Australian weather data, taken from Kaggle. This dataset is used to build machine learning models that predict whether it will rain tomorrow, using data about the weather every day from 2007 to 2017. To download the data, click here.\nThe objective from this session is to:\n# import packages\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# control some deprecation warnings in seaborn\nwarnings.filterwarnings(\n    \"ignore\",\n    category=FutureWarning,\n    module=\"seaborn\"\n)\n\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')\nFirst, we will take a subset of the data, using Australia’s five biggest cities. This gives us a more manageable dataset to work with.\n# subset of observations from five biggest cities\nbig_cities = (\n    df.loc[df['Location'].isin(['Adelaide', 'Brisbane', 'Melbourne', 'Perth', 'Sydney'])]\n    .copy()\n)",
    "crumbs": [
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html#exploratory-data-analysis",
    "href": "sessions/05-eda-seaborn/index.html#exploratory-data-analysis",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nWhat does exploratory data analysis aim to achieve? What are you looking for when visualising data? Patterns, shapes, signals!\nWhen we describe a variable, or a sample of that variable, we are interested in understanding the characteristics of the observations. The starting point for doing this is describing the value that the data tends to take (central tendency), and how much it tends to deviate from its typical value (spread). Visualising the distribution of a variable can tell us these things (approximately), and can tell us about the shape of the data too.\nThe “central tendency” is the average or most common value that a variable takes. Mean, median, and mode are all descriptions of the central tendency.\n\nMean - Sum of values in a sample divided by the total number of observations.\nMedian - The midpoint value if the sample is ordered from highest to lowest.\nMode - The most common value in the sample1.\n\nThe mean is the most common approach, but the mean, median, and mode choice are context-dependent. Other approaches exist, too, such as the geometric mean2.\n\n# mode rainfall by location\nbig_cities.groupby('Location')['Rainfall'].agg(pd.Series.mode)\n\nLocation\nAdelaide     0.0\nBrisbane     0.0\nMelbourne    0.0\nPerth        0.0\nSydney       0.0\nName: Rainfall, dtype: float64\n\n\n\n# mode location\nbig_cities['Location'].agg(pd.Series.mode)\n\n0    Sydney\nName: Location, dtype: object\n\n\n\n# mode location using value counts\nbig_cities['Location'].value_counts().iloc[0:1]\n\nLocation\nSydney    3344\nName: count, dtype: int64\n\n\n\n# mean rainfall by location\nnp.round(big_cities.groupby('Location')['Rainfall'].mean(), decimals=2)\n\nLocation\nAdelaide     1.57\nBrisbane     3.14\nMelbourne    1.87\nPerth        1.91\nSydney       3.32\nName: Rainfall, dtype: float64\n\n\n\n# median rainfall by location\nbig_cities.groupby('Location')['Rainfall'].median()\n\nLocation\nAdelaide     0.0\nBrisbane     0.0\nMelbourne    0.0\nPerth        0.0\nSydney       0.0\nName: Rainfall, dtype: float64\n\n\n\n# geometric mean max temperature by location\nbig_cities.groupby('Location')['MaxTemp'].apply(lambda x: np.exp(np.log(x).mean()))\n\nLocation\nAdelaide     21.888697\nBrisbane     26.152034\nMelbourne    19.972352\nPerth        24.320203\nSydney       22.570993\nName: MaxTemp, dtype: float64\n\n\nThe values across the different measures of central tendency are not always the same. In this case, the mean and median differs massively.\nQuestions:\n\nWhy is that? Why would the median rainfall be zero for all five cities?\nDoes this matter? How would it change our understanding of the rainfall variable?\n\nDistributions can tell us more. We have simulated three different distributions that have slightly different shapes, to see how their mean and median values differ.\n\n\nPlot Code (Click to Expand)\n# generate distributions\nnp.random.seed(123)\nnormal_dist = np.random.normal(10, 1, 1000)\nright_skewed_dist = np.concatenate([np.random.normal(8, 2, 600), np.random.normal(14, 4, 400)])\nleft_skewed_dist = np.concatenate([np.random.normal(14, 2, 600), np.random.normal(8, 4, 400)])\n\n# set figure size\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# function for calculating summary statistics and plotting distributions\ndef plot_averages(ax, data, title):\n    mean = np.mean(data)\n    median = np.median(data)\n    \n    sns.histplot(data, color=\"#d9dcd6\", bins=30, ax=ax)\n    ax.axvline(mean, color=\"#0081a7\", linewidth=3, linestyle=\"--\", label=f\"Mean: {mean:.2f}\")\n    ax.axvline(median, color=\"#ef233c\", linewidth=3, linestyle=\"--\", label=f\"Median: {median:.2f}\")\n    ax.set_title(title)\n    ax.set_ylabel('')\n    ax.legend()\n\n# plot distributions\nfig, axes = plt.subplots(1, 3, sharey=True)\n\nplot_averages(axes[0], normal_dist, \"Normal Distribution\\n(Mean ≈ Median)\")\nplot_averages(axes[1], right_skewed_dist, \"Right-Skewed Distribution\\n(Mean &gt; Median)\")\nplot_averages(axes[2], left_skewed_dist, \"Left-Skewed Distribution\\n(Mean &lt; Median)\")\n\nplt.suptitle(\"Comparison of Mean & Median Across Distributions\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe mean and median of the normal distribution are identical, while the two skewed distributions have slightly different means and medians.\n\nThe mean is larger than the median when the distribution is right-skewed, and the median is larger than the mean when it is left-skewed.\n\nWhen the distribution is skewed, the median value will be a better description of the central tendency, because the mean value is more sensitive to extreme values (and skewed distributions have longer tails of extreme values).\n\n\nThese differences point to another important factor to consider when summarising data - the spread or deviation of the sample.\n\nHow do we measure how a sample is spread around the central tendency?\n\nStandard deviation and variance quantify spread.\nVariance, the average squared difference between observations and the mean value, measures how spread out a sample is.\nStandard deviation is the square root of the variance. It’s easier to interpret because it’s in the same units as the sample.\n\n\n\n\nPlot Code (Click to Expand)\n# generate distributions\nnp.random.seed(123)\nmean = 10\nstd_devs = [1, 2, 3]\ndistributions = [np.random.normal(mean, std_dev, 1000) for std_dev in std_devs]\n\n# function for calculating summary statistics and plotting distributions\ndef plot_spread(ax, data, std_dev, title):\n    mean = np.mean(data)\n    std_dev = np.std(data)\n\n    sns.histplot(data, color=\"#d9dcd6\", bins=30, ax=ax)\n    ax.axvline(mean, color=\"#0081a7\", linewidth=3, linestyle=\"--\", label=f\"Mean: {mean:.2f}\")\n    ax.axvline(mean + std_dev, color=\"#ee9b00\", linewidth=3, linestyle=\"--\", label=f\"Mean + 1 SD: {mean + std_dev:.2f}\")\n    ax.axvline(mean - std_dev, color=\"#ee9b00\", linewidth=3, linestyle=\"--\", label=f\"Mean - 1 SD: {mean - std_dev:.2f}\")\n    ax.set_title(f\"{title}\")\n    ax.legend()\n\n# plot distributions\nfig, axes = plt.subplots(1, 3, sharey=True, sharex=True)\n\nfor i, std_dev in enumerate(std_devs):\n    plot_spread(axes[i], distributions[i], std_dev, f\"Standard Deviation = {std_dev}\")\n\nplt.suptitle(\"Effect of Standard Deviation on Distribution Shape\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs standard deviation increases, the spread of values around the mean increases.\nWe can compute various summary statistics that describe a sample (mean, median, standard deviation, kurtosis etc. etc.), or we can just visualise it!\nVisualising distributions is a good starting point for understanding a sample. It can quickly and easily tell you a lot about the data.",
    "crumbs": [
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html#exploring-australian-weather",
    "href": "sessions/05-eda-seaborn/index.html#exploring-australian-weather",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "Exploring Australian Weather",
    "text": "Exploring Australian Weather\n\nVisualising Single Variables\nWe can start by visualising the distribution of rainfall and sunshine in Australia’s big cities, including dashed lines to show the mean and median values.\n\n# plot distribution of rainfall\nrainfall_mean = np.mean(big_cities['Rainfall'])\nrainfall_median = np.median(big_cities['Rainfall'].dropna())\n\nsns.histplot(data=big_cities, x='Rainfall', binwidth=10, color=\"#d9dcd6\")\nplt.axvline(rainfall_mean, color=\"#0081a7\", linestyle=\"--\", linewidth=2, label=f\"Mean: {rainfall_mean:.2f}\")\nplt.axvline(rainfall_median, color=\"#ef233c\", linestyle=\"--\", linewidth=2, label=f\"Median: {rainfall_median:.2f}\")\n\nplt.title(\"Distribution of Rainfall in Australia's Big Cities\")\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# plot distribution of sunshine\nsunshine_mean = np.mean(big_cities['Sunshine'])\nsunshine_median = np.median(big_cities['Sunshine'].dropna())\n\nsns.histplot(data=big_cities, x='Sunshine', binwidth=1, color=\"#d9dcd6\")\nplt.axvline(sunshine_mean, color=\"#0081a7\", linestyle=\"--\", linewidth=2, label=f\"Mean: {sunshine_mean:.2f}\")\nplt.axvline(sunshine_median, color=\"#ef233c\", linestyle=\"--\", linewidth=2, label=f\"Median: {sunshine_median:.2f}\")\n\nplt.title(\"Distribution of Sunshine in Australia's Big Cities\")\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nRainfall is very skewed, because the vast majority of days have zero rainfall. The distribution of sunshine is a little more evenly spread.\nWhile these two plots require a little more code, we can get most of what we want with a lot less just using sns.histplot() on its own. For example, plotting the distribution of maximum temperature, without all the other bells and whistles, already tells us a lot.\n\nsns.histplot(data=big_cities, x='MaxTemp')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can also look at the distribution of observations split by group, using sns.countplot(). Below, we see the number of observations per city in our subset.\n\nsns.countplot(big_cities, x='Location', color=\"#d9dcd6\", edgecolor='black')\nplt.ylim(3000, 3500)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSimilarly, we can look at the number of days with or without rain the next day.\n\nsns.countplot(big_cities, x='RainTomorrow', color=\"#d9dcd6\", edgecolor='black')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nHowever, it’s worth noting there is a simpler approach to this, just using pd.DataFrame.value_counts().\n\nbig_cities['RainTomorrow'].value_counts()\n\nRainTomorrow\nNo     11673\nYes     3543\nName: count, dtype: int64\n\n\n\n\nVisualising Multiple Variables\nWe will often want to know how values of a given variable change based on the values of another. This may not indicate a relationship, but it helps us better understand our data. There are lots of ways we can do this.\nWe can use sns.barplot() to plot the average hours of sunshine by location.\n\nsns.barplot(big_cities, x='Location', y='Sunshine', color=\"#d9dcd6\", edgecolor='black')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nOr we could use sns.boxplot(), visualising the distribution of maximum temperatures and humidity at 3pm by location.\n\nsns.boxplot(big_cities, x='Location', y='MaxTemp', color=\"#d9dcd6\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.boxplot(data=big_cities, x='RainTomorrow', y='Humidity3pm', color=\"#d9dcd6\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAnother way we can compare the distribution of values by groups is using sns.kdeplot(), which visualises a kernel-density estimation.\n\nsns.kdeplot(data=big_cities, x='Humidity3pm', hue='RainTomorrow')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhile there are lots of different ways we can quickly and easily compare two variables, or compare the value of a variable by groups, the right approach will always be context-dependent. It will all depend on what questions you have about your data, and which variables you are interested in.\nIf you have lots of questions about multiple variables, one way of exploring quickly is sns.pairplot().\n\nbiggest_cities = big_cities.loc[big_cities[\"Location\"].isin(['Sydney', 'Melbourne'])]\nsns.pairplot(\n    biggest_cities,\n    vars=['MinTemp', 'Sunshine', 'Rainfall'],\n    hue='Location'\n    )\n\nplt.show()\n\n\n\n\n\n\n\n\nI tend to struggle to infer much from complex plots like this, so I prefer to create separate plots using sns.scatterplot().\n\nsns.scatterplot(big_cities, x='Sunshine', y='MaxTemp', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nScatterplots can help you visualise how two continuous variables vary together. The above plot shows that sunshine hours are positively associated with maximum temperature, but there is significant noise.\nIf we compare this with a scatterplot visualising the association between two variables that should have a strong relationship, such as humidity at 9am and 3pm, we can see the difference.\n\nsns.scatterplot(big_cities, x='Humidity9am', y='Humidity3pm', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThere is still plenty of noise, but as humidity at 9am increases, it is clear that humidity at 3pm is likely to increase.\nAnother change we might make, to reduce the noise, is adding grouping structures to our scatterplot. Perhaps much of the noise in the sunshine scatterplot is because we are looking at data across many cities.\n\nsns.scatterplot(biggest_cities, x='Sunshine', y='MaxTemp', hue='Location', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWhen we compare just two cities, there still appears to be significant noise.\nSometimes you might need to do some more complex operations to transform the data before visualising it, in order to ask more specific questions. For example, you might want to compare how the total rainfall per day has varied over time in the data.\n\n(\n    big_cities\n    # convert date to datetime\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    # create year-month column\n    .assign(Year_Month=lambda x: x['Date'].dt.to_period('M'))\n    # group by year-month and calculate sum of rainfall\n    .groupby('Year_Month')['Rainfall'].sum()\n    # convert year-month index back to column in dataframe\n    .reset_index()\n    # create year-month timestamp for plotting\n    .assign(Year_Month=lambda x: x['Year_Month'].dt.to_timestamp()) \n    # pass df object to seaborn lineplot\n    .pipe(lambda df: sns.lineplot(data=df, x='Year_Month', y='Rainfall', linewidth=2))\n)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe above plot leverages an approach called method chaining, where we call multiple methods one after the other in the same operation3. Method chaining syntax is sometimes a little easier to follow, and you don’t have to create new objects for every operation, which can be a tidier way to work.\nWe can do the same to transform the data and visualise the mean average sunshine per month.\n\n(\n    big_cities\n    # convert date to datetime object\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    # set date column as index\n    .set_index('Date')\n    # resample by month-end for monthly aggregations\n    .resample('ME')\n    # calculate mean sunshine per month\n    .agg({'Sunshine': 'mean'})\n    # convert month index back to column in dataframe\n    .reset_index()\n    # pass df object to seaborn lineplot\n    .pipe(lambda df: sns.lineplot(data=df, x='Date', y='Sunshine', color=\"#1f77b4\", linewidth=2))\n)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFinally, we could combine two plots to look at how the average rainfall and average sunshine both vary by month.\n\nfig, axes = plt.subplots(1, 2)\n\n(\n    big_cities\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    .assign(Month=lambda x: x['Date'].dt.month)\n    .groupby('Month')['Rainfall'].mean()\n    .reset_index()\n    .pipe(lambda df: sns.lineplot(data=df, x='Month', y='Rainfall', color=\"#1f77b4\", linewidth=2, ax=axes[0]))\n)\n\n(\n    big_cities\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    .assign(Month=lambda x: x['Date'].dt.month) \n    .groupby('Month')['Sunshine'].mean() \n    .reset_index()\n    .pipe(lambda df: sns.lineplot(data=df, x='Month', y='Sunshine', color=\"#ff7f0e\", linewidth=2, ax=axes[1]))\n)\n\nxticks = range(1, 13)\nxticklabels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nfor ax in axes:\n    ax.set_xticks(xticks)  # Set ticks\n    ax.set_xticklabels(xticklabels, rotation=45)\n    ax.set_xlabel('')\n    ax.set_ylabel('')\naxes[0].set_title('Average Rainfall by Month', fontsize=16)\naxes[1].set_title('Average Sunshine by Month', fontsize=16)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html#exercises",
    "href": "sessions/05-eda-seaborn/index.html#exercises",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "Exercises",
    "text": "Exercises\nSome of these questions are easily answered by scrolling up and finding the answer in the output of the above code, however, the goal is to find the answer using code. No one actually cares what the answer to any of these questions is, it’s the process that matters!\nRemember, if you don’t know the answer, it’s okay to Google it (or speak to others, including me, for help)!\n\n\nImport Data (to Reset)\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')\n\n\n\nWhat does the distribution of minimum daily temperatures look like in these cities? Are there any unusual patterns?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nsns.histplot(big_cities[\"MinTemp\"].dropna(), kde=True)\nplt.title(\"Distribution of MinTemp\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nDoes the amount of sunshine vary depending on whether it rains the next day? Visualise this.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nsns.boxplot(data=big_cities, x=\"RainTomorrow\", y=\"Sunshine\")\nplt.title(\"Sunshine by Rain Tomorrow\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHow closely related are atmospheric pressure readings in the morning compared to the afternoon?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nsns.scatterplot(data=big_cities, x=\"Pressure9am\", y=\"Pressure3pm\")\nplt.title(\"Pressure at 9am vs 3pm\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHow does humidity in the afternoon vary across the five cities? What can you infer from this?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nsns.violinplot(data=big_cities, x=\"Location\", y=\"Humidity3pm\")\nplt.title(\"Humidity at 3pm by City\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nAre days when rain is expected tomorrow more or less common in this dataset? Show the distribution.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nsns.countplot(data=big_cities, x=\"RainTomorrow\")\nplt.title(\"Rain Tomorrow Counts\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nIs there any relationship between afternoon temperature and humidity? Does this relationship change depending on whether it rains the next day?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nsns.scatterplot(data=big_cities, x=\"Temp3pm\", y=\"Humidity3pm\", hue=\"RainTomorrow\")\nplt.title(\"Temp vs Humidity at 3pm by Rain Tomorrow\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHow strongly are the different continuous variables in this dataset correlated with each other? Create a correlation matrix.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\ncorr = big_cities.select_dtypes(include=\"number\").corr().round(2)\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, cmap=\"coolwarm\", center=0)\nplt.title(\"Correlation Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nExplore the relationships between rainfall, sunshine, and afternoon humidity across the cities. What patterns stand out?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nsns.pairplot(big_cities, vars=[\"Rainfall\", \"Sunshine\", \"Humidity3pm\"], hue=\"Location\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHow has the maximum temperature in Brisbane changed over time? Create a time series visualisation.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nbrisbane = big_cities.loc[big_cities[\"Location\"] == \"Brisbane\"]\nbrisbane['Date'] = pd.to_datetime(brisbane['Date'])\nbrisbane_daily = brisbane.groupby(\"Date\")[\"MaxTemp\"].mean().reset_index()\n\nplt.figure(figsize=(12, 4))\nsns.lineplot(data=brisbane_daily, x=\"Date\", y=\"MaxTemp\")\nplt.title(\"Daily Max Temp in Brisbane\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does the distribution of daily rainfall amounts look like? Is it skewed or symmetric?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nsns.kdeplot(data=big_cities, x=\"Rainfall\", fill=True)\nplt.title(\"Rainfall Distribution\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nHow does the average morning wind speed compare across the five cities?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nsns.barplot(data=big_cities, x=\"Location\", y=\"WindSpeed9am\", ci=None)\nplt.title(\"Average Morning Wind Speed by City\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nIn Perth, is there any visible relationship between the amount of sunshine and rainfall?\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nperth = big_cities.loc[big_cities[\"Location\"] == \"Perth\"]\nsns.scatterplot(data=perth, x=\"Sunshine\", y=\"Rainfall\")\nplt.title(\"Sunshine vs Rainfall in Perth\")\nplt.show()",
    "crumbs": [
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html#footnotes",
    "href": "sessions/05-eda-seaborn/index.html#footnotes",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe mode value is generally most useful when dealing with categorical variables.↩︎\nThe geometric mean multiplies all values in the sample and takes the \\(n\\)th root of that multiplied value. It can be useful when dealing with skewed data or data with very large ranges, and when dealing with rates, proportions etc. However it can’t handle zeros or negative values.↩︎\nThis may not be something you feel comfortable with yet, but it is something you may come across, and could explore in the future.↩︎",
    "crumbs": [
      "Exploration & Visualisation",
      "5. EDA with Seaborn"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html",
    "href": "sessions/02-jupyter_notebooks/index.html",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "This is the second session following Code Club’s relaunch. The focus is introducing jupyter notebooks and explaining to users how to get started with a new project and briefly introducing some key concepts.\nWe are also planning some time for Q&A following the first session.",
    "crumbs": [
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#session-slides",
    "href": "sessions/02-jupyter_notebooks/index.html#session-slides",
    "title": "Jupyter Notebooks",
    "section": "Session Slides",
    "text": "Session Slides\nUse the left ⬅️ and right ➡️ arrow keys to navigate through the slides below. To view in a separate tab/window, follow this link.",
    "crumbs": [
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#the-tools-you-will-need",
    "href": "sessions/02-jupyter_notebooks/index.html#the-tools-you-will-need",
    "title": "Jupyter Notebooks",
    "section": "The Tools You Will Need",
    "text": "The Tools You Will Need\nThough Jupyter notebooks can be used with a variety of coding languages and in different settings the key tools used in this session are:\n\nLanguage: Python\nDependency Management & Virtual Environments: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code/Jupyter Notebooks (or your preferred IDE)\n\nYou can install all the tools you’ll need by running the following one-liner run in PowerShell:\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop\nYou can find more information on these topics in the Python Onboarding session",
    "crumbs": [
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#project-setup",
    "href": "sessions/02-jupyter_notebooks/index.html#project-setup",
    "title": "Jupyter Notebooks",
    "section": "Project Setup",
    "text": "Project Setup\nOur project set-up will follow the same steps as used in the onboarding session, by using uv to set up a new project folder.\nTo get started we will use PowerShell powershell to open a command prompt, it should open in your C drive (e.g., C:\\Users\\user.name). If it does not, run cd ~, and it should return to your home directory. We recommend the use of a single folder to hold your python projects while learning, because we will be using git version control we will call this “Git”. we can use the command mkdir code_club to make this folder and then use cd code_club to relocate to this folder1.\nWe will create a new uv project in this directory using the command uv init. The new project will contain everything we need, including a Python installation, a virtual environment, and the necessary project files for tracking and managing any packages installed in the virtual environment. To set up a new project called test-project, use the following command:\nuv init test_project\nHaving created this new directory, navigate to it using cd test_project.\nFor this session you will need to add 3 Python packages, ipykernel2, pandas and seaborn We can use the following command:\nuv add ipykernel pandas seaborn\nWe are going to create a blank notebook in this file by running the command new-item first_notebook.ipynb if you now run ls you will note this file has been created\nYour Python project is now set up, and you are ready to start writing some code. You can open VS Code from your PowerShell window by running code ..\n\nOpening your project in VS Code\nYou could also do this from within VS Code as most IDEs include a terminal interface which will be demonstrated in session.\nFor now launch VS Code and click File &gt; Open Folder.... You’ll want to make sure you select the root level of your project. Once you’ve opened the folder, the file navigation pane in VS Code should display the files that uv has created, as well as the notebook you created: first_notebook.ipynb. Click on this to open it.\nOnce VS Code realises you’ve opened a folder with Python code and a virtual environment, it should do the following:\n\nSuggest you install the Python extension (and, once you’ve created a Jupyter notebook, the Jupyter one) offered by Microsoft - go ahead and do this. If this doesn’t happen, you can install extensions manually from the Extensions pane on the left-hand side.\nSelect the uv-created .venv as the python Environment we’re going to use to actually run our code. If this doesn’t happen, press ctrl-shift-P, type “python environment” to find the Python - Create Environment... option, hit enter, choose “Venv” and proceed to “Use Existing”.\n\nIf VS Code has found the virtual environment, it may pick up the correct kernel. If not you may need to select this manually this can be done by clicking in the top right where you can see Select Kernel (see below)\n\n\n\nClick ‘Select Kernel’\n\n\nWe can then select the appropriate kernel from python environments and looking for\n\n\n\nclick Python Environments\n\n\n\n\n\nclick venv - recommended\n\n\nOnce the kernel is enabled you are ready to start adding cells to your notebook. these can either be code cells which is where you include your program elements or markdown which enable the addition of headings, analysis and commentary.",
    "crumbs": [
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#footnotes",
    "href": "sessions/02-jupyter_notebooks/index.html#footnotes",
    "title": "Jupyter Notebooks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe recommend using the C drive for all Python projects, especially if using version control. Storing projects like these on One Drive will create many unnecessary issues. It can be helpful to use a sub-directory to store projects but is not necessary and is not a requirement for code club↩︎\nStrictly speaking, we should install ipykernel as a development dependency (a dependency that is needed for any development but not when the project is put into production). In this case, we would add it by running uv add --dev ipykernel. However, in this case, it is simpler to just add it as a regular dependency, and it doesn’t harm.↩︎",
    "crumbs": [
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#scatterplots-show-patterns",
    "href": "sessions/13-intro-linear-regression/slides.html#scatterplots-show-patterns",
    "title": "An Introduction to Linear Regression",
    "section": "Scatterplots Show Patterns",
    "text": "Scatterplots Show Patterns"
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#fitted-lines-show-associations",
    "href": "sessions/13-intro-linear-regression/slides.html#fitted-lines-show-associations",
    "title": "An Introduction to Linear Regression",
    "section": "Fitted Lines Show Associations",
    "text": "Fitted Lines Show Associations"
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#regression-is-just-fitting-lines-to-data",
    "href": "sessions/13-intro-linear-regression/slides.html#regression-is-just-fitting-lines-to-data",
    "title": "An Introduction to Linear Regression",
    "section": "Regression is Just Fitting Lines to Data",
    "text": "Regression is Just Fitting Lines to Data\n\nRegression finds the line that passes through the data in the way that minimises the distance between the line and all observations.\nThis is often referred to as the “line of best fit”.\nThe line of best fit is the best representation of the relationship between two variables given the data."
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#regression-estimates-effects",
    "href": "sessions/13-intro-linear-regression/slides.html#regression-estimates-effects",
    "title": "An Introduction to Linear Regression",
    "section": "Regression Estimates Effects",
    "text": "Regression Estimates Effects\n\nRegression quantifies the relationship between one or more explanatory variables (or predictors) and the outcome (the variable we want to analyse).\nCorrelation indicates whether two variables are related. Regression tells us by how much.\nFor example, if flipper length increases by 10mm, body mass increases by ~500g.\nThis precision opens up a world of possibilities for analysis."
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#why-description-is-not-enough",
    "href": "sessions/13-intro-linear-regression/slides.html#why-description-is-not-enough",
    "title": "An Introduction to Linear Regression",
    "section": "Why Description is Not Enough",
    "text": "Why Description is Not Enough\n\nDescribing the pattern is useful, but we often want more.\n\nWill a penguin with a 205mm flipper be heavier than one with 195mm?\nIf we measure flipper length, can we estimate body mass?\nDoes this relationship hold for new penguins we haven’t seen?\n\nRegression lets us make predictions, test hypotheses, and make inferences."
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#linear-regression-fits-a-straight-line",
    "href": "sessions/13-intro-linear-regression/slides.html#linear-regression-fits-a-straight-line",
    "title": "An Introduction to Linear Regression",
    "section": "Linear Regression Fits a Straight Line",
    "text": "Linear Regression Fits a Straight Line\n\nThe most simple regression model is one that fits a straight line through two variables, \\(X\\) (the predictor) and \\(Y\\) (the outcome).\n\nThe straight line indicates that the relationship between \\(X\\) and \\(Y\\) is constant.\nChanges in the value of \\(X\\) have the same effect on \\(Y\\) across the entire range of \\(X\\).\n\nThis is linear regression. It is the most common type of regression, and the foundation for so much more."
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#regression-minimises-prediction-error",
    "href": "sessions/13-intro-linear-regression/slides.html#regression-minimises-prediction-error",
    "title": "An Introduction to Linear Regression",
    "section": "Regression Minimises Prediction Error",
    "text": "Regression Minimises Prediction Error\n\nThe line of best fit minimises the distance between the line (the predicted value) and observations.\nLinear regression uses the following method to do this:\n\nCalculate the error (residual) for each point.\nSquare each error (so they’re all positive).\nSum all squared errors, called the Residual Sum of Squares (RSS).\nFind the line that minimises the RSS.\n\nThis is called Ordinary Least Squares (OLS) estimation."
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#finding-the-line-of-best-fit",
    "href": "sessions/13-intro-linear-regression/slides.html#finding-the-line-of-best-fit",
    "title": "An Introduction to Linear Regression",
    "section": "Finding the Line of Best Fit",
    "text": "Finding the Line of Best Fit"
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#finding-the-line-of-best-fit-1",
    "href": "sessions/13-intro-linear-regression/slides.html#finding-the-line-of-best-fit-1",
    "title": "An Introduction to Linear Regression",
    "section": "Finding the Line of Best Fit",
    "text": "Finding the Line of Best Fit"
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#finding-the-line-of-best-fit-2",
    "href": "sessions/13-intro-linear-regression/slides.html#finding-the-line-of-best-fit-2",
    "title": "An Introduction to Linear Regression",
    "section": "Finding the Line of Best Fit",
    "text": "Finding the Line of Best Fit"
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#comparing-different-lines",
    "href": "sessions/13-intro-linear-regression/slides.html#comparing-different-lines",
    "title": "An Introduction to Linear Regression",
    "section": "Comparing Different Lines",
    "text": "Comparing Different Lines"
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#predictions-come-from-the-fitted-line",
    "href": "sessions/13-intro-linear-regression/slides.html#predictions-come-from-the-fitted-line",
    "title": "An Introduction to Linear Regression",
    "section": "Predictions Come From the Fitted Line",
    "text": "Predictions Come From the Fitted Line\n\nOnce you have a line fitted to the data, you can make predictions (and with a little more work, inferences).\n\nThe predicted outcome for a given value of \\(X\\) is just the \\(Y\\) value of the line at \\(X\\).\nIn regression, predictions are sometimes referred to as fitted values.\nThe difference between the predicted value from the line and the actual value is the prediction error, or the residual.\n\nInferences (statements about the effect of \\(X\\) on \\(Y\\)) are derived from the calculation of the slope of the line.\n\nThe slope of the fitted line suggests that 10mm increases in flipper length result in 500g increases in body mass.\nIn regression, these values are referred to as coefficients."
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#reading-fitted-values-residuals",
    "href": "sessions/13-intro-linear-regression/slides.html#reading-fitted-values-residuals",
    "title": "An Introduction to Linear Regression",
    "section": "Reading Fitted Values & Residuals",
    "text": "Reading Fitted Values & Residuals\n\n\n Flipper Length  Body Mass  Fitted  Residual\n          178.0     3250.0  3055.2     194.8\n          196.0     3675.0  3957.9    -282.9\n          195.0     4000.0  3907.8      92.2\n          210.0     4850.0  4660.1     189.9\n          192.0     4050.0  3757.3     292.7\n          201.0     4300.0  4208.7      91.3\n          197.0     3300.0  4008.1    -708.1\n          220.0     5400.0  5161.6     238.4\n          229.0     5800.0  5613.0     187.0\n          210.0     4400.0  4660.1    -260.1"
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#dispersion-around-the-line-is-unexplained-variance",
    "href": "sessions/13-intro-linear-regression/slides.html#dispersion-around-the-line-is-unexplained-variance",
    "title": "An Introduction to Linear Regression",
    "section": "Dispersion Around the Line is Unexplained Variance",
    "text": "Dispersion Around the Line is Unexplained Variance"
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#the-regression-formula",
    "href": "sessions/13-intro-linear-regression/slides.html#the-regression-formula",
    "title": "An Introduction to Linear Regression",
    "section": "The Regression Formula",
    "text": "The Regression Formula\n\nThe formula for a simple linear regression model, predicting \\(Y\\) with one predictor \\(X\\):\n\n\\[\nY =\n\\underbrace{\\vphantom{\\beta_0} \\overset{\\color{#41B6E6}{\\text{Intercept}}}{\\color{#41B6E6}{\\beta_0}} +\n\\overset{\\color{#005EB8}{\\text{Slope}}}{\\color{#005EB8}{\\beta_1}}X \\space \\space}_{\\text{Explained Variance}} +\n\\overset{\\mathstrut \\color{#ED8B00}{\\text{Error}}}{\\underset{\\text{Unexplained}}{\\color{#ED8B00}{\\epsilon}}}\n\\]\n\nThis breaks the problem down into three components, and estimates two parameters:\n\n\\(\\beta_0\\) - The intercept, estimating the average value of \\(Y\\) when \\(X = 0\\).\n\\(\\beta_1\\) - The slope, estimating the effect that \\(X\\) has on the outcome, \\(Y\\).\n\\(\\epsilon\\) - The error term, capturing the remaining variance in the outcome \\(Y\\) that is not explained by the rest of the model."
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#applying-the-formula",
    "href": "sessions/13-intro-linear-regression/slides.html#applying-the-formula",
    "title": "An Introduction to Linear Regression",
    "section": "Applying the Formula",
    "text": "Applying the Formula\n\nThe regression formula for predicting or explaining body mass from flipper length:\n\n\\[\n\\text{Body Mass} = \\beta_0 + \\beta_1 \\times \\text{Flipper Length}\n\\]\n\nIntercept (\\(\\beta_0\\)) = -5872.1g\n\nAverage body mass when flipper length equals zero (not meaningful here, but necessary for the line).\n\nSlope (\\(\\beta_1\\)) = 50.2g\n\nHow much mass changes per mm increase in flipper length."
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#regression-can-handle-multiple-variables",
    "href": "sessions/13-intro-linear-regression/slides.html#regression-can-handle-multiple-variables",
    "title": "An Introduction to Linear Regression",
    "section": "Regression Can Handle Multiple Variables",
    "text": "Regression Can Handle Multiple Variables\n\nUnlike correlation and a lot of data visualisation, regression is not pairwise.\nRegression can include many predictors.\nYou can add as many variables to a regression model as you want.\n\nThough you should only add those that matter (and even then less is often more).\n\nWe know that flipper length influences body mass, but perhaps bill length and/or bill depth also matter?"
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#from-linear-to-generalised-linear-models",
    "href": "sessions/13-intro-linear-regression/slides.html#from-linear-to-generalised-linear-models",
    "title": "An Introduction to Linear Regression",
    "section": "From Linear to Generalised Linear Models",
    "text": "From Linear to Generalised Linear Models\n\nLinear regression assumes your outcome is continuous and normally distributed, but this is often not the case.\nGeneralised linear models extend this idea to other types of outcome using a “link function”.\nThe link function transforms your outcome so linear regression can work on it. It bends the scale so a straight line fits the data.\n\nLogistic regression (logit link function) - Binary outcomes (survival vs death, electoral victory vs loss).\nPoisson regression (log link function) - Count outcomes (number of attendances).\n\nGeneralised linear models use the same core idea, but with a transformation step before fitting the line."
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#dealing-with-grouping-structures",
    "href": "sessions/13-intro-linear-regression/slides.html#dealing-with-grouping-structures",
    "title": "An Introduction to Linear Regression",
    "section": "Dealing with Grouping Structures",
    "text": "Dealing with Grouping Structures\n\nLinear regression assumes all observations are independent.\nSometimes (often) there are higher-level grouping structures that moderate the effect of \\(X\\) on \\(Y\\).\n\nStudents in different classes (or schools).\nPatients attending different hospitals.\nDifferent species of penguin.\n\nMultilevel models account for grouping structure by fitting lines to each group without treating each group as completely distinct.\n\nGroups can share information while still differing.\n\nSame linear framework, just allowing for hierarchy and grouping structure."
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#the-possibilities-are-endless",
    "href": "sessions/13-intro-linear-regression/slides.html#the-possibilities-are-endless",
    "title": "An Introduction to Linear Regression",
    "section": "The Possibilities are Endless",
    "text": "The Possibilities are Endless\n\nThese are just three examples of the ways that the simple linear model can be adapted to fit different needs.\nThe real power of linear regression is that it combines simplicity with flexibility.\nIt works, and it can work in so many different situations."
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#key-takeaways",
    "href": "sessions/13-intro-linear-regression/slides.html#key-takeaways",
    "title": "An Introduction to Linear Regression",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nIf things vary together, we can measure that and use it to understand the relationship between them.\nRegression quantifies those relationships. It asks not just whether variables are related, but by how much.\nRegression is just fitting lines to data.\n\nWhen we minimise the error in the line of best fit, we get a line that describes how variables are related in the data.\nThis allows us to make predictions about unseen data and inferences about the relationship between variables.\n\nThe linear model is incredibly powerful, but also incredibly flexible.\n\nOnce you’ve figured out linear regression, you have an entire toolbox at your disposal."
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#further-learning",
    "href": "sessions/13-intro-linear-regression/slides.html#further-learning",
    "title": "An Introduction to Linear Regression",
    "section": "Further Learning",
    "text": "Further Learning\n\nMLU-Explain - Linear Regression\nStatQuest (Josh Starmer) - Linear Regression\njbstatistics (Jeremy Balka) - Simple Linear Regression\nAndrew Gelman et al. - Regression & Other Stories"
  },
  {
    "objectID": "sessions/13-intro-linear-regression/slides.html#thank-you",
    "href": "sessions/13-intro-linear-regression/slides.html#thank-you",
    "title": "An Introduction to Linear Regression",
    "section": "Thank You!",
    "text": "Thank You!\nContact:\n scwcsu.analytics.specialist at nhs dot net\nCode & Slides:\n /NHS-South-Central-and-West/code-club\n… And don’t forget to give us your feedback."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#what-to-expect",
    "href": "sessions/01-onboarding/slides.html#what-to-expect",
    "title": "Python Onboarding",
    "section": "What to Expect?",
    "text": "What to Expect?\n\nLearning a language is hard. It can be frustrating. Perseverance is key to success.\nThese sessions will introduce you to Python, showing you what is possible and how to achieve some of what might benefit your work.\nBut the real learning comes by doing. You need to run the code yourself, have a play around, and cement what you’ve learned by applying it.\nPractice, repetition, and making mistakes along the way is how real progress is made."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#why-learn-python",
    "href": "sessions/01-onboarding/slides.html#why-learn-python",
    "title": "Python Onboarding",
    "section": "Why Learn Python?",
    "text": "Why Learn Python?\n\nCoding skills, generally, and Python specifically, seem to be a priority in the NHS right now. It’s a clear direction of travel. Learning now sets you up for the future.\nPython and the applied skills taught in these sessions will enable you to use advanced methods and design flexible, scalable solutions.\nPython is very valuable for career development.\nIt is (hopefully) fun!"
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#the-toolkit",
    "href": "sessions/01-onboarding/slides.html#the-toolkit",
    "title": "Python Onboarding",
    "section": "The Toolkit",
    "text": "The Toolkit\n\nWe will be using the following tools throughout this course:\n\nLanguage: Python\nDependency management: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code/Jupyter Notebooks (or your preferred IDE)\n\nYou can install all these tools by running the following in PowerShell:\n\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop"
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#python",
    "href": "sessions/01-onboarding/slides.html#python",
    "title": "Python Onboarding",
    "section": "Python",
    "text": "Python\n\nPython is an all-purpose programming language that is the most popular worldwide and widely used in almost every industry.\nPython’s popularity is owed to its flexibility – it is the second-best tool for every job.\nIt is a strong choice for data science and analytics, being one of the best languages for data wrangling, data visualisation, statistics, and machine learning.\n\nIt is also well-suited to web development, scientific computing, and automation."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#dependency-management",
    "href": "sessions/01-onboarding/slides.html#dependency-management",
    "title": "Python Onboarding",
    "section": "Dependency Management",
    "text": "Dependency Management\n\nDependency management refers to the process of tracking and managing all of the packages (dependencies) a project needs to run. It ensures:\n\nThe right packages are installed.\nThe correct versions are used.\nConflicts between packages are avoided.\n\nWe are using uv for dependency management."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#virtual-environments",
    "href": "sessions/01-onboarding/slides.html#virtual-environments",
    "title": "Python Onboarding",
    "section": "Virtual Environments",
    "text": "Virtual Environments\n\nVirtual environments are isolated Python environments that allow you to manage dependencies for a specific project without the state of those dependencies affecting other projects or your wider system. They help by:\n\nKeeping dependencies separate for each project.\nAvoiding version conflicts between projects.\nMaking dependency management more predictable and reproducible.\n\nVirtual environments are a part of dependency management, and we will use uv to manage both the dependencies and virtual environments."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#version-control",
    "href": "sessions/01-onboarding/slides.html#version-control",
    "title": "Python Onboarding",
    "section": "Version Control",
    "text": "Version Control\n\nVersion control is the practice of tracking and managing changes to code or files over time, allowing you to:\n\nRevert to earlier versions if needed.\nCollaborate with others on the same project easily.\nMaintain a history of changes.\n\nWe are using Git (the version control system) and GitHub (the platform for hosting our work)."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#ide",
    "href": "sessions/01-onboarding/slides.html#ide",
    "title": "Python Onboarding",
    "section": "IDE",
    "text": "IDE\n\nAn IDE (Integrated Development Environment) is fully featured software that provides everything you need to write code as conveniently as possible.\nIt typically includes a code editor, debugger, build tools, and features like syntax highlighting and code completion.\nSome common IDEs used for Python include VS Code, PyCharm, Vim, Jupyter Notebooks/JupyterLab, and Positron.\nWe will use VS Code or Jupyter Notebooks (which is not exactly an IDE but is similar)."
  },
  {
    "objectID": "sessions/15-beyond-linearity/slides.html#linear-models-for-linear-effects",
    "href": "sessions/15-beyond-linearity/slides.html#linear-models-for-linear-effects",
    "title": "Beyond Linearity",
    "section": "Linear Models for Linear Effects",
    "text": "Linear Models for Linear Effects\n\nLinear regression fits a straight line through data\nAssumes constant effect: each unit increase in \\(X\\) changes \\(Y\\) by the same amount\nWorks when the relationship is linear\n\n\\[\nY = \\beta_0 + \\beta_1X + \\epsilon\n\\]"
  },
  {
    "objectID": "sessions/15-beyond-linearity/slides.html#linear-relationships-in-the-real-world",
    "href": "sessions/15-beyond-linearity/slides.html#linear-relationships-in-the-real-world",
    "title": "Beyond Linearity",
    "section": "Linear Relationships in the Real World",
    "text": "Linear Relationships in the Real World"
  },
  {
    "objectID": "sessions/15-beyond-linearity/slides.html#but-real-data-isnt-always-linear",
    "href": "sessions/15-beyond-linearity/slides.html#but-real-data-isnt-always-linear",
    "title": "Beyond Linearity",
    "section": "But Real Data Isn’t Always Linear",
    "text": "But Real Data Isn’t Always Linear\n\nGrowth curves (exponential, logistic)\nBinary outcomes (survived/died, yes/no)\nCount data (number of events)\nBounded outcomes (proportions, rates)\nCurved relationships (U-shaped, S-shaped)"
  },
  {
    "objectID": "sessions/15-beyond-linearity/slides.html#probability-of-disease-by-age",
    "href": "sessions/15-beyond-linearity/slides.html#probability-of-disease-by-age",
    "title": "Beyond Linearity",
    "section": "Probability of Disease by Age",
    "text": "Probability of Disease by Age"
  },
  {
    "objectID": "sessions/15-beyond-linearity/slides.html#fitting-a-straight-line-fails",
    "href": "sessions/15-beyond-linearity/slides.html#fitting-a-straight-line-fails",
    "title": "Beyond Linearity",
    "section": "Fitting a Straight Line Fails",
    "text": "Fitting a Straight Line Fails"
  },
  {
    "objectID": "sessions/15-beyond-linearity/slides.html#the-problem-is-the-scale",
    "href": "sessions/15-beyond-linearity/slides.html#the-problem-is-the-scale",
    "title": "Beyond Linearity",
    "section": "The Problem is the Scale",
    "text": "The Problem is the Scale\n\nLinear regression assumes outcomes can take any value\nBut many outcomes are constrained:\n\nProbabilities must be between 0 and 1\nCounts must be non-negative integers\nProportions are bounded\n\nForcing a straight line violates these constraints"
  },
  {
    "objectID": "sessions/15-beyond-linearity/slides.html#the-core-insight",
    "href": "sessions/15-beyond-linearity/slides.html#the-core-insight",
    "title": "Beyond Linearity",
    "section": "The Core Insight",
    "text": "The Core Insight\n\nDon’t force data onto a straight line\nTransform the scale so linear regression works\nFit the line on the transformed scale\nTransform back to get predictions\nThis is what link functions do."
  },
  {
    "objectID": "sessions/15-beyond-linearity/slides.html#how-link-functions-work",
    "href": "sessions/15-beyond-linearity/slides.html#how-link-functions-work",
    "title": "Beyond Linearity",
    "section": "How Link Functions Work",
    "text": "How Link Functions Work\n\nThe link function connects the linear predictor to the outcome:\n\nTake your outcome \\(Y\\) (bounded, binary, count, etc.)\nApply a transformation that “stretches” or “squashes” the scale\nFit a linear model on the transformed scale\nTransform predictions back to the original scale"
  },
  {
    "objectID": "sessions/15-beyond-linearity/slides.html#the-logit-link-for-binary-outcomes",
    "href": "sessions/15-beyond-linearity/slides.html#the-logit-link-for-binary-outcomes",
    "title": "Beyond Linearity",
    "section": "The Logit Link for Binary Outcomes",
    "text": "The Logit Link for Binary Outcomes\n\nFor binary outcomes (0 or 1), we model the log-odds:\n\n\\[\n\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1X\n\\]\n\nLeft side: log-odds (can be any value)\nRight side: linear predictor (can be any value)\nTransform back to get probability: \\(p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X)}}\\)\nThis is logistic regression."
  },
  {
    "objectID": "sessions/15-beyond-linearity/slides.html#visualising-the-transformation",
    "href": "sessions/15-beyond-linearity/slides.html#visualising-the-transformation",
    "title": "Beyond Linearity",
    "section": "Visualising the Transformation",
    "text": "Visualising the Transformation"
  },
  {
    "objectID": "sessions/15-beyond-linearity/slides.html#logistic-regression-fits-an-s-curve",
    "href": "sessions/15-beyond-linearity/slides.html#logistic-regression-fits-an-s-curve",
    "title": "Beyond Linearity",
    "section": "Logistic Regression Fits an S-Curve",
    "text": "Logistic Regression Fits an S-Curve"
  },
  {
    "objectID": "sessions/15-beyond-linearity/slides.html#different-outcomes-need-different-links",
    "href": "sessions/15-beyond-linearity/slides.html#different-outcomes-need-different-links",
    "title": "Beyond Linearity",
    "section": "Different Outcomes Need Different Links",
    "text": "Different Outcomes Need Different Links\n\nBinary outcomes → Logit link → Logistic regression\nCount data → Log link → Poisson regression\nContinuous positive → Log link → Log-linear models\nProportions → Logit link → Beta regression\nAll follow the same pattern: transform, fit linear model, transform back."
  },
  {
    "objectID": "sessions/15-beyond-linearity/slides.html#poisson-regression-uses-log-link",
    "href": "sessions/15-beyond-linearity/slides.html#poisson-regression-uses-log-link",
    "title": "Beyond Linearity",
    "section": "Poisson Regression Uses Log Link",
    "text": "Poisson Regression Uses Log Link\n\nModel the log of the expected count:\n\n\\[\n\\log(\\text{E}[Y]) = \\beta_0 + \\beta_1X\n\\]\n\nLeft side: log of expected count (can be any value)\nRight side: linear predictor (can be any value)\nTransform back: \\(\\text{E}[Y] = e^{\\beta_0 + \\beta_1X}\\)\nPredictions are always positive, matching count data."
  },
  {
    "objectID": "sessions/15-beyond-linearity/slides.html#what-weve-learned",
    "href": "sessions/15-beyond-linearity/slides.html#what-weve-learned",
    "title": "Beyond Linearity",
    "section": "What We’ve Learned",
    "text": "What We’ve Learned\n\nReal data often doesn’t fit straight lines.\nLink functions transform non-linear problems into linear ones.\nLogistic regression (logit link) handles binary outcomes.\nPoisson regression (log link) handles count data.\nLinear regression isn’t limited to linear relationships."
  },
  {
    "objectID": "sessions/15-beyond-linearity/slides.html#the-power-of-this-approach",
    "href": "sessions/15-beyond-linearity/slides.html#the-power-of-this-approach",
    "title": "Beyond Linearity",
    "section": "The Power of This Approach",
    "text": "The Power of This Approach\n\nSimplicity - Same core idea (fit a line) works everywhere.\nFlexibility - Adapts to different data types and structures.\nInterpretability - Coefficients still represent effects.\nExtensibility - Once you know linear regression, you can learn anything."
  },
  {
    "objectID": "sessions/15-beyond-linearity/slides.html#further-learning",
    "href": "sessions/15-beyond-linearity/slides.html#further-learning",
    "title": "Beyond Linearity",
    "section": "Further Learning",
    "text": "Further Learning\n\nStatQuest (Josh Starmer) - Logistic Regression\nAndrew Gelman et al. - Regression & Other Stories"
  },
  {
    "objectID": "sessions/15-beyond-linearity/slides.html#thank-you",
    "href": "sessions/15-beyond-linearity/slides.html#thank-you",
    "title": "Beyond Linearity",
    "section": "Thank You!",
    "text": "Thank You!\nContact:\n scwcsu.analytics.specialist at nhs dot net\nCode & Slides:\n /NHS-South-Central-and-West/code-club\n… And don’t forget to give us your feedback."
  },
  {
    "objectID": "sessions/08-functions/slides.html#what-are-functions",
    "href": "sessions/08-functions/slides.html#what-are-functions",
    "title": "Functions in Python",
    "section": "What are Functions?",
    "text": "What are Functions?\n\nFunctions are self-contained, reusable blocks of code that perform a specific task.\nThey take an input, perform a task on that input (such as a calculation or transformation), and return a result.\nFunctions help to organise code, avoid repetition, and make complex processes easier to understand and maintain."
  },
  {
    "objectID": "sessions/08-functions/slides.html#anatomy-of-a-function",
    "href": "sessions/08-functions/slides.html#anatomy-of-a-function",
    "title": "Functions in Python",
    "section": "Anatomy of a Function",
    "text": "Anatomy of a Function\n\nUnderstanding how to use (and create) functions can feel a little abstract at first.\nFunctions must have names (how they are called), arguments (inputs passed to it), statements (the tasks carried out), and return values (the output/result it returns).\nPython functions are structured as below:"
  },
  {
    "objectID": "sessions/08-functions/slides.html#shop-bought-versus-home-baked",
    "href": "sessions/08-functions/slides.html#shop-bought-versus-home-baked",
    "title": "Functions in Python",
    "section": "Shop-Bought Versus Home-Baked",
    "text": "Shop-Bought Versus Home-Baked\n\nAll languages come with built-in functions available - you will have used some of Python’s built-in functions already, for example print(), type() and sum().\nThey operate exactly the same as ones you make yourself - they are called, they take an argument and they return a result.\nYou can create your own functions or use pre-built functions either provided by the language itself or by others (imported from packages/libraries)."
  },
  {
    "objectID": "sessions/08-functions/slides.html#functions-are-not-new",
    "href": "sessions/08-functions/slides.html#functions-are-not-new",
    "title": "Functions in Python",
    "section": "Functions Are Not New",
    "text": "Functions Are Not New\n\nWhile functions are a fundamental building block of programming, they exist anywhere that you might write code.\nFor example, Excel includes lots of functions (for example, SUM(), VLOOKUP(), LINEST()), as does SQL ( for example, COUNT(), AVG(), COALESCE())!\nFunctions work very similarly in whatever language you might use, i.e. they have the same component parts. What changes is the syntax."
  },
  {
    "objectID": "sessions/08-functions/slides.html#advantages-of-functions",
    "href": "sessions/08-functions/slides.html#advantages-of-functions",
    "title": "Functions in Python",
    "section": "Advantages of Functions",
    "text": "Advantages of Functions\n\nReusability - Write code once and use it multiple times. Stay DRY (Don’t Repeat Yourself)!\nModularity - Break down complex tasks into smaller, manageable chunks.\nReadability - Make code clearer and more organised.\nMaintainability - Easier to make changes to one part of the code without breaking everything.\nReliability - More efficient code with fewer errors that others can follow."
  },
  {
    "objectID": "sessions/08-functions/slides.html#best-practices",
    "href": "sessions/08-functions/slides.html#best-practices",
    "title": "Functions in Python",
    "section": "Best Practices",
    "text": "Best Practices\n\nUse Descriptive Names - Give your function a name that describes what it does.\nPerform One Task - Functions should carry out a single task so as to avoid unnecessary complexity.\nDocument Your Functions - There are lots of ways to help anyone using your functions (including future you), such as docstrings and type hints.\nAvoid Side Effects - Functions should not change anything outside of itself.\nExplicitly Handle Errors - Setting guardrails helps avoid unexpected consequences.\nWrite Testable Code - Functions should be easy to test (and should be tested)."
  },
  {
    "objectID": "sessions/08-functions/slides.html#what-are-programming-paradigms",
    "href": "sessions/08-functions/slides.html#what-are-programming-paradigms",
    "title": "Functions in Python",
    "section": "What are Programming Paradigms",
    "text": "What are Programming Paradigms\n\nProgramming paradigms are just the different approaches or styles to programming that define how code is structured/organised.\nFunctional and Object-Oriented Programming (OOP) are two of the most common paradigms.\nLanguages like Java, C++, and Ruby are OOP, while languages like Haskell and Scala are functional.\nLots of languages (including Python) can support multiple paradigms."
  },
  {
    "objectID": "sessions/08-functions/slides.html#what-is-functional-programming",
    "href": "sessions/08-functions/slides.html#what-is-functional-programming",
    "title": "Functions in Python",
    "section": "What is Functional Programming?",
    "text": "What is Functional Programming?\n\nFunctional programming is a programming paradigm — a style of building the structure and elements of computer programs — that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data."
  },
  {
    "objectID": "sessions/08-functions/slides.html#what-is-functional-programming-actually",
    "href": "sessions/08-functions/slides.html#what-is-functional-programming-actually",
    "title": "Functions in Python",
    "section": "What is Functional Programming, Actually?",
    "text": "What is Functional Programming, Actually?\n\nIn simple (and perhaps not entirely accurate) terms, functional programming just focuses on the use of functions for writing code.\nFunctional programming treats coding as the evaluation of mathematical functions and avoids changes in state or mutable data.\nIt relies on “pure” functions that always produce the same output for the same inputs, and do not cause side effects!"
  },
  {
    "objectID": "sessions/08-functions/slides.html#conclusion",
    "href": "sessions/08-functions/slides.html#conclusion",
    "title": "Functions in Python",
    "section": "Conclusion",
    "text": "Conclusion\n\nFunctions are a great way to organise and simplify your code, and avoids repetition and inefficiency.\nUnderstanding how functions work is key to learning to use them, create them, and debug them.\nUnderstanding functional programming is key to following along in conversations with nerds."
  },
  {
    "objectID": "sessions/11-comparing-samples/index.html",
    "href": "sessions/11-comparing-samples/index.html",
    "title": "Regression Fundamentals: Comparing Samples",
    "section": "",
    "text": "This notebook is the first in a five-part series covering foundational statistics and the building blocks for regression modelling. This session walks through the process of comparing samples, demonstrating why comparisons matter and how we approach them, and applies these ideas using data on fatal car crashes in the U.S.\nWe will discuss why we compare groups in data analysis, the differences between population and sample data, how to visualise and interpret group differences, and how to assess whether differences are statistically meaningful.\nThe goal of this session is for everyone to understand the role of comparison in statistics, as well as to recognise what comparisons between groups can tell us and what their limitations are.",
    "crumbs": [
      "Data Science",
      "11. Comparing Samples"
    ]
  },
  {
    "objectID": "sessions/11-comparing-samples/index.html#why-compare",
    "href": "sessions/11-comparing-samples/index.html#why-compare",
    "title": "Regression Fundamentals: Comparing Samples",
    "section": "Why Compare?",
    "text": "Why Compare?\nComparison is fundamental to how we learn from data. When we observe something in data, whether it is a specific observation, a broader pattern across observations, or the outcome of a calculation, we need context to understand its meaning. Is this value high or low? Is it unusual? Should we pay more attention to it?\nImagine an online store making changes to its website with the intention of boosting sales. After a week, the changes made generated £250k in sales. Is that good? We don’t know because we don’t have anything to compare against. The store could compare this figure to the sales from the previous week. Better yet, they could run the original version of the website concurrently alongside the new version, serving different versions of the website to users at random, to more directly compare sales. If the original version of the website generated only £230k in sales, we now have a meaningful comparison that suggests that the new version of the site may boost sales.\nRaw numbers rarely tell us complete stories. A patient’s blood pressure is only meaningful because we know what a healthy range is, and we can use this for comparison. Without comparison, data is meaningless.\n\nWhat does Comparison Really Tell Us?\nWhen we compare two groups in our data, what we really want to know is whether those groups differ in the real world, not just whether they differ in the data. The comparison in the data serves as a proxy for understanding differences in the wild. The data is a “sample” of what the real world (the population) looks like. But suppose we see a difference between two groups in our data. How do we know if that reflects a difference that is occurring in the population, instead of being caused by random variation in the data?\nSeparating real patterns, or signal, from the noise in data is a fundamental part of statistics and is the driving force behind everything in statistical inference. Good comparisons account for the possibility of random variation and consider the ways in which the comparisons we are making may be flawed or incomplete. If the online store compared the previous week’s sales, this would still be useful, but what if the previous week included a holiday that led to a significant boost of sales, or the week the new site was launched was payday for a lot of customers? It is important to consider whether your comparison is really meaningful.\nQuestions:\n\nHow do you currently decide whether the difference you observe in your data is real or occurred by chance?\nWhy is it important to know if differences observed in data occurred by chance?\n\n\n\nPopulation vs. Sample\nThe population is every possible unit or observation relevant to what you are studying, while the sample is a subset of the population. If you wanted to estimate how income affects housing prices in UK cities, the population would be every city in the UK. A sample would be data covering a handful of (hopefully representative) cities.\n\n\n\nSource: Martijn Wieling\n\n\nIf we had access to the entire population, comparisons would be straightforward. However, we usually don’t, so we have to take a sample of the population and make inferences about the population based on our sample. That means dealing with uncertainty, variation, and potential bias.\nTo compare groups responsibly, we need to consider how sampling affects what we observe and how it may limit our ability to make accurate comparisons. The sample is a small snapshot of the population, and there are several reasons why it might not be representative of the wider population.\nBelow is an example illustrating the difference between the population and the sample, simulating drawing ten cards from a standard deck and calculating the average value of the cards drawn.\n\nimport numpy as np\nimport random\n\n# simulate drawing 10 cards from a standard deck\ndeck = list(range(1, 14)) * 4\n\n# draw two random samples of ten cards\nsample1 = random.sample(deck, 10)\nsample2 = random.sample(deck, 10)\n\n# compute sample means\nsample_mean1 = np.mean(sample1)\nsample_mean2 = np.mean(sample2)\n\n# compute population mean\npopulation_mean = np.mean(deck)\n\nprint(f\"Sample means: {sample_mean1}, {sample_mean2}\")\nprint(f\"Population mean: {population_mean}\")\n\nSample means: 7.1, 8.4\nPopulation mean: 7.0\n\n\nWe have taken two samples from the population. Their mean values are 7.1 and 7.1, which vary slightly from the population mean (7.0). Why do the sample means differ from each other and the population mean?\nSampling variability is inevitable. Each sample captures only a slice of the full population, and in small samples, this can lead to significant variances in the sample and population means. Perhaps the first ten cards drawn from the deck have a high number of face cards, or the second sample has lots of 2s, 3s, and 4s. Even if the process for drawing a sample is fair, individual samples will always vary. This is a core challenge of inference. We rely on well-designed comparisons to manage these uncertainties, using statistical tools that help us determine whether sample-level observations likely reflect real population-level differences.",
    "crumbs": [
      "Data Science",
      "11. Comparing Samples"
    ]
  },
  {
    "objectID": "sessions/11-comparing-samples/index.html#comparing-car-crash-fatalities---high-or-not",
    "href": "sessions/11-comparing-samples/index.html#comparing-car-crash-fatalities---high-or-not",
    "title": "Regression Fundamentals: Comparing Samples",
    "section": "Comparing Car Crash Fatalities - High or Not?",
    "text": "Comparing Car Crash Fatalities - High or Not?\nNow we can apply this logic to a real-world dataset. We will use a Tidy Tuesday dataset that records the daily count of fatal U.S. car crashes from 1992–2016, originally from a study into the effects of the annual cannabis holiday (4:20pm to 11:59pm on April 20th), 4/20, on fatal car accidents.\nWhile previous research has concluded that fatalities are higher on 4/20, suggesting that the holiday is the cause of the increase, Harper and Palayew find no evidence for an increase in fatalities on 4/20, but they do find an increase for other holidays like July 4th. We will investigate whether we can see a spike in fatalities on 4/20 by comparing 4/20 with other days of the year.\n\nImport & Process Data\n\nimport pandas as pd\n\n# load data\nraw_420 = pd.read_csv('data/daily_accidents_420.csv', parse_dates=['date'])\n\n# inspect data\nraw_420.head()\n\n\n\n\n\n\n\n\ndate\ne420\nfatalities_count\n\n\n\n\n0\n1992-01-01\nFalse\n144\n\n\n1\n1992-01-02\nFalse\n111\n\n\n2\n1992-01-07\nFalse\n85\n\n\n3\n1992-01-12\nFalse\n127\n\n\n4\n1992-01-03\nFalse\n182\n\n\n\n\n\n\n\nAs always, we need to do some checks to inspect data quality and identify any potential issues.\n\n# count missing values\nraw_420.isna().sum()\n\ndate                 0\ne420                13\nfatalities_count     0\ndtype: int64\n\n\n\n# inspect missing values\nraw_420.loc[raw_420['e420'].isna()]\n\n\n\n\n\n\n\n\ndate\ne420\nfatalities_count\n\n\n\n\n1099\n1994-04-20\nNaN\n1\n\n\n1428\n1995-04-20\nNaN\n2\n\n\n1834\n1996-04-20\nNaN\n1\n\n\n2201\n1997-04-20\nNaN\n2\n\n\n3301\n2000-04-20\nNaN\n1\n\n\n4400\n2003-04-20\nNaN\n1\n\n\n5485\n2006-04-20\nNaN\n1\n\n\n5867\n2007-04-20\nNaN\n1\n\n\n6234\n2008-04-20\nNaN\n1\n\n\n6968\n2010-04-20\nNaN\n2\n\n\n7702\n2012-04-20\nNaN\n1\n\n\n8393\n2014-04-20\nNaN\n1\n\n\n8802\n2015-04-20\nNaN\n1\n\n\n\n\n\n\n\n\n# inspect 04/20 values\nraw_420.loc[raw_420['date'] == '1994-04-20']\n\n\n\n\n\n\n\n\ndate\ne420\nfatalities_count\n\n\n\n\n951\n1994-04-20\nTrue\n47\n\n\n1098\n1994-04-20\nFalse\n63\n\n\n1099\n1994-04-20\nNaN\n1\n\n\n\n\n\n\n\nThere are several observations where values for e420 are missing. All of the missing values are on 4/20, but closer inspection shows that there are multiple observations for these days. This appears to be because e420 == True only for the time period covering the holiday (which isn’t the entire day), and the NAs are likely to be crashes where the exact time was undetermined and may or may not fall in the 4/20 window.\nWe will transform the data to take the sum value of all 4/20 fatalities, for simplicity1, and select the columns we want to use in our analysis.\n\ndf = (\n    raw_420\n    # group by date, sum fatalities\n    .groupby(['date'], as_index=False)['fatalities_count'].sum()\n    # add 4/20 and 7/4 indicators\n    .assign(\n        is_420=lambda d: pd.to_datetime(d['date']).dt.strftime('%m-%d').eq('04-20'),\n        is_july4=lambda d: pd.to_datetime(d['date']).dt.strftime('%m-%d').eq('07-04')\n        )\n)\n\n\ndf.head()\n\n\n\n\n\n\n\n\ndate\nfatalities_count\nis_420\nis_july4\n\n\n\n\n0\n1992-01-01\n144\nFalse\nFalse\n\n\n1\n1992-01-02\n111\nFalse\nFalse\n\n\n2\n1992-01-03\n182\nFalse\nFalse\n\n\n3\n1992-01-04\n152\nFalse\nFalse\n\n\n4\n1992-01-05\n99\nFalse\nFalse\n\n\n\n\n\n\n\nFirst, lets calculate the mean value of fatalities on 4/20 and all other days, to see whether there is an obvious spike.\n\n(\n    df\n    .groupby('is_420')['fatalities_count']\n    .mean()\n    .round(2)\n)\n\nis_420\nFalse    145.09\nTrue     139.32\nName: fatalities_count, dtype: float64\n\n\nInstead, the average number of fatalities is slightly lower than the average across the rest of the year.\nWe can also add July 4th to our comparison, to see if other holidays cause a spike in the average number of fatalities.\n\n(\n    df.groupby(['is_420', 'is_july4'], as_index=False)['fatalities_count']\n    .mean()\n    .round(2)\n)\n\n\n\n\n\n\n\n\nis_420\nis_july4\nfatalities_count\n\n\n\n\n0\nFalse\nFalse\n145.01\n\n\n1\nFalse\nTrue\n176.00\n\n\n2\nTrue\nFalse\n139.32\n\n\n\n\n\n\n\nThere are a lot more fatalities from car crashes on July 4th than the rest of the year (including 4/20). This gives us a good starting point to start digging further into the data.\n\n\nVisual Comparisons\nWe can make comparisons in a variety of ways, from describing the difference between samples using descriptive measures (such as the mean value) to carrying out statistical tests that estimate the likelihood that observed differences between samples occur in the population. Visualising data is an effective way to make quick comparisons between samples. We can identify visual patterns much faster than we can using descriptive measures.\nWe can start by plotting our data to better understand how fatalities vary over time and how the number of car crash fatalities on 4/20 and July 4th compare to the rest of the year.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(15, 8))\n\n# line plot of daily fatalities\nsns.lineplot(data=df, x='date', y='fatalities_count', color=\"#0081a7\", linewidth=0.25, alpha=0.8)\n\n# scatter plot for 4/20\nsns.scatterplot(\n    data=df.loc[df['is_420']],\n    x='date', y='fatalities_count',\n    color='#ef233c', label='4/20',\n    s=100\n)\n\n# scatter plot for 7/4\nsns.scatterplot(\n    data=df.loc[df['is_july4']],\n    x='date', y='fatalities_count',\n    color='#ffb703', label='7/4',\n    s=100\n)\n\nplt.xlabel('Date')\nplt.ylabel('Car Crash Fatalities')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Daily Fatalities (1992-2016)\n\n\n\n\n\nFigure 1 shows that there is clearly a seasonal trend in daily car crash fatalities, and there is significant variance in fatalities on both 4/20 and July 4th. While this plot does tell us a lot about the data, it hasn’t made comparison easier.\nWe can visualise the distribution of samples as a way to make comparisons. This can help us identify differences in the shape of the samples, which tells us a lot. Figure 2 below compares 4/20 and all other days.\n\nplt.rcParams['figure.figsize'] = [12,6]\n\n# define colour palette\ncustom_palette = {False: '#0081a7', True: '#ef233c'}\n\n# histogram\nplt.subplot(1, 2, 1)\nsns.histplot(data=df, x='fatalities_count', hue='is_420', kde=True, palette=custom_palette)\nplt.title('Histogram')\n\n# boxplot\nplt.subplot(1, 2, 2)\nsns.boxplot(data=df, x='is_420', y='fatalities_count', hue='is_420', palette=custom_palette, legend=False)\nplt.xticks([0, 1], ['Other Days', '4/20'])\nplt.title('Boxplot')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Distribution of Fatalities on 4/20 & Other Days\n\n\n\n\n\nThe imbalance between 4/20 and other days in the year makes it impossible to really see what is going on in our histogram.\nWe can normalise the two distributions such that the total area of both equals one. This preserves their shape but accounts for the count imbalance between the two. We can also replace the boxplot with a violin plot, which will give us a little more intuition for the shape of the two groups.\n\nplt.subplot(1, 2, 1)\nsns.histplot(data=df, x='fatalities_count', hue='is_420', palette=custom_palette, kde=True, stat='density', common_norm=False)\nplt.title('Density-Normalised Histogram')\n\nplt.subplot(1, 2, 2)\nsns.violinplot(data=df, x='is_420', y='fatalities_count', inner='quart', hue='is_420', palette=custom_palette, legend=False)\nplt.xticks([0, 1], ['Other Days', '4/20'])\nplt.title('Violin Plot')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 3: (Normalised) Distribution of Fatalities on 4/20 & Other Days\n\n\n\n\n\nThere is minimal difference between the two distributions in Figure 3, though the peak of the 4/20 distribution does appear to be slightly lower than the other days.\nWe can also visualise the distribution of fatalities on July 4th, for comparison.\n\ncustom_palette = {False: '#0081a7', True: '#ffb703'}\n\nplt.subplot(1, 2, 1)\nsns.histplot(data=df, x='fatalities_count', hue='is_july4', palette=custom_palette, kde=True, stat='density', common_norm=False)\nplt.title('Density-Normalised Histogram')\n\nplt.subplot(1, 2, 2)\nsns.violinplot(data=df, x='is_july4', y='fatalities_count', inner='quart', hue='is_july4', palette=custom_palette, legend=False)\nplt.xticks([0, 1], ['Other Days', '7/4'])\nplt.title('Violin Plot')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 4: (Normalised) Distribution of Fatalities on July 4th & Other Days\n\n\n\n\n\nThe differences between the two distributions in Figure 4 are a lot clearer. While there is some overlap, the July 4th distribution shows that there are generally more deaths on July 4th than on other days.\nThere are many ways to make comparisons visually, and the plots above are just an example of some of the approaches you can use to compare samples. It’s always worth looking at the data in a variety of ways and finding the most appropriate ways to make comparisons. The right way to compare your samples depends on the question you are asking, so it’s important to think carefully about your approach and what it is telling you.\n\n\nTesting Comparisons\nVisual and descriptive comparisons are limited because they only tell us whether there is a difference. They don’t help us infer whether those difference occurred due to random variation or if there is something real going on. Visual comparisons cannot tell us whether we should expect to observe the differences we see in our samples in the population.\nThat’s where statistical tests come in! Once we’ve visualized potential differences, we can test whether those differences are likely to occur in the population, using a two-sample \\(t\\)-test.\nA \\(t\\)-test is a statistical test used to compare the means of two groups to determine if the difference between them is statistically significant. It takes into account:\n\nThe size of the difference between the two group means.\nThe variability (spread) of the data within each group.\nThe sample size (number of observations in each group).\n\nThe \\(t\\)-test calculates a p-value, which is the probability of observing a difference as extreme or more extreme than the one found, assuming there is no true difference between the groups in the population (i.e., the null hypothesis is true). The Null Hypothesis Significance Testing (NHST) framework sets a threshold (typically 0.05) where, if the \\(p\\)-value is below that threshold, you reject the null hypothesis and conclude that the difference between the groups is likely real and not due to random chance (referred to as statistically significant).\n\nHowever, statistical significance and the NHST are thorny issues. Instead, it is better to think of \\(p\\)-values as continuous instead of treating them as binary. If we compute a difference (or effect size) of 0.45 with a \\(p\\)-value of 0.01, that means we’d expect to see an effect size this large less than 1% of the time by chance alone, assuming no true relationship. Whether a difference of 0.45, with a 1% chance of this occuring by chance, is enough for us to conclude this is substantively important is dependent on context.\n\n\n\n\n\n\nNoteWhy Significance Is a Thorny Issue\n\n\n\nWhile \\(p\\)-values are widely used, they come with caveats:\n\nBinary thinking - Treating \\(p\\) &lt; 0.05 as “proof” of an effect and rejecting \\(p\\) &gt; 0.05 as no effect is misleading. Evidence exists on a continuum.\nSample size matters - With huge samples, even tiny effects can be “significant”. With small samples, meaningful effects might not reach the threshold.\nMisinterpretation - A \\(p\\)-value doesn’t tell us how big or important an effect is, nor the probability that the hypothesis is true.\nMultiple testing - The more tests we run, the more likely we’ll find “significant” results by chance (false positives).\n\nThe better approach is to treat \\(p\\)-values as one piece of evidence. Pair them with effect sizes, confidence intervals, visualizations, and context.\n\n\nLet’s first carry out a \\(t\\)-test to compare fatalities on 4/20 and all other days.\n\nfrom scipy.stats import ttest_ind\n\n# create our samples for comparison\ngroup_420 = df.loc[df.is_420, 'fatalities_count']\ngroup_other = df.loc[~df.is_420, 'fatalities_count']\n\n# calculate t-statistic and p-value\nt_stat, p_val = ttest_ind(group_420, group_other, equal_var=False)\nprint(f\"t-statistic = {t_stat:.3f}, p-value = {p_val:.3f}\")\n\n# calculate mean difference\nmean_diff = group_420.mean() - group_other.mean()\n\nprint(f\"Mean difference = {mean_diff:.2f}\")\n\nt-statistic = -0.863, p-value = 0.397\nMean difference = -5.77\n\n\nThere is a difference in the means of the two samples of almost 6 fatalities, but the \\(p\\)-value is almost 0.4, which suggests we are very uncertain about whether this is a meaningful effect or just the product of random variation. A high \\(p\\) suggests we probably need more data, or a more precise approach to estimating 4/20s effect, before we can draw any conclusions.\n\nWhile our \\(t\\)-test found minimal evidence that there is a meaningful difference in the number of fatalities by car crash on 4/20, perhaps the bigger mean difference we saw on July 4th will be sufficient to draw some conclusions.\n\nfrom scipy.stats import ttest_ind\n\n# create our samples for comparison\ngroup_july4 = df.loc[df.is_july4, 'fatalities_count']\ngroup_other = df.loc[~df.is_july4, 'fatalities_count']\n\n# calculate t-statistic and p-value\nt_stat, p_val = ttest_ind(group_july4, group_other, equal_var=False)\nprint(f\"t-statistic = {t_stat:.3f}, p-value = {p_val:.3f}\")\n\n# calculate mean difference\nmean_diff = group_july4.mean() - group_other.mean()\n\nprint(f\"Mean difference = {mean_diff:.2f}\")\n\nt-statistic = 6.381, p-value = 0.000\nMean difference = 31.01\n\n\nThere are, on average, 31 more fatalities on July 4th than the rest of the year, and the \\(p\\)-value is so small that it rounds to zero. This indicates that it is highly likely that this is a real effect that we should expect to observe in the population.\nHowever, it is important to note that this does not confirm our theory that the cause of this spike in fatalities is July 4th (the holiday). We are only able to conclude that this difference is not caused by random variation, but our test does not prove what has caused it.\n\n\nUsing Simulation for Comparisons (Click to Expand)\n\n\nSimulation-Based Tests\nFor anyone that struggles to remember all the different types of statistical tests you can use to compare samples and when a particular test is appropriate, I would highly recommend learning the Simulation-Based Testing framework, which uses simulation methods to approximate the process that statistical tests carry out when comparing samples.\nA detailed explanation of how this approach to testing works is out of the scope of this session, but below is a replication of our tests using a simulation-based method.\n\ndef simulate_two_groups(data1, data2):\n\n    n, m = len(data1), len(data2)\n    data = np.append(data1, data2)\n    np.random.shuffle(data)\n    group1 = data[:n]\n    group2 = data[n:]\n    return group1.mean() - group2.mean()\n\n\n# run 10000 simulations to test null\nnp.random.seed(42)\nsimulated_diffs = [simulate_two_groups(group_420, group_other) for _ in range(10000)]\n\n# observed mean difference\nobserved_diff = group_420.mean() - group_other.mean()\n\n# calculate p-value\ndiffs = np.array(simulated_diffs)\np_sim = np.mean(np.abs(diffs) &gt;= np.abs(observed_diff))\n\n# plot distribution of simulated differences with p-value\nsns.histplot(diffs, kde=True, color='#0081a7')\nplt.axvline(observed_diff, color='#ef233c', linewidth=3, linestyle=\"--\", label='Observed Mean Difference')\nplt.legend(loc='upper right')\n\n# annotate p-value on the plot\nplt.text(\n    x=observed_diff+8,\n    y=plt.gca().get_ylim()[1]*0.9,\n    s=f'p-value = {p_sim:.4f}'\n    )\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nPermutation Test (4/20 Vs Other Days)\n\n\n\n\nSimulation confirms the result and emphasizes flexibility: even when assumptions are questionable, we can still test meaningfully. It also reinforces that inference is about what would happen if we repeated the experiment many times.\n\n# run 10000 simulations to test null\nnp.random.seed(42)\nsimulated_diffs = [simulate_two_groups(group_july4, group_other) for _ in range(10000)]\n\n# observed mean difference\nobserved_diff = group_july4.mean() - group_other.mean()\n\n# calculate p-value\ndiffs = np.array(simulated_diffs)\np_sim = np.mean(np.abs(diffs) &gt;= np.abs(observed_diff))\n\n# plot distribution of simulated differences with p-value\nplt.figure(figsize=(12, 6))\nsns.histplot(diffs, kde=True, color='#0081a7')\nplt.axvline(observed_diff, color='#ef233c', linewidth=3, linestyle=\"--\", label='Observed Mean Difference')\nplt.legend(loc='upper right')\nplt.title('')\n\n# annotate p-value on the plot\nplt.text(\n    x=observed_diff-20,\n    y=plt.gca().get_ylim()[1]*0.9,\n    s=f'p-value = {p_sim:.4f}'\n    )\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nPermutation Test (7th July Vs Other Days)",
    "crumbs": [
      "Data Science",
      "11. Comparing Samples"
    ]
  },
  {
    "objectID": "sessions/11-comparing-samples/index.html#limitations-of-comparison",
    "href": "sessions/11-comparing-samples/index.html#limitations-of-comparison",
    "title": "Regression Fundamentals: Comparing Samples",
    "section": "Limitations of Comparison",
    "text": "Limitations of Comparison\nComparing samples of data can be very useful. There is descriptive value in just knowing that differences exist in the data, and this may point to a meaningful difference in the population. However, if you are trying to understand what caused the differences between the two samples, comparison is not enough.\nOur analysis shows that there are fewer crashes on 4/20 than other days, but there was considerable uncertainty about this comparison, and the approach had significant flaws. Our comparison assumes that the only difference between 4/20 and other days in the data is the date itself, and the cultural holiday that takes place on this date. We haven’t accounted for other causes of variation in the number of crashes. We haven’t accounted for other holidays, weather patterns, or daily differences in the number of people on the road.\nComparisons only measure what we observe, not necessarily what we want to know. It is important to consider how your comparison might not answer the question you are actually asking. And it is important to consider ways that your comparison may be flawed, and what else may be going on in the data.",
    "crumbs": [
      "Data Science",
      "11. Comparing Samples"
    ]
  },
  {
    "objectID": "sessions/11-comparing-samples/index.html#wrapping-up",
    "href": "sessions/11-comparing-samples/index.html#wrapping-up",
    "title": "Regression Fundamentals: Comparing Samples",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nWe’ve walked the workflow for carrying out comparisons using statistical methods. With these methods we can make meaningful comparisons between samples in our data. This gives us a solid foundation for carrying out analysis. We can identify differences, quantify uncertainty, and make inferences from data. But these methods have limitations. They don’t account for multiple variables or continuous predictors. Context and sample size also matter.\nIn future sessions we will take this a step further, analysing how variables relate to each other. That lets us ask new types of questions. We’ll explore how variables change together, detect trends, and lay the foundation for regression.",
    "crumbs": [
      "Data Science",
      "11. Comparing Samples"
    ]
  },
  {
    "objectID": "sessions/11-comparing-samples/index.html#footnotes",
    "href": "sessions/11-comparing-samples/index.html#footnotes",
    "title": "Regression Fundamentals: Comparing Samples",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is definitely not an appropriate way to deal with this problem if we were being precise, but in order to save on time this approach makes sense. For a more complete analysis, check out the Harper & Palayew paper this data is taken from.↩︎",
    "crumbs": [
      "Data Science",
      "11. Comparing Samples"
    ]
  },
  {
    "objectID": "sessions/schedule.html",
    "href": "sessions/schedule.html",
    "title": "Code Club Essentials Schedule",
    "section": "",
    "text": "This is the schedule and contents page for Code Club’s coding and data science essentials for FY25/26.\nThe Demonstration, Presentation, and Notebook columns indicate the content to be expected for each session:\n\nDemonstration: A live show-and-tell relating to the discussion topic.\nPresentation: A slide deck covering the discussion topic.\nNotebook: A Jupyter Notebook containing code-along elements or examples for people to work through after the session.\n\nTutorials will be divided into Modules. We recommend that people attend or watch tutorials in the Core module in order to gain a fundamental understanding of coding concepts and resources. Further modules are to be confirmed, but will likely include Automation, Dashboards and Visualisation, and Data Science. People will be able to attend those modules that interest them.\nWe have mapped the contents of the syllabus to competencies in the National Competency Framework for Data Professionals in Health and Care so that you can see which sessions will help you on your way towards them. For full details of the skills in the framework, the self-assessment tool can be found on FutureNHS.\nPlease note that this is a first draft of the mapping of NCF competencies to our syllabus and it is awaiting review.\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  Session Date\n  Module\n  Session Name\n  Description\n  Demonstration\n  Presentation\n  Notebook\n  NCF Competency\n\n\n\n  \n    22/04/2025\n    N/A\n    Nectar Re-Launch\n    Showcasing the re-launch of Code Club\n    🎬\n    💻\n    -\n    -\n  \n  \n    01/05/2025\n    On-boarding\n     Python On-boarding\n    What to install, basic virtual environment management, introduction to VS Code\n    🎬\n    -\n    -\n    SA21 : Python Proficiency\n  \n  \n    15/05/2025\n    On-boarding\n    Jupyter Notebooks\n    Demonstration of Jupyter Notebooks\n    🎬\n    -\n    📖\n    SA21 : Python Proficiency\n  \n  \n    29/05/2025\n    Exploration & Visualisation\n    EDA with Pandas\n    Introduction to exploratory data analysis using pandas\n    🎬\n    -\n    📖\n    SA4 : Descriptive and Explicative Analytics\n  \n  \n    12/06/2025\n    Exploration & Visualisation\n    Visualisation with Seaborn\n    Aesthetically-pleasing visualisations\n    🎬\n    -\n    📖\n    SA1 : Data Visualisation\n  \n  \n    03/07/2025\n    Exploration & Visualisation\n    EDA with Seaborn\n    Using seaborn to visualise and explore data\n    🎬\n    -\n    📖\n    SA4 : Descriptive and Explicative Analytics\n  \n  \n    17/07/2025\n    Core concepts\n    Data Types\n    Introduction to data types\n    -\n    💻\n    📖\n    SA21 : Python Proficiency\n  \n  \n    31/07/2025\n    Core concepts\n    Control Flow\n    Introduction to control flow\n    -\n    💻\n    📖\n    SA21 : Python Proficiency\n  \n  \n    14/08/2025\n    Core concepts\n    Functions & Functional Programming\n    Introduction to functions and functional programming\n    -\n    💻\n    📖\n    SA21 : Python Proficiency\n  \n  \n    28/08/2025\n    Core concepts\n    Object-Oriented Programming\n    Introduction to object-oriented programming\n    -\n    💻\n    📖\n    SA21 : Python Proficiency\n  \n  \n    11/09/2025\n    Exploration & Visualisation\n    Streamlit dashboards\n    How to present data (visualisations) in a Streamlit dashboard\n    🎬\n    -\n    -\n    SA1 : Data Visualisation\n  \n  \n    25/09/2025\n    Data Science\n    Comparing Samples\n    Understanding data distributions and comparisons between samples\n    -\n    💻\n    📖\n    SA4 : Descriptive and Explicative Analytics\n  \n  \n    09/10/2025\n    Data Science\n    Analysing Relationships\n    Quantifying relationships with hypothesis tests and statistical significance\n    -\n    💻\n    📖\n    SA15 : Hypothesis Testing\n  \n  \n    23/10/2025\n    Data Science\n    Introduction to Linear Regression\n    Introduction to regression concepts and the component parts of linear regression\n    -\n    💻\n    📖\n    SA7 : Advanced Statistics\n  \n  \n    06/11/2025\n    Data Science\n    Implementing Linear Regression\n    Building linear models, assessing model fit, and interpreting coefficients\n    🎬\n    💻\n    📖\n    SA7 : Advanced Statistics\n  \n  \n    20/11/2025\n    Data Science\n    Beyond Linearity\n    Introduction to generalised linear regression models\n    🎬\n    💻\n    📖\n    SA7 : Advanced Statistics",
    "crumbs": [
      "Session Schedule"
    ]
  },
  {
    "objectID": "sessions/10-streamlit/index.html",
    "href": "sessions/10-streamlit/index.html",
    "title": "Streamlit",
    "section": "",
    "text": "It is a Python library and a web platform that enables coders like you to create interactive, web-based dashboards and apps entirely1 using Python; no need to learn a language like Javascript.\nTo get an overview of its capabilities, you can visit the official website here.\nYou can see some examples of apps and dashboards created by the Streamlit community here, or you could catch up with the recording of the Code Club session recording when we showcased our own!\n\n\n\n\nTo view the dashboard in a separate browser tab, click here. The inspiration for this dashboard was a piece of work that required Providers to send data in Excel or .csv files to a team inbox for it to be processed, uploaded to the warehouse, imported back into Excel to be visualised, only for it to be sent back out to Providers. The demonstration dashboard illustrates how it would be possible to create a web-based app where Providers could upload their own data, explore and visualise it, and finally apply some kind of pre-built model to the data. In this case, we have applied a couple of forecasting models to the data: one traditional statistical model and one modern machine learning-based model. If you would like to pinch learn from the code we used to create our dashboard, you can find it here. The app.py script is the one that is run to pull all of the pages together (see more in the “How to get started” section). The “Pages” folder contains the scripts for the individual pages.\n\n\n\n\n\n\nNoteDemo data\n\n\n\nIf you would like to make use of the data used in the demonstration, please contact your Code Club hosts via the Teams channel or write to the Specialist Analytics Team",
    "crumbs": [
      "Exploration & Visualisation",
      "10. Streamlit Dashboards"
    ]
  },
  {
    "objectID": "sessions/10-streamlit/index.html#what-is-streamlit",
    "href": "sessions/10-streamlit/index.html#what-is-streamlit",
    "title": "Streamlit",
    "section": "",
    "text": "It is a Python library and a web platform that enables coders like you to create interactive, web-based dashboards and apps entirely1 using Python; no need to learn a language like Javascript.\nTo get an overview of its capabilities, you can visit the official website here.\nYou can see some examples of apps and dashboards created by the Streamlit community here, or you could catch up with the recording of the Code Club session recording when we showcased our own!\n\n\n\n\nTo view the dashboard in a separate browser tab, click here. The inspiration for this dashboard was a piece of work that required Providers to send data in Excel or .csv files to a team inbox for it to be processed, uploaded to the warehouse, imported back into Excel to be visualised, only for it to be sent back out to Providers. The demonstration dashboard illustrates how it would be possible to create a web-based app where Providers could upload their own data, explore and visualise it, and finally apply some kind of pre-built model to the data. In this case, we have applied a couple of forecasting models to the data: one traditional statistical model and one modern machine learning-based model. If you would like to pinch learn from the code we used to create our dashboard, you can find it here. The app.py script is the one that is run to pull all of the pages together (see more in the “How to get started” section). The “Pages” folder contains the scripts for the individual pages.\n\n\n\n\n\n\nNoteDemo data\n\n\n\nIf you would like to make use of the data used in the demonstration, please contact your Code Club hosts via the Teams channel or write to the Specialist Analytics Team",
    "crumbs": [
      "Exploration & Visualisation",
      "10. Streamlit Dashboards"
    ]
  },
  {
    "objectID": "sessions/10-streamlit/index.html#how-can-it-benefit-me-as-an-analyst",
    "href": "sessions/10-streamlit/index.html#how-can-it-benefit-me-as-an-analyst",
    "title": "Streamlit",
    "section": "How can it benefit me as an analyst?",
    "text": "How can it benefit me as an analyst?\n\nSince everything is built using Python scripts, it is easy to integrate data processing, modelling and visualisation into one app, rather than requiring separate applications to handle different elements.\nEnables the flexibility of making use of Python’s diverse range of libraries. They can be used to create finely customised visualisations and greater levels of interactivity. Streamlit itself gets updated all the time with new functionality that often just requires one line of code to implement!\nIt is an excellent way to practice your Python coding since it brings lots of elements together:\n\nProcessing data with dataframes.\nVisualisations.\nMore advanced control flow that takes user interactions into account.\nInteracting with GitHub (when you want to have your dashboard/app hosted online).\nWriting Python as complete scripts (as opposed to working cell by cell in Jupyter), which is much more like the reality of developing real-life applications.\n\nDeveloping an app or dashboard using pure code makes version control much easier than trying to do so using proprietary software. Such software can often be over-engineered with lots of things that you don’t really need, and active settings can often be hidden in layer upon layer of menus📎. It is much easier to keep on top of which settings have been applied when you have everything in front of you in a well-commented script.\nIt is free, open-source software, so you do not need to be assigned a potentially expensive licence before you can get started and start experimenting.\nStreamlit can be used as a way to bring complex analytical models to life:\n\nThe PenCHORD team at the University of Exeter, who run the Health Service Modelling Associates course, have produced a web app that allows users to interact with a Discrete Event Simulation model in order to learn how they work. You could build something similar to this to allow stakeholders to interact with your data modelling, enabling them to test different scenarios. For inspiration, you can find a list of real-life healthcare projects undertaken by HSMA alumni that have a Streamlit interface here.",
    "crumbs": [
      "Exploration & Visualisation",
      "10. Streamlit Dashboards"
    ]
  },
  {
    "objectID": "sessions/10-streamlit/index.html#how-to-get-started",
    "href": "sessions/10-streamlit/index.html#how-to-get-started",
    "title": "Streamlit",
    "section": "How to get started",
    "text": "How to get started\n\nInstallation\n\nStart a new project as outlined in the Project Setup section of the “Python Onboarding” materials.\nYou will need to install the Streamlit package with the Powershell command uv add streamlit. You will probably also want to install a package for handling dataframes such as pandas or polars, and a visualisation package such as seaborn or altair2.\n\n\n\nYour first app\nStreamlit apps are written in standard .py Python files. It is convention to use the alias “st” when importing the package, i.e. import streamlit as st.\nYour first app can be as simple as the examples in the animation on the Streamlit homepage. Streamlit contains a very handy method called .write(), which can be used to render all sorts of things, not just text. You can also use the .markdown() method to render text using markdown notation.\nIf you want to see a very simple example and have a play around, have a look at Streamlit’s Playground\nOnce you have written your script and want to run it locally in a browser window:\n\nActivate your Python environment by entering the following command in Powershell: ./.venv/Scripts/activate.ps1.\n\nYou many need to swap “venv” with the specific name of your virtual environment, if you have given it a name.\n\nLaunch the app by entering this command in Powershell: python -m streamlit run app.py, where “app.py” is the name of the Python file containing your script.\n\nYour app will be launched in a browser window on a localhost.\n\n\n\n\n\n\nTipBefore your very eyes…\n\n\n\nThe great thing about working with Streamlit is that you can see the effects of changes to your script more-or-less immediately. Save changes to your .py file and refresh the browser to render the changes.",
    "crumbs": [
      "Exploration & Visualisation",
      "10. Streamlit Dashboards"
    ]
  },
  {
    "objectID": "sessions/10-streamlit/index.html#deployment",
    "href": "sessions/10-streamlit/index.html#deployment",
    "title": "Streamlit",
    "section": "Deployment",
    "text": "Deployment\nIf you would like to have your app / dashboard hosted online, the recommended options are to get a free account on one of the following:\n\nStreamlit Community Cloud\nPosit Cloud\n\nNote that you will also need a free GitHub account. This is where you will upload your code and your environment requirements. Streamlit / Posit will use that information when rendering the app. You will need to set your repository to “Public” visibility.\nIf you need help pushing the necessary files to GitHub, please do feel free to contact your hosts at Code Club. If you want to get an idea of how your repository should look, have another look at the repository for the demonstration dashboard.\n\n\n\n\n\n\nImportantIG Alert!\n\n\n\nIf you are going to be sharing your app / dashboard online, make sure that you are following all Information Governance requirements for sharing data publicly. Your masterpiece and its repository will be freely accessible to the public. It is advised that you stick to using data that has already been published, such as Fingertips data, or to create a proof-of-concept using dummy or synthetic data. If you are creating an app where end-users upload their own data, make sure that you include any test data in your .gitignore so that it does not get uploaded to GitHub.\n\n\n\n\n\n\n\n\nWarningProduction servers\n\n\n\nPlease be aware that SCW does not currently have a production server licences, so if you have an idea for something that contains patient-level or commissioning data that you want to be able to share with customers, please contact the Specialist Analytics Team.",
    "crumbs": [
      "Exploration & Visualisation",
      "10. Streamlit Dashboards"
    ]
  },
  {
    "objectID": "sessions/10-streamlit/index.html#footnotes",
    "href": "sessions/10-streamlit/index.html#footnotes",
    "title": "Streamlit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWell, almost entirely. You may find you need to tweak the layout of the pages using some HTML, but snippets of code for this purpose are easy to come by. It is normally something like fixing the width of the sidebar. The important thing is that all of the elements that do something, i.e. process data, produce charts and enable interactivity, are programmed in Python.↩︎\nStreamlit has some methods that are designed specifcally for rendering outputs from certain packages, such as .altair_chart() for Altair charts and .pyplot() for rendering matplotlib.pyplot charts. They can often be used to tweak the default rendering: By default, rendering a Pandas dataframe will include the index. Using .dataframe() to render the dataframe allows you to hide the index, e.g. st.dataframe(df, hide_index= True).↩︎",
    "crumbs": [
      "Exploration & Visualisation",
      "10. Streamlit Dashboards"
    ]
  }
]