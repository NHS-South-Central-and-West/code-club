{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Webscraping and APIs\n",
        "\n",
        "Ever wanted to automate the downloading of data from external websites?\n",
        "Ever had a request where you need historic data from NHS publications\n",
        "where each .csv file is on its own web page and you are faced with\n",
        "negotiating dozens of clicks and dialogue boxes? Ever wanted to pipe\n",
        "public data directly into a report to provide wider context entirely\n",
        "fuss-free? Then you have come to the right tutorial. We are going to\n",
        "cover two methods for doing these very things, things that you can take\n",
        "away and apply today: no highfalutin concepts to digest, no need to hold\n",
        "out hope for a juicy data science project to get your teeth into. What’s\n",
        "more, we are going to give you some examples of public datasets that you\n",
        "can try these out on, giving you data with which to exercise your other\n",
        "Python skills.\n",
        "\n",
        "The first method we are going to cover is webscraping, which is\n",
        "basically a way to retrieve elements on web pages by accessing them via\n",
        "the HTML tags (stay with us, there’s no need to become an expert in\n",
        "HTML, the Python package is going to do the hard work).\n",
        "\n",
        "The second method is to access data made available via APIs (Application\n",
        "Programming Interfaces). We will specifically be looking at APIs that\n",
        "provide the data in JSON format, something which only requires minimal\n",
        "manipulation to put it into a dataframe.\n",
        "\n",
        "However, before we get to the good stuff, there are a few things to\n",
        "touch on that will prove essential when using these two methods. They\n",
        "relate to accessing URLs (web links) dynamically and making requests to\n",
        "content via the web. We will give you a brief introduction to prime you\n",
        "for when they come up later.\n",
        "\n",
        "## Regular Expressions\n",
        "\n",
        "This is a very important part of webscraping and making API calls since\n",
        "you are often interested in accessing web pages, hosted files and API\n",
        "endpoints based on a URL pattern. Where it becomes useful is when you\n",
        "want to programmatically access any URLs or file names that match a\n",
        "pattern, but contain an element that can vary: for example, when there\n",
        "are monthly editions of a publication, where the file name or URL\n",
        "contains the name of the month. Take the example below:"
      ],
      "id": "e4d16dca-2265-4a05-a76c-8f51f96dbcd0"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "dynamic_section = r'^england-[a-z]+-202[0-9]$'"
      ],
      "id": "regex-example"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This regular expression is intended to represent the changing part of a\n",
        "URL that points to the specific pages where monthly publications are\n",
        "hosted. Let’s look at the elements:\n",
        "\n",
        "-   The “r” in front of the string tells Python that it should handle\n",
        "    whatever comes between the quotation marks as a **raw** string,\n",
        "    which is to say that it should ignore any of Python’s conventions\n",
        "    around special characters, such as backslash being an escape\n",
        "    character, and that it should pass the regex string to the `re`\n",
        "    functions without manipulating it in any way.\n",
        "-   The “^” states that the following characters must come at the\n",
        "    beginning of the string that you are searching, i.e. nothing should\n",
        "    come before it.\n",
        "-   The “\\$” denotes the end of the string, i.e. nothing should come\n",
        "    after it.\n",
        "-   The string must start with the sequence “england-”.\n",
        "-   It must end with the sequence “-202x”, where x is any digit from 0\n",
        "    to 9.\n",
        "-   And the “\\[a-z\\]+” means that the string will contain one or more\n",
        "    (the “+”) lower case alphabetical characters (“\\[a-z\\]”, i.e. any\n",
        "    lower case letter in the range between the square brackets).\n",
        "\n",
        "Taking the following as the base URL…\n",
        "\n",
        "`'https://digital.nhs.uk/data-and-information/publications/statistical/learning-disabilities-health-check-scheme'`\n",
        "\n",
        "The following would be valid URLs matching the pattern of\n",
        "`dynamic_section`:\n",
        "\n",
        "`https://digital.nhs.uk/data-and-information/publications/statistical/learning-disabilities-health-check-scheme/england-january-2024`\n",
        "\n",
        "`https://digital.nhs.uk/data-and-information/publications/statistical/learning-disabilities-health-check-scheme/england-may-2021`\n",
        "\n",
        "But the following would be ignored:\n",
        "\n",
        "`https://digital.nhs.uk/data-and-information/publications/statistical/learning-disabilities-health-check-scheme/england-quarter-4-2020-21`\n",
        "\n",
        "If you want a hand with creating a regular expression that does the job\n",
        "for your situation, <https://pythex.org> has an interface that can be\n",
        "used to test regex patterns against text springs, as well as a regex\n",
        "cheatsheet. The [Geeks for\n",
        "Geeks](https://www.geeksforgeeks.org/python/python-regex/) tutorial has\n",
        "a good set of examples of what each regex character sequence could be\n",
        "used to target.\n",
        "\n",
        "> **Choose the right library**\n",
        ">\n",
        "> The Python library to use when constructing regular expressions is\n",
        "> `re`. There is also a library called `regex`, but it is an older\n",
        "> library with less functionality. You do not need to explicitly\n",
        "> *install* `re` since it comes with Python when that is installed, but\n",
        "> you still need to *import* it.\n",
        "\n",
        "## The REST standard and GET requests\n",
        "\n",
        "When it comes to making requests for data online, it is important to\n",
        "understand a little about how they are made and the standard that\n",
        "underpins them. The requests that we are concerned with follow the\n",
        "**REST** (**RE**presentational **S**tate **T**ransfer) standard. REST\n",
        "guides the design of processes, standardising and simplifying the\n",
        "communication of requests for data hosted on web servers. As a result,\n",
        "operations are made using a standard set of terms. The most common ones\n",
        "are listed below:\n",
        "\n",
        "-   **GET**: is used when you want to **read** data on the server.\n",
        "-   **POST**: is used to **create** data.\n",
        "-   **PATCH** (or PUT): is used to **update** data.\n",
        "-   **DELETE**: no surprise, is used to **delete** data.\n",
        "\n",
        "Since this tutorial is teaching you to be a consumer of this data, we\n",
        "are really only interested in GET requests. Whether you are webscraping\n",
        "or making a request to an API endpoint, you will be making a GET\n",
        "request. The first step of each is to make a GET request using the\n",
        "`request` library and checking which response is returned:\n",
        "\n",
        "-   A response of **200** is positive, i.e. there is data to be had via\n",
        "    the supplied URL.\n",
        "-   **400** is a negative response.\n",
        "-   **304** is a “not modified” or “no new data” reponse, which will\n",
        "    come up again later when we cover API call etiquette.\n",
        "-   These codes are often built into try/except or if/else blocks to\n",
        "    govern what happens when there is / isn’t any available data.\n",
        "-   There are [many other\n",
        "    codes](https://restfulapi.net/http-status-codes/) that you may wish\n",
        "    to handle, particularly if you want to generate informative error\n",
        "    messages, but the three listed above should be enough to get you\n",
        "    started."
      ],
      "id": "4618ccd7-f0b5-4b41-95c0-b5e8dc8ee566"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import request as req\n",
        "\n",
        "url = [...] # the target URL\n",
        "\n",
        "response = req.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "  # do something with the data\n",
        "else:\n",
        "  print(f'Failed to fetch webpage: {response.status_code}')"
      ],
      "id": "get-response-example"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have introduced those concepts, we can start having a look\n",
        "at the things you were promised.\n",
        "\n",
        "## Webscraping\n",
        "\n",
        "This is a really handy tool for automating the extraction from a web\n",
        "page of anything that is encoded with HTML tags. It can be used to:\n",
        "\n",
        "-   Download files hosted on the web server and made available via\n",
        "    hyperlinks; for example, monthly / annual data publications.\n",
        "-   Copy down data tables appearing on a web page and converting them to\n",
        "    a Pandas dataframe; for example, data collection deadlines / data\n",
        "    dissemination dates.\n",
        "-   Copy text from the page title, headers or the body of the page.\n",
        "-   Copy images, hyperlinks, mailto links, iframes…\n",
        "\n",
        "### HTML tags\n",
        "\n",
        "These are important since they encode the structure of a web page.\n",
        "Understanding these gives you an idea of what is possible when it comes\n",
        "to accessing elements of the structure of a web page.\n",
        "\n",
        "There’s a nice compact HTML cheatsheet from [Stanford\n",
        "University](https://web.stanford.edu/group/csp/cs21/htmlcheatsheet.pdf),\n",
        "but if you are not into the whole brevity thing, [Geeks for\n",
        "Geeks](https://www.geeksforgeeks.org/html/html-cheat-sheet/) has got you\n",
        "covered again with a nicely laid-out explanation of each.\n",
        "\n",
        "### Beautiful Soup\n",
        "\n",
        "One of the most commonly used libraries for webscraping is Beautiful\n",
        "Soup. It parses the HTML and allows the user to access the elements\n",
        "using familiar Python syntax. The\n",
        "[documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "is very comprehensive, and the Quick Start section provides plenty of\n",
        "useful, simple examples.\n",
        "\n",
        "To install it in your Python environment, enter `uv add beautifulsoup4`\n",
        "into your terminal.\n",
        "\n",
        "Note that when you import it into your script, you write:\n",
        "`from bs4 import BeautifulSoup`.\n",
        "\n",
        "### Examples\n",
        "\n",
        "Let’s see how this is used in practise. First of all, we will install\n",
        "the packages that we are going to be using."
      ],
      "id": "8bacb13a-a59f-4087-8760-6a8ed7e6de71"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup # the webscraping library\n",
        "\n",
        "import requests as req # the web request library\n",
        "\n",
        "import re # the regular expressions library that comes with the Python installation.\n",
        "\n",
        "# this will help us construct dynamic URLs from different elements joined together.\n",
        "# it can also be used to make HTTP requests.\n",
        "# it is installed by entering \"uv add urllib3\" into your terminal\n",
        "from urllib.parse import urljoin \n",
        "\n",
        "# used for converting files into binary data, which can then be converted into \n",
        "# other formats.\n",
        "# it comes as part of the Python installation.\n",
        "from io import StringIO \n",
        "\n",
        "import pandas as pd # so that we can store our data in a dataframe\n",
        "\n",
        "import os # operating system functions, such as accessing file directories"
      ],
      "id": "import-packages"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### A simple request to retrieve a web page title\n",
        "\n",
        "> **Instantiation**\n",
        ">\n",
        "> Note that it is conventional to instantiate a BeautifulSoup parser\n",
        "> object as “soup”."
      ],
      "id": "3fdcf282-05c1-4948-a3ea-7b68c33460dd"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Webpage title: Our values - NHS SCW Support and Transformation for Health and Care"
          ]
        }
      ],
      "source": [
        "url = 'https://www.scwcsu.nhs.uk/about/our-values' # define the url in question\n",
        "\n",
        "response = req.get(url) # define the response as a GET request to the URL\n",
        "\n",
        "# if there is a positive reponse to the request, create a BeautifulSoup parser object\n",
        "# that collects the parsed content of the response.\n",
        "# Then print the web page \"title\" element.\n",
        "# The parser library being used is Python's in-built \"html.parser\"\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    print('Webpage title:', soup.title.string)\n",
        "\n",
        "# otherwise, return a helpful error message\n",
        "else:\n",
        "    print(f'Failed to fetch webpage: {response.status_code}')"
      ],
      "id": "page-title"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a list of alternative parser libraries, see\n",
        "[this](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser)\n",
        "section of the BeautifulSoup documentation.\n",
        "\n",
        "#### Display the full HTML of a web page.\n",
        "\n",
        "If you want to inspect all of the HTML code for a given page, so that\n",
        "you can get an idea of what is available, you can use the `prettify()`\n",
        "method. This re-uses the same `soup` object defined above. The output of\n",
        "the code will not be produced since it has been deactivated so that it\n",
        "does not take up too much space on our website. We recommend that you\n",
        "run it in a downloaded copy of the accompanying Jupyter Notebook."
      ],
      "id": "3c54975e-18ac-473c-84c4-ffdb995326c6"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(soup.prettify())"
      ],
      "id": "full-html"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Scrape information from a table on a web page."
      ],
      "id": "04b42410-20d9-4911-8d69-7f11f9e0a0ec"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'bs4.element.ResultSet'>"
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "url = ('https://digital.nhs.uk/data-and-information/data-collections-and-data-sets/data-sets/mental-health-services-data-set/submit-data')\n",
        "\n",
        "response = req.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "tables = soup.find_all('table') # if you are sure there is only one, use \"soup.find()\"\n",
        "print(type(tables))\n",
        "\n",
        "# get the first item in the BeautifulSoup ResultSet, convert it into a string, \n",
        "# and read the html into a pandas DataFrame\n",
        "table_df = pd.read_html(str(tables))[0]     # if you are sure there is just one, no need to select by index \"[0]\"\n",
        "\n",
        "table_df"
      ],
      "id": "cell-scrape-table"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Locate a .csv file on a webpage.\n",
        "\n",
        "Most of the code is the same as the “title” example, but this time we\n",
        "are looking for a hyperlink on the page that points to a .csv file (that\n",
        "is to say, the URL ends with the .csv file extension)."
      ],
      "id": "a4dc3127-7e17-4172-9005-65fcabe4a772"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found .csv file: https://files.digital.nhs.uk/32/0B358C/oaps-open-data-mar-2024.csv"
          ]
        }
      ],
      "source": [
        "url = ('https://digital.nhs.uk/data-and-information/publications/statistical/out-of-area-placements-in-mental-health-services/march-2024') \n",
        "\n",
        "response = req.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # find a hyperlink element (has the tag \"a\") where the hyperlink element\n",
        "    # ends with \"csv\".\n",
        "    csv_link = soup.find(\"a\", href=lambda href: href and href.endswith('csv'))\n",
        "\n",
        "    file_url = csv_link[\"href\"] # just return the URL from within the HTML statement\n",
        "\n",
        "    print(\"Found .csv file:\", file_url)\n",
        "\n",
        "else:\n",
        "    print(f'Failed to fetch webpage: {response.status_code}')\n"
      ],
      "id": "locate-csv"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Download the file via the discovered hyperlink.\n",
        "\n",
        "First of all, we need to check whether the file is available for us to\n",
        "download, so we also need to check the reponse here, too."
      ],
      "id": "dd41664a-8590-4439-91ef-e810ae6f4887"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: oaps-open-data-mar-2024.csv"
          ]
        }
      ],
      "source": [
        "file_name = file_url.split(\"/\")[-1]  # extract the file name from the URL i.e. the bit after the last \"/\"\n",
        "file_response = req.get(file_url)\n",
        "\n",
        "if file_response.status_code == 200:\n",
        "\n",
        "    # save the file to the current directory\n",
        "    with open(f'{file_name}', \"wb\") as file:\n",
        "        file.write(file_response.content)\n",
        "    print(f\"Downloaded: {file_name}\")\n",
        "else:\n",
        "    print(f\"Failed to download: {file_url}\")"
      ],
      "id": "download-csv"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Read .csv data directly into a Pandas dataframe\n",
        "\n",
        "Using the `StringIO()` class from the `io` library, create an in-memory\n",
        "stream of the data that can be operated on like a file, without having\n",
        "first saved it down as one. The .csv data is treated like a long string\n",
        "of text where fields are separated by delimiters (commas by default) and\n",
        "rows are separated by newline characters (typically `\\n`). The\n",
        "`.read_csv()` method in Pandas converts this string into a DataFrame."
      ],
      "id": "542023cd-c1f8-496f-9204-3f9be271a995"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "from io import StringIO \n",
        "\n",
        "csv_content = StringIO(file_response.text)\n",
        "\n",
        "df = pd.read_csv(csv_content)\n",
        "\n",
        "df.head(3)"
      ],
      "id": "cell-csv-to-df"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using a regular expression and `urljoin` to locate files on multiple web pages.\n",
        "\n",
        "This is a hefty bit of code with multiple `for` loops and `if`\n",
        "statements. Hopefully, the inline comments explain what is going on at\n",
        "each stage. The great thing about using Python code is that you can\n",
        "easily re-use this code, simply replacing the base URL and the dynamic\n",
        "section. You can also specify the file type, in case you want to use it\n",
        "to download .xlsx files, for example.\n",
        "\n",
        "When defining the “dynamic_section”, you need to make sure that you have\n",
        "identified a regular expression that matches the pattern of all the\n",
        "target URLs that you are interested in.\n",
        "\n",
        "While the regular expression in the example above ended with\n",
        "`202[0-9]$`, it has been set to `2024$` here so that it doesn’t download\n",
        "too many files in one go."
      ],
      "id": "f5a66243-7d17-4209-bb63-f77e53545ffe"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = 'https://digital.nhs.uk/data-and-information/publications/statistical/learning-disabilities-health-check-scheme'\n",
        "\n",
        "target_urls = []                           # empty list that will later get filled with target URLs in a for loop.\n",
        "\n",
        "dynamic_section = r'^england-[a-z]+-2024$' # the regular expression for the URLs we are interested in. note that the $ implies that you don't want anything else to follow.\n",
        "\n",
        "response = req.get(url)                    # get the response from the base URL\n",
        "\n",
        "ext = '.csv'                               # specify the file type\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")     # if there is a successful response, create a BeautifulSoup object.\n",
        "\n",
        "    for link in soup.find_all('a', href = True):                # for each of the instances of the pattern we are looking for...\n",
        "        sublink = link[\"href\"]\n",
        "        if re.match(dynamic_section,sublink.split('/')[-1]):\n",
        "            full_url = urljoin(url, sublink)                   \n",
        "            target_urls.append(full_url)                        # ... add the constructed full URL to a list of target URLs\n",
        "        \n",
        "    for link in target_urls:                                    # check for a successful response (code 200) from each URL...\n",
        "        response = req.get(link)                                \n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\") # ... and create a BeautifulSoup object for each.\n",
        "\n",
        "            for link in soup.find_all(\"a\", href=True):          # for each URL found on each of the pages in target_urls...\n",
        "                file_url = link['href']                         \n",
        "\n",
        "                if file_url.endswith((ext)):                    # ... check for .csv file extensions\n",
        "                    print(\"Found .csv file:\", file_url)\n",
        "\n",
        "                    file_name = file_url.split(\"/\")[-1]         # extract the file name from the URL i.e. everything after the last /\n",
        "                    file_response = req.get(file_url)           # check the response for each file\n",
        "            \n",
        "                    if file_response.status_code == 200:        # if there's a successful response...\n",
        "                        \n",
        "                        with open(file_name, \"wb\") as file:     # ... save the file to the current directory\n",
        "                            file.write(file_response.content)\n",
        "                        print(f\"Downloaded: {file_name}\")\n",
        "                    else:\n",
        "                        print(f\"Failed to download: {file_url}\")\n",
        "\n",
        "else:\n",
        "    print(f'Failed to fetch webpage: {response.status_code}')   # this else statement pairs with the original response code check for the base URL\n",
        "                                                                # (see the first \"if\" in this code block)"
      ],
      "id": "multiple-pages"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## APIs\n",
        "\n",
        "The second method we are going to cover in this tutorial is making\n",
        "requests to data that has been made available via a web API endpoint.\n",
        "API stands for **A**pplication **P**rogramming **I**nterface, and these\n",
        "are used to extend the functionality of an application allowing it to\n",
        "communicate with another.\n",
        "\n",
        "We will specifically be looking at web APIs that return their data in\n",
        "JSON format[1]. For a very simple explanation of what JSON data is, have\n",
        "a look at the [W3\n",
        "Schools](https://www.w3schools.com/whatis/whatis_json.asp) page. In\n",
        "short, it is a format that is typically used for sending data from a\n",
        "server to a web page. The key thing to understand about them is that\n",
        "each record takes the form of a list of key : value pairs, looking just\n",
        "like a Python dictionary, and multiple records are contained in an array\n",
        "denoted by square brackets. You will see this structure reflected in the\n",
        "Python code below.\n",
        "\n",
        "For this demonstration, we are going to pull in flood alerts with at\n",
        "least a severity score of 3 from the Environment Agency’s live flood\n",
        "alert data[2]. If you want to see the data that we will be extracting in\n",
        "its original JSON format, follow this\n",
        "[URL](https://environment.data.gov.uk/flood-monitoring/id/floods?min-severity=3).\n",
        "It is the same URL as will be used in the the Python-based request.\n",
        "\n",
        "> **Important**\n",
        ">\n",
        "> The options available for querying the data is determined by the way\n",
        "> in which the API endpoint has been constructed by the developers. For\n",
        "> example, the data may be aggregated by default, meaning that an\n",
        "> unfiltered request returns the data aggregated to the highest level.\n",
        "> This may well be intended to stop people from requesting masses of\n",
        "> granular data by default, but it can be difficult to get an idea of\n",
        "> what the available categories are in the data breakdown, if there\n",
        "> isn’t a dedicated part of the API that allows for the breakdown\n",
        "> categories to be returned in a list. It is important to read the API\n",
        "> documentation thoroughly to get an undestanding of what your options\n",
        "> are.\n",
        "\n",
        "Now for the code itself. As was mentioned in the section above on REST\n",
        "and GET requests, we need to import the `requests` library to make the\n",
        "GET request. We also need to import the `json` library, which decodes\n",
        "the JSON format, converting the JSON data types into Python data types.\n",
        "A table of these conversions can be found\n",
        "[here](https://docs.python.org/3/library/json.html#encoders-and-decoders).\n",
        "\n",
        "The example below has been placed inside a function. This isn’t\n",
        "essential, but it can prove useful if you want to expand the function to\n",
        "take inputs that make it reusable and apply it to different extracts of\n",
        "the data. You will also see that placing the code inside a function\n",
        "becomes necessary when using Python **generators**, which is another\n",
        "topic [covered later\n",
        "on](../../intermediate-skills-sessions/xx-generators/index.html) in our\n",
        "Intermediate Skills curriculum.\n",
        "\n",
        "[1] XML is another common format for data made available via an API\n",
        "endpoint.\n",
        "\n",
        "[2] [Environment Agency real-time flood-monitoring API\n",
        "documentation](https://environment.data.gov.uk/flood-monitoring/doc/reference)"
      ],
      "id": "d2f1fdf1-9065-481f-8d71-9870edddc799"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import requests as req\n",
        "\n",
        "def get_flood_alerts():\n",
        "    request_url = 'https://environment.data.gov.uk/flood-monitoring/id/floods?min-severity=3' # Step 1\n",
        "    response = req.get(request_url) # Step 2\n",
        "    if response.status_code == 200: # Step 3\n",
        "        results_json = response.json()['items'] # Step 4\n",
        "        alerts = [{   # Step 5\n",
        "            'id' : json['@id'], # Step 6\n",
        "            'description' : json['description'],\n",
        "            'area_name' : json['eaAreaName'],\n",
        "            'flood_area_id' : json['floodAreaID'],\n",
        "            'is_tidal' : json['isTidal'],\n",
        "            'severity' : json['severity'],\n",
        "            'severity_level' : json['severityLevel'],\n",
        "            'time_message_changed' : json['timeMessageChanged'],\n",
        "            'time_raised' : json['timeRaised'],\n",
        "            'time_severity_changed' : json['timeSeverityChanged']\n",
        "        } for json in results_json] # Step 5 continued\n",
        "        return alerts # Step 7\n",
        "    else:\n",
        "        print(f'Failed to fetch data. Response status: {response.status_code}') # Step 3 continued"
      ],
      "id": "flood-data-function"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The steps below are labelled in the code above:**\n",
        "\n",
        "1.  The first step is to define the request URL, which points to the API\n",
        "    endpoint. If you follow the link in a web browser, you will see all\n",
        "    of the relevant records laid out in JSON format. Note that in the\n",
        "    example below there is the section `\"min-severity=3\"` which comes\n",
        "    after a question mark `?`. The question mark indicates that\n",
        "    everything after it relates to an **optional filter**, which is to\n",
        "    say that you can get a cut of the data based on specific criteria.\n",
        "    You can add multiple filters by joining them together with “&”.\n",
        "2.  Then a GET request is made and assigned to the variable “response”.\n",
        "3.  Then, as with the webscraping, we want to handle the response from\n",
        "    the webserver gracefully, in case the data is not available. This\n",
        "    means placing details of what we are requesting in an if/else\n",
        "    conditional statement. If we get a positive response of 200, proceed\n",
        "    with retrieving the data; otherwise, return the error code.\n",
        "4.  The results of the JSON data request are returned as a dictionary\n",
        "    (remember that data in JSON format is very similar to Python\n",
        "    dictionaries). In the flood data API, `'items'` is the key in the\n",
        "    dictionary, and the values are all of the data records[1]. In the\n",
        "    line `results_json = response.json()['items']`, we are accessing all\n",
        "    of the records stored against “items”.\n",
        "5.  The values corresponding to the key “items” are held in an array (in\n",
        "    square brackets). We give the results that we want to return the\n",
        "    variable name `alerts`. We then create a `for` loop to create a\n",
        "    Python list of all of the records, and for each record there is a\n",
        "    key : value pair for each field in the data, replicating the array /\n",
        "    JSON record object structure. Think of each key in the dictionary as\n",
        "    being a column name, each value as the record value and each item in\n",
        "    the `alerts` list as being a row.\n",
        "6.  In the dictionary, we give the key a name of our choosing. It is\n",
        "    what *we* want the column name to be. On the value side of the\n",
        "    dictionary, we are accessing the value corresponding to each key in\n",
        "    the JSON reponse. In the line `for json in results_json`, each\n",
        "    `json` is it’s own dictionary containing a record, for example\n",
        "    `\"@id\" : \"http://environment.data.gov.uk/flood-monitoring/id/floods/112WAFTUBA\",`\n",
        "    `\"description\" : \"Upper Bristol Avon area\", \"eaAreaName\" : \"Wessex\"...`.\n",
        "    We want to access the value corresponding to each key in the JSON\n",
        "    data and store it against *our* key.\n",
        "7.  The list of records is returned in a format that can be used by\n",
        "    other Python packages.\n",
        "\n",
        "Let’s put the results of the query into a Pandas DataFrame and view the\n",
        "results. For this we call the function we defined above and have Pandas\n",
        "convert that list into a DataFrame, with column names that we have given\n",
        "it and rows that correspond to each record.\n",
        "\n",
        "[1] In actual fact, the format that the flood alert data is provided in\n",
        "is a little more complex and is more like a nested dictionary, where you\n",
        "have a top-level set of key-value pairs and one of those keys has values\n",
        "that are themselves dictionaries. In the case of the flood alert data,\n",
        "there are some metadata fields that are defined in the top level\n",
        "alongside “items”, and then the value against the “items” key is then\n",
        "itself an array of key-value pairs."
      ],
      "id": "0a3513dc-e0e0-41b7-8a26-3d7d81d6c51b"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows and columns in the dataset: (230, 10)"
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "alerts = pd.DataFrame(get_flood_alerts())\n",
        "\n",
        "print(f'Number of rows and columns in the dataset: {alerts.shape}')\n",
        "\n",
        "alerts.head()"
      ],
      "id": "cell-flood-data-results"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API request etiquette\n",
        "\n",
        "These are particularly important if you are making requests to free,\n",
        "public APIs, such as those provided by the government and NHS. These\n",
        "considerations are a little more advanced, and are probably only\n",
        "required if you are intending to develop something that supplies data\n",
        "with frequent updates, but it is worth being aware of them: You may\n",
        "start with a project that is small and simple, but it might then develop\n",
        "into something more data-hungry\n",
        "\n",
        "#### Respect rate limits\n",
        "\n",
        "APIs typically define:\n",
        "\n",
        "-   Requests per second/minute\n",
        "-   Daily/monthly quotas\n",
        "-   Burst versus sustained limits\n",
        "\n",
        "They may not be stated explicitly, but you should assume that these\n",
        "limits exist and throttle your calls appropriately. For more information\n",
        "on throttling techniques, have a look at this [Medium\n",
        "post](https://medium.com/@datajournal/how-to-throttle-requests-c1f9dcd8508f).\n",
        "\n",
        "#### Use caching\n",
        "\n",
        "If the API isn’t subject to frequent changes, cache results locally so\n",
        "that you do not need to make repeated requests for the same data. Here’s\n",
        "a [Geeks for Geeks\n",
        "tutorial](https://www.geeksforgeeks.org/python/how-to-implement-file-caching-in-python/)\n",
        "on different types of caching in Python to get you started.\n",
        "\n",
        "If you intend to create your own web app that is pulling in data from\n",
        "other sources, it is likely that you will be using a particular web app\n",
        "framework, and these will provide their own tools for web caching. For\n",
        "example, we have introduced\n",
        "[Steamlit](../../sessions/10-streamlit/index.html) in another tutorial\n",
        "and their overview of caching can be found\n",
        "[here](https://docs.streamlit.io/develop/concepts/architecture/caching).\n",
        "Another framework for creating web apps is\n",
        "[Flask](https://flask.palletsprojects.com/en/stable/) and a simple\n",
        "introduction to web caching using that framework can be found on\n",
        "[PyQuestHub](https://pyquesthub.com/implementing-web-caching-in-python-for-enhanced-performance).\n",
        "\n",
        "#### Avoid polling too aggressively\n",
        "\n",
        "-   Make requests at a reasonable rate: do you really need to check for\n",
        "    updates every second?. You can find out more about polling in this\n",
        "    [Medium blog\n",
        "    post](https://medium.com/@sankalpa115/what-is-polling-b1ff70e87001).\n",
        "-   Make use of webhooks: these allow for automatic communication\n",
        "    between systems, eliminating the need for one system to constantly\n",
        "    check another for updates. Data is pushed automatically whenever an\n",
        "    event occurs. You can find another trusty Geek’s for Geek’s tutorial\n",
        "    on webhooks\n",
        "    [here](https://www.geeksforgeeks.org/blogs/what-is-a-webhook-and-how-to-use-it/).\n",
        "\n",
        "#### Use conditional requests\n",
        "\n",
        "If supported, you can use `ETag` and `If-Modified-Since`, which return a\n",
        "`304 Not Modfied` reponse instead of a full JSON payload. In essence,\n",
        "this response is saying that no changes have been made to the content\n",
        "made available via the endpoint. You could build in some logic that\n",
        "handles the error without crashing the program, and also notifies the\n",
        "end user that there is no new data since the last update.\n",
        "\n",
        "Examples of each, plus a combined approach, can be found in this [Python\n",
        "Lore\n",
        "tutorial](https://www.pythonlore.com/handling-http-conditional-requests-with-requests/).\n",
        "\n",
        "#### Select only what you need\n",
        "\n",
        "Just as you would with a SQL query, try to select only what you need.\n",
        "Some APIs support field selection in the optional filters part of the\n",
        "URL (e.g `?fields=metric,value`). Similarly, try not to request all of\n",
        "the records made available via the API. Would having a rolling 12\n",
        "months’ data be sufficient? Could you store historic data locally? Try\n",
        "to use any filters available in the API to exclude any data that you do\n",
        "not need.\n",
        "\n",
        "#### Identify yourself\n",
        "\n",
        "Some APIs require users to provide a `User-Agent` string. Failure to do\n",
        "so could mean that your request gets blocked by the web server. Web\n",
        "servers use the information to serve appropriate content, implement\n",
        "rate-limiting or block automated requests[1]. You can even add some kind\n",
        "of contact information or a link to the repository for your application\n",
        "so that you can be contacted if there is an issue (only include contact\n",
        "information you are willing to share publicly!). Below is an example of\n",
        "some Python that could be used to generate a `User-Agent` string:\n",
        "\n",
        "[1] See\n",
        "(https://webscraping.ai/faq/requests/how-do-i-set-a-user-agent-string-for-requests)"
      ],
      "id": "22968182-46ab-41f6-8c1b-37ecc2eace2f"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'User-Agent': 'MyApiProject/1.0 (Python 3.12; Windows); https://github.com/NHS-South-Central-and-West/code-club'}"
          ]
        }
      ],
      "source": [
        "import platform     # to get operating system information\n",
        "import sys          # to get Python version information\n",
        "\n",
        "APP_NAME = \"MyApiProject\"\n",
        "APP_VERSION = \"1.0\"\n",
        "GITHUB_PAGE = \"https://github.com/NHS-South-Central-and-West/code-club\"\n",
        "\n",
        "def build_user_agent():\n",
        "    python_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
        "    os_info = platform.system()\n",
        "    return f\"{APP_NAME}/{APP_VERSION} (Python {python_version}; {os_info}); {GITHUB_PAGE}\"\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": build_user_agent()\n",
        "}\n",
        "\n",
        "# let's see what that looks like:\n",
        "\n",
        "print(headers)"
      ],
      "id": "user-agent-builder"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, when you make the request, you pass the `User-Agent` string to the\n",
        "“headers” keyword argument:"
      ],
      "id": "dcf9b47b-b454-45e9-a57f-8cfc528be632"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = requests.get(\"https://environment.data.gov.uk/flood-monitoring/id/floods\", headers=headers)"
      ],
      "id": "user-agent-request"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Read the smallprint**\n",
        ">\n",
        "> It is advisable that you read any Terms of Service applied to the use\n",
        "> of an API. Providers of free APIs may forbid commercial use (and you\n",
        "> need to be sure what is meant by this), redistribution of the data and\n",
        "> automated, high-frequency usage.\n",
        "\n",
        "## The `fingertips_py` package\n",
        "\n",
        "This is a package that was originally developed by Public Health England\n",
        "to make it easy to import data via the Fingertips API endpoint. It’s an\n",
        "example of what the possibilities are, hopefully serving as inspiration\n",
        "for your own Python projects. It’s also pretty useful, if you want to\n",
        "make use of Fingertips data yourself!\n",
        "\n",
        "We have created a walkthrough of using the `fingertips_py` package\n",
        "[here](../../intermediate-skills-sessions/18-webscraping-apis/fingertips_py.html).\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1.  Write a regular expression that could be used to identify all of the\n",
        "    Excel files on the following web page:[Mental Health Services Data\n",
        "    Set Submission\n",
        "    Reports](https://digital.nhs.uk/data-and-information/data-collections-and-data-sets/data-sets/mental-health-services-data-set/mental-health-services-data-set-mhsds-submission-update).\n",
        "\n",
        "**Note:** The displayed document titles may not reflect the actual file\n",
        "URLs.\n",
        "\n",
        "> **Solution**\n",
        ">\n",
        "> ``` python\n",
        "> pattern = r'.*mswm-submission-tracker.*.xlsm'\n",
        ">\n",
        "> # . matches any character; * means any number of those.\n",
        "> # That pattern can occur before or after \"mswm-submission-tracker\".\n",
        "> ```\n",
        "\n",
        "1.  Which REST API response code means a positive result, i.e. that data\n",
        "    is available?\n",
        "\n",
        "> **Solution**\n",
        ">\n",
        "> `200`\n",
        "\n",
        "1.  Which REST API response code means that no new data is available?\n",
        "\n",
        "> **Solution**\n",
        ">\n",
        "> `304`\n",
        "\n",
        "1.  Write some Python code to return the planned outages table on the\n",
        "    [SUS Service Announcements and Outages\n",
        "    page](https://digital.nhs.uk/services/secondary-uses-service-sus/secondary-uses-service-sus-what-s-new/service-announcements-and-outages)\n",
        "    to a Pandas DataFrame and then print the result. Make sure that you\n",
        "    handle any error raised due to the web page being unavailable.\n",
        "\n",
        "> **Solution**\n",
        ">\n",
        "> ``` python\n",
        "> import pandas as pd\n",
        "> from bs4 import BeautifulSoup\n",
        ">\n",
        "> url = 'https://digital.nhs.uk/services/secondary-uses-service-sus/secondary-uses-service-sus-what-s-new/service-announcements-and-outages'\n",
        ">\n",
        "> response = req.get(url)\n",
        ">\n",
        "> if response.status_code == 200:\n",
        ">     soup = BeautifulSoup(response.content, \"html.parser\")\n",
        ">     table = soup.find('table')\n",
        ">     table_df = pd.read_html(str(table))\n",
        ">     print(table_df)\n",
        "> else:\n",
        ">     print(f'Outages page currently unavailable: {response.status_code}')\n",
        "> ```\n",
        ">\n",
        ">     [                         Date         Time                  Type\n",
        ">     0   Wednesday 4 February 2026  6pm to 10pm  SUS+/DLP maintenance\n",
        ">     1  Wednesday 11 February 2026  6pm to 10pm  SUS+/DLP maintenance\n",
        ">     2  Wednesday 25 February 2026  6pm to 10pm  SUS+/DLP maintenance]\n",
        "\n",
        "1.  Which BeautifulSoup method can you use to return the HTML in a\n",
        "    nicely laid out format?\n",
        "\n",
        "> **Solution**\n",
        ">\n",
        "> `print(soup.prettify())`\n",
        "\n",
        "1.  Using the example under “Using a regular expression and `urljoin` to\n",
        "    locate files on multiple web pages.” as a template, write some\n",
        "    Python code that will download all of the NHS Talking Therapies Data\n",
        "    Quality Reports for 2025 accessible via the [official statistics\n",
        "    page](https://digital.nhs.uk/data-and-information/publications/statistical/nhs-talking-therapies-monthly-statistics-including-employment-advisors).\n",
        "\n",
        "> **Solution**\n",
        ">\n",
        "> ``` python\n",
        "> import pandas as pd\n",
        "> from bs4 import BeautifulSoup\n",
        "> import re\n",
        ">\n",
        "> url = 'https://digital.nhs.uk/data-and-information/publications/statistical/nhs-talking-therapies-monthly-statistics-including-employment-advisors'\n",
        ">\n",
        "> target_urls = []                           \n",
        ">\n",
        "> dynamic_section = r'^performance-[a-z]+-2025$' \n",
        ">\n",
        "> response = req.get(url)                    \n",
        ">\n",
        "> ext = '.csv'                               \n",
        ">\n",
        "> if response.status_code == 200:\n",
        ">     soup = BeautifulSoup(response.content, \"html.parser\")     \n",
        ">\n",
        ">     for link in soup.find_all('a', href = True):                \n",
        ">         sublink = link[\"href\"]\n",
        ">         if re.match(dynamic_section,sublink.split('/')[-1]):\n",
        ">             full_url = urljoin(url, sublink)                   \n",
        ">             target_urls.append(full_url)                        \n",
        ">         \n",
        ">     for link in target_urls:                                    \n",
        ">         response = req.get(link)                                \n",
        ">         if response.status_code == 200:\n",
        ">             soup = BeautifulSoup(response.content, \"html.parser\") \n",
        ">\n",
        ">             for link in soup.find_all(\"a\", href=True):          \n",
        ">                 file_url = link['href']                         \n",
        ">\n",
        ">                 if file_url.endswith((ext)):                   \n",
        ">                     print(\"Found .csv file:\", file_url)\n",
        ">\n",
        ">                     file_name = file_url.split(\"/\")[-1]        \n",
        ">                     file_response = req.get(file_url)          \n",
        ">             \n",
        ">                     if file_response.status_code == 200:       \n",
        ">                         \n",
        ">                         with open(file_name, \"wb\") as file:     \n",
        ">                             file.write(file_response.content)\n",
        ">                         print(f\"Downloaded: {file_name}\")\n",
        ">                     else:\n",
        ">                         print(f\"Failed to download: {file_url}\")\n",
        ">\n",
        "> else:\n",
        ">     print(f'Failed to fetch webpage: {response.status_code}')   \n",
        "> ```\n",
        "\n",
        "1.  Which character designates the beginning of the filter section of a\n",
        "    URL when filtering a JSON API request? Which character is used to\n",
        "    join multiple filters together?\n",
        "\n",
        "> **Solution**\n",
        ">\n",
        "> Designates the beginning of the filter section: `?`\n",
        ">\n",
        "> Joins multiple filters together: `&`\n",
        "\n",
        "1.  Write a Python function that will return a dataframe of the daily\n",
        "    number of patients admitted to hospital with COVID-19 in 2025 via\n",
        "    the [UKHSA data dashboard\n",
        "    API](https://ukhsa-dashboard.data.gov.uk/access-our-data). You will\n",
        "    need to read the API documentation, making use of the examples. The\n",
        "    [UKHSA data dashboard page](https://ukhsa-dashboard.data.gov.uk/).\n",
        "\n",
        "-   Filter the data to just **2025**.\n",
        "-   The `geography_type` should be **Nation**.\n",
        "-   The `geography` should be **England**.\n",
        "-   Return the following columns:\n",
        "    -   `theme`\n",
        "    -   `sub_theme`\n",
        "    -   `topic`\n",
        "    -   `geography`\n",
        "    -   `metric`\n",
        "    -   `year`\n",
        "    -   `date`\n",
        "    -   `metric_value`\n",
        "\n",
        "**HINT:** Instead of “items” (as in the flood alerts example), the\n",
        "records are contained in a list called “results”.\n",
        "\n",
        "> **Solution**\n",
        ">\n",
        "> ``` python\n",
        "> import json\n",
        "> import pandas as pd\n",
        "> import requests as req\n",
        ">\n",
        "> def get_covid_admissions():\n",
        ">     request_url = 'https://api.ukhsa-dashboard.data.gov.uk/themes/infectious_disease/sub_themes/respiratory/topics/COVID-19/geography_types/Nation/geographies/England/metrics/COVID-19_healthcare_admissionByDay?year=2025'\n",
        ">     response = req.get(request_url)\n",
        ">     if response.status_code == 200:\n",
        ">         results_json = response.json()[\"results\"]\n",
        ">         records = [{   \n",
        ">             'theme': json['theme'],\n",
        ">             'sub_theme': json['sub_theme'],\n",
        ">             'topic': json['topic'],\n",
        ">             'geography': json['geography'],\n",
        ">             'metric': json['metric'],\n",
        ">             'year': json['year'],\n",
        ">             'date': json['date'],\n",
        ">             'metric_value': json['metric_value'],\n",
        ">         } for json in results_json\n",
        ">         ]\n",
        ">         return records\n",
        ">     else:\n",
        ">         print(f'Failed to fetch data. Response status: {response.status_code}')\n",
        ">\n",
        "> records = pd.DataFrame(get_covid_admissions())\n",
        ">\n",
        "> print(f'Number of rows and columns in the dataset: {records.shape}')\n",
        ">\n",
        "> records.head()\n",
        "> ```\n",
        ">\n",
        ">     Number of rows and columns in the dataset: (5, 8)\n",
        ">\n",
        "> <div>\n",
        "> <style scoped>\n",
        ">     .dataframe tbody tr th:only-of-type {\n",
        ">         vertical-align: middle;\n",
        ">     }\n",
        ">\n",
        ">     .dataframe tbody tr th {\n",
        ">         vertical-align: top;\n",
        ">     }\n",
        ">\n",
        ">     .dataframe thead th {\n",
        ">         text-align: right;\n",
        ">     }\n",
        "> </style>\n",
        ">\n",
        "> |  | theme | sub_theme | topic | geography | metric | year | date | metric_value |\n",
        "> |----|----|----|----|----|----|----|----|----|\n",
        "> | 0 | infectious_disease | respiratory | COVID-19 | England | COVID-19_healthcare_admissionByDay | 2025 | 2025-01-01 | 144.0 |\n",
        "> | 1 | infectious_disease | respiratory | COVID-19 | England | COVID-19_healthcare_admissionByDay | 2025 | 2025-01-02 | 132.0 |\n",
        "> | 2 | infectious_disease | respiratory | COVID-19 | England | COVID-19_healthcare_admissionByDay | 2025 | 2025-01-03 | 119.0 |\n",
        "> | 3 | infectious_disease | respiratory | COVID-19 | England | COVID-19_healthcare_admissionByDay | 2025 | 2025-01-04 | 120.0 |\n",
        "> | 4 | infectious_disease | respiratory | COVID-19 | England | COVID-19_healthcare_admissionByDay | 2025 | 2025-01-05 | 121.0 |\n",
        ">\n",
        "> </div>"
      ],
      "id": "596bb5ad-71e0-42c4-9466-487c44a1a4a8"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "C:.chick- NHS_dev_club_webpage.venv"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  }
}