[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Code Club!",
    "section": "",
    "text": "Code Club aims to support everyone at SCW in developing technical and analytical skills through interpretive dance code. We believe these skills are indispensable to the NHS today and in the future, enabling the delivery of high-quality insights through data science and advanced analytics, and the automation of day-to-day tasks with programming. We want to foster an environment that welcomes everybody, sparks ideas, and nurtures collaboration.\nThe Code Club syllabus has been designed to help people with little to no coding experience develop their skills in Python and extend their analytical skills through code. Sessions will be an hour long and held once per fortnight at 2:00 PM on Thursdays. To get an idea of what we will be covering and see if it is right for you, go to the Schedule page. We would love for you to join us!"
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html",
    "href": "sessions/05-eda-seaborn/index.html",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "",
    "text": "We are using Australian weather data, taken from Kaggle. To download the data, click here.\n# import packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')\nObjectives:\n# subset of observations from five biggest cities\nbig_cities = (\n    df.loc[df['Location'].isin(['Adelaide', 'Brisbane', 'Melbourne', 'Perth', 'Sydney'])]\n    .copy()\n)"
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html#visualising-a-single-variable",
    "href": "sessions/05-eda-seaborn/index.html#visualising-a-single-variable",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "Visualising a Single Variable",
    "text": "Visualising a Single Variable\n\nWhat do we want to know when we are visualising a sample taken from a single variable?\nWe want to understand the value that the value tends to take, and how much it tends to deviate from its typical value.\n\nThe central tendency and deviation are ways to describe a sample.\nVisualising the distribution of a variable can tell us these things (approximately), and can tell us about the shape of the data too.\n\n\n\nDescribing a Sample\n\nWhat is the best way to describe a variable?\n\nWhat is the average value? Or the value it is most likely to take? What is the best value to describe it in one go?\n\nThe ‚Äúcentral tendency‚Äù is the average or most common value that a variable takes. Mean, median, and mode are all descriptions of the central tendency.\n\nMean - Sum of values in a sample divided by the total number of observations\nMedian - The midpoint value if the sample is ordered from highest to lowest\nMode - The most common value in the sample\n\nThe mean is the most common approach, but the mean, median, and mode choice are context-dependent. Other approaches exist, too, such as the geometric mean.\n\nThe geometric mean multiplies all values in the sample and takes the \\(n\\)th root of that multiplied value.\nIt can be useful when dealing with skewed data or data with very large ranges, and when dealing with rates, proportions etc. However it can‚Äôt handle zeros or negative values.\n\nThe mode value is generally most useful when dealing with categorical variables.\n\n\n# mode rainfall by location\nbig_cities.groupby('Location')['Rainfall'].agg(pd.Series.mode)\n\nLocation\nAdelaide     0.0\nBrisbane     0.0\nMelbourne    0.0\nPerth        0.0\nSydney       0.0\nName: Rainfall, dtype: float64\n\n\n\n# mode location\nbig_cities['Location'].agg(pd.Series.mode)\n\n0    Sydney\nName: Location, dtype: object\n\n\n\n# mode location using value counts\nbig_cities['Location'].value_counts().iloc[0:1]\n\nLocation\nSydney    3344\nName: count, dtype: int64\n\n\n\n# mean rainfall by location\nnp.round(big_cities.groupby('Location')['Rainfall'].mean(), decimals=2)\n\nLocation\nAdelaide     1.57\nBrisbane     3.14\nMelbourne    1.87\nPerth        1.91\nSydney       3.32\nName: Rainfall, dtype: float64\n\n\n\n# median rainfall by location\nbig_cities.groupby('Location')['Rainfall'].median()\n\nLocation\nAdelaide     0.0\nBrisbane     0.0\nMelbourne    0.0\nPerth        0.0\nSydney       0.0\nName: Rainfall, dtype: float64\n\n\n\n# geometric mean max temperature by location\nbig_cities.groupby('Location')['MaxTemp'].apply(lambda x: np.exp(np.log(x).mean()))\n\nLocation\nAdelaide     21.888697\nBrisbane     26.152034\nMelbourne    19.972352\nPerth        24.320203\nSydney       22.570993\nName: MaxTemp, dtype: float64\n\n\n\nWhy do the mean and median differ so much? Why would the median rainfall be zero for all five cities?\nDoes this matter? How would it change our understanding of the rainfall variable?\nVisualising the distribution can tell us more!\n\n\n\nComparing the Mean & Median\n\nWe have simulated three different distributions that have slightly different shapes. and see how their mean and median values differ.\n\n\n# generate distributions\nnp.random.seed(123)\nnormal_dist = np.random.normal(10, 1, 1000)\nright_skewed_dist = np.concatenate([np.random.normal(8, 2, 600), np.random.normal(14, 4, 400)])\nleft_skewed_dist = np.concatenate([np.random.normal(14, 2, 600), np.random.normal(8, 4, 400)])\n\n\n# import packages\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# control some deprecation warnings in seaborn\nwarnings.filterwarnings(\n    \"ignore\",\n    category=FutureWarning,\n    module=\"seaborn\"\n)\n\n# set figure size\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# function for calculating summary statistics and plotting distributions\ndef plot_averages(ax, data, title):\n    mean = np.mean(data)\n    median = np.median(data)\n    \n    sns.histplot(data, color=\"#d9dcd6\", bins=30, ax=ax)\n    ax.axvline(mean, color=\"#0081a7\", linewidth=3, linestyle=\"--\", label=f\"Mean: {mean:.2f}\")\n    ax.axvline(median, color=\"#ef233c\", linewidth=3, linestyle=\"--\", label=f\"Median: {median:.2f}\")\n    ax.set_title(title)\n    ax.set_ylabel('')\n    ax.legend()\n\n\n# plot distributions\nfig, axes = plt.subplots(1, 3, sharey=True)\n\nplot_averages(axes[0], normal_dist, \"Normal Distribution\\n(Mean ‚âà Median)\")\nplot_averages(axes[1], right_skewed_dist, \"Right-Skewed Distribution\\n(Mean &gt; Median)\")\nplot_averages(axes[2], left_skewed_dist, \"Left-Skewed Distribution\\n(Mean &lt; Median)\")\n\nplt.suptitle(\"Comparison of Mean & Median Across Distributions\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe mean and median of the normal distribution are identical, while the two skewed distributions have slightly different means and medians.\n\nThe mean is larger than the media when the distribution is right-skewed, and the median is larger than the mean when it is left-skewed.\nWhen the distribution is skewed, the median value will be a better description of the central tendency, because the mean value is more sensitive to extreme values (and skewed distributions have longer tails of extreme values).\n\nThese differences point to another important factor to consider when summarising data - the spread or deviation of the sample.\nHow do we measure how a sample is spread around the central tendency?\n\nStandard deviation and variance quantify spread.\nVariance, the average squared difference between observations and the mean value, measures how spread out a sample is.\nStandard deviation is the square root of the variance. It‚Äôs easier to interpret because it‚Äôs in the same units as the sample.\n\n\n\n# generate distributions\nnp.random.seed(123)\nmean = 10\nstd_devs = [1, 2, 3]\ndistributions = [np.random.normal(mean, std_dev, 1000) for std_dev in std_devs]\n\n\n# function for calculating summary statistics and plotting distributions\ndef plot_spread(ax, data, std_dev, title):\n    mean = np.mean(data)\n    std_dev = np.std(data)\n\n    sns.histplot(data, color=\"#d9dcd6\", bins=30, ax=ax)\n    ax.axvline(mean, color=\"#0081a7\", linewidth=3, linestyle=\"--\", label=f\"Mean: {mean:.2f}\")\n    ax.axvline(mean + std_dev, color=\"#ee9b00\", linewidth=3, linestyle=\"--\", label=f\"Mean + 1 SD: {mean + std_dev:.2f}\")\n    ax.axvline(mean - std_dev, color=\"#ee9b00\", linewidth=3, linestyle=\"--\", label=f\"Mean - 1 SD: {mean - std_dev:.2f}\")\n    ax.set_title(f\"{title}\")\n    ax.legend()\n\n\n# plot distributions\nfig, axes = plt.subplots(1, 3, sharey=True, sharex=True)\n\nfor i, std_dev in enumerate(std_devs):\n    plot_spread(axes[i], distributions[i], std_dev, f\"Standard Deviation = {std_dev}\")\n\nplt.suptitle(\"Effect of Standard Deviation on Distribution Shape\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs standard deviation increases, the spread of values around the mean increases.\nWe can compute various summary statistics that describe a sample (mean, median, standard deviation, kurtosis etc. etc.), or we can just visualise it!\nVisualising distributions is a good starting point for understanding a sample. It can quickly and easily tell you a lot about the data.\n\n\n# plot distribution of rainfall\nrainfall_mean = np.mean(big_cities['Rainfall'])\nrainfall_median = np.median(big_cities['Rainfall'].dropna())\n\nsns.histplot(data=big_cities, x='Rainfall', binwidth=10, color=\"#d9dcd6\")\nplt.axvline(rainfall_mean, color=\"#0081a7\", linestyle=\"--\", linewidth=2, label=f\"Mean: {rainfall_mean:.2f}\")\nplt.axvline(rainfall_median, color=\"#ef233c\", linestyle=\"--\", linewidth=2, label=f\"Median: {rainfall_median:.2f}\")\n\nplt.title(\"Distribution of Rainfall in Australia's Big Cities\")\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# plot distribution of sunshine\nsunshine_mean = np.mean(big_cities['Sunshine'])\nsunshine_median = np.median(big_cities['Sunshine'].dropna())\n\nsns.histplot(data=big_cities, x='Sunshine', binwidth=1, color=\"#d9dcd6\")\nplt.axvline(sunshine_mean, color=\"#0081a7\", linestyle=\"--\", linewidth=2, label=f\"Mean: {sunshine_mean:.2f}\")\nplt.axvline(sunshine_median, color=\"#ef233c\", linestyle=\"--\", linewidth=2, label=f\"Median: {sunshine_median:.2f}\")\n\nplt.title(\"Distribution of Sunshine in Australia's Big Cities\")\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThese two plots require a little more code, but we can get most of what we want with a lot less.\n\n\nsns.histplot(data=big_cities, x='MaxTemp')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.countplot(big_cities, x='Location', color=\"#d9dcd6\", edgecolor='black')\nplt.ylim(3000, 3500)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.countplot(big_cities, x='RainTomorrow', color=\"#d9dcd6\", edgecolor='black')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nbig_cities['RainTomorrow'].value_counts()\n\nRainTomorrow\nNo     11673\nYes     3543\nName: count, dtype: int64"
  },
  {
    "objectID": "sessions/05-eda-seaborn/index.html#visualising-multiple-variables",
    "href": "sessions/05-eda-seaborn/index.html#visualising-multiple-variables",
    "title": "Visually Exploring Data Using Seaborn",
    "section": "Visualising Multiple Variables",
    "text": "Visualising Multiple Variables\n\nWe will often want to know how values of a given variable change based on the values of another.\nThis may not indicate a relationship, but it helps us better understand our data.\n\n\nsns.barplot(big_cities, x='Location', y='Sunshine', color=\"#d9dcd6\", edgecolor='black')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.boxplot(big_cities, x='Location', y='MaxTemp', color=\"#d9dcd6\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.boxplot(data=big_cities, x='RainTomorrow', y='Humidity3pm', color=\"#d9dcd6\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.kdeplot(data=big_cities, x='Humidity3pm', hue='RainTomorrow')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n(\n    big_cities\n    # convert date to datetime\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    # create year-month column\n    .assign(Year_Month=lambda x: x['Date'].dt.to_period('M'))\n    # group by year-month and calculate sum of rainfall\n    .groupby('Year_Month')['Rainfall'].sum()\n    # convert year-month index back to column in dataframe\n    .reset_index()\n    # create year-month timestamp for plotting\n    .assign(Year_Month=lambda x: x['Year_Month'].dt.to_timestamp()) \n    # pass df object to seaborn lineplot\n    .pipe(lambda df: sns.lineplot(data=df, x='Year_Month', y='Rainfall', linewidth=2))\n)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n(\n    big_cities\n    # convert date to datetime object\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    # set date column as index\n    .set_index('Date')\n    # resample by month-end for monthly aggregations\n    .resample('ME')\n    # calculate mean sunshine per month\n    .agg({'Sunshine': 'mean'})\n    # convert month index back to column in dataframe\n    .reset_index()\n    # pass df object to seaborn lineplot\n    .pipe(lambda df: sns.lineplot(data=df, x='Date', y='Sunshine', color=\"#1f77b4\", linewidth=2))\n)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(1, 2)\n\n(\n    big_cities\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    .assign(Month=lambda x: x['Date'].dt.month)\n    .groupby('Month')['Rainfall'].mean()\n    .reset_index()\n    .pipe(lambda df: sns.lineplot(data=df, x='Month', y='Rainfall', color=\"#1f77b4\", linewidth=2, ax=axes[0]))\n)\n\n(\n    big_cities\n    .assign(Date=pd.to_datetime(big_cities['Date']))\n    .assign(Month=lambda x: x['Date'].dt.month) \n    .groupby('Month')['Sunshine'].mean() \n    .reset_index()\n    .pipe(lambda df: sns.lineplot(data=df, x='Month', y='Sunshine', color=\"#ff7f0e\", linewidth=2, ax=axes[1]))\n)\n\nxticks = range(1, 13)\nxticklabels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nfor ax in axes:\n    ax.set_xticks(xticks)  # Set ticks\n    ax.set_xticklabels(xticklabels, rotation=45)\n    ax.set_xlabel('')\n    ax.set_ylabel('')\naxes[0].set_title('Average Rainfall by Month', fontsize=16)\naxes[1].set_title('Average Sunshine by Month', fontsize=16)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html",
    "href": "sessions/02-jupyter_notebooks/index.html",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "This is the second session following Code Club‚Äôs relaunch. The focus is introducing jupyter notebooks and explaining to users how to get started with a new project and briefly introducing some key concepts.\nWe are also planning some time for Q&A following the first session.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#session-slides",
    "href": "sessions/02-jupyter_notebooks/index.html#session-slides",
    "title": "Jupyter Notebooks",
    "section": "Session Slides",
    "text": "Session Slides\nUse the left ‚¨ÖÔ∏è and right ‚û°Ô∏è arrow keys to navigate through the slides below. To view in a separate tab/window, follow this link.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#the-tools-you-will-need",
    "href": "sessions/02-jupyter_notebooks/index.html#the-tools-you-will-need",
    "title": "Jupyter Notebooks",
    "section": "The Tools You Will Need",
    "text": "The Tools You Will Need\nThough Jupyter notebooks can be used with a variety of coding languages and in different settings the key tools used in this session are:\n\nLanguage: Python\nDependency Management & Virtual Environments: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code/Jupyter Notebooks (or your preferred IDE)\n\nYou can install all the tools you‚Äôll need by running the following one-liner run in PowerShell:\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop\nYou can find more information on these topics in the Python Onboarding session",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#project-setup",
    "href": "sessions/02-jupyter_notebooks/index.html#project-setup",
    "title": "Jupyter Notebooks",
    "section": "Project Setup",
    "text": "Project Setup\nOur project set-up will follow the same steps as used in the onboarding session, by using uv to set up a new project folder.\nTo get started we will use PowerShell powershell to open a command prompt, it should open in your C drive (e.g., C:\\Users\\user.name). If it does not, run cd ~, and it should return to your home directory. We recommend the use of a single folder to hold your python projects while learning, because we will be using git version control we will call this ‚ÄúGit‚Äù. we can use the command mkdir code_club to make this folder and then use cd code_club to relocate to this folder1.\nWe will create a new uv project in this directory using the command uv init. The new project will contain everything we need, including a Python installation, a virtual environment, and the necessary project files for tracking and managing any packages installed in the virtual environment. To set up a new project called test-project, use the following command:\nuv init test_project\nHaving created this new directory, navigate to it using cd test_project.\nFor this session you will need to add 3 Python packages, ipykernel2, pandas and seaborn We can use the following command:\nuv add ipykernel pandas seaborn\nWe are going to create a blank notebook in this file by running the command new-item first_notebook.ipynb if you now run ls you will note this file has been created\nYour Python project is now set up, and you are ready to start writing some code. You can open VS Code from your PowerShell window by running code ..\n\nOpening your project in VS Code\nYou could also do this from within VS Code as most IDEs include a terminal interface which will be demonstrated in session.\nFor now launch VS Code and click File &gt; Open Folder.... You‚Äôll want to make sure you select the root level of your project. Once you‚Äôve opened the folder, the file navigation pane in VS Code should display the files that uv has created, as well as the notebook you created: first_notebook.ipynb. Click on this to open it.\nOnce VS Code realises you‚Äôve opened a folder with Python code and a virtual environment, it should do the following:\n\nSuggest you install the Python extension (and, once you‚Äôve created a Jupyter notebook, the Jupyter one) offered by Microsoft - go ahead and do this. If this doesn‚Äôt happen, you can install extensions manually from the Extensions pane on the left-hand side.\nSelect the uv-created .venv as the python Environment we‚Äôre going to use to actually run our code. If this doesn‚Äôt happen, press ctrl-shift-P, type ‚Äúpython environment‚Äù to find the Python - Create Environment... option, hit enter, choose ‚ÄúVenv‚Äù and proceed to ‚ÄúUse Existing‚Äù.\n\nIf VS Code has found the virtual environment, it may pick up the correct kernel. If not you may need to select this manually this can be done by clicking in the top right where you can see Select Kernel (see below)\n\n\n\nClick ‚ÄòSelect Kernel‚Äô\n\n\nWe can then select the appropriate kernel from python environments and looking for\n\n\n\nclick Python Environments\n\n\n\n\n\nclick venv - recommended\n\n\nOnce the kernel is enabled you are ready to start adding cells to your notebook. these can either be code cells which is where you include your program elements or markdown which enable the addition of headings, analysis and commentary.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/index.html#footnotes",
    "href": "sessions/02-jupyter_notebooks/index.html#footnotes",
    "title": "Jupyter Notebooks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe recommend using the C drive for all Python projects, especially if using version control. Storing projects like these on One Drive will create many unnecessary issues. It can be helpful to use a sub-directory to store projects but is not necessary and is not a requirement for code club‚Ü©Ô∏é\nStrictly speaking, we should install ipykernel as a development dependency (a dependency that is needed for any development but not when the project is put into production). In this case, we would add it by running uv add --dev ipykernel. However, in this case, it is simpler to just add it as a regular dependency, and it doesn‚Äôt harm.‚Ü©Ô∏é",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "2. Jupyter Notebooks"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html",
    "href": "sessions/01-onboarding/index.html",
    "title": "Python Onboarding",
    "section": "",
    "text": "This is the first session of Code Club‚Äôs relaunch. It focuses on giving users all the tools they need to get started using Python and demonstrates the setup for a typical Python project.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#session-slides",
    "href": "sessions/01-onboarding/index.html#session-slides",
    "title": "Python Onboarding",
    "section": "Session Slides",
    "text": "Session Slides\nUse the left ‚¨ÖÔ∏è and right ‚û°Ô∏è arrow keys to navigate through the slides below. To view in a separate tab/window, follow this link.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#the-tools-you-will-need",
    "href": "sessions/01-onboarding/index.html#the-tools-you-will-need",
    "title": "Python Onboarding",
    "section": "The Tools You Will Need",
    "text": "The Tools You Will Need\nWhile this course focuses on Python, we will use several other tools throughout.\n\nLanguage: Python\nDependency Management & Virtual Environments: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code/Jupyter Notebooks (or your preferred IDE)\n\nYou can install all the tools you‚Äôll need by running the following one-liner run in PowerShell:\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop\n\nPython\nPython is an all-purpose programming language that is one of, if not the most popular, in the world1 and is widely used in almost every industry. Its popularity is owed to its flexibility as a language that can be used to achieve nearly any job. It is often referred to as the second-best tool for every job. Specialist languages might be better for specific tasks (for example, R for statistics), but Python is good at everything.\nIt is a strong choice for data science and analytics, being one of the best languages for data wrangling, data visualisation, statistics, and machine learning. It is also well-suited to web development, scientific computing, and automation.\n\n\nDependency Management\nOne of Python‚Äôs greatest weaknesses is dependency management. Despite its many strengths, there is no escaping the dependency hell that every Python user faces.\nDependency management refers to the process of tracking and managing all of the packages (dependencies) a project needs to run. It is a consideration in any programming language. It ensures:\n\nThe right packages are installed.\nThe correct versions are used.\nConflicts between packages are avoided.\n\nThere are many reasons that Python handles dependency management so poorly, but there are some tools that make this a little easier on users. We are using uv for dependency management. It is relatively new, but it is quickly becoming the consensus tool for dependency management in Python because it makes the process about as painless as it can be without moving to a different language entirely.\n\nVirtual Environments\nVirtual environments are a component of dependency management. Dependency management becomes much messier when you have many Python projects, each using their own packages, some overlapping and some requiring specific versions, either for compatibility or functionality reasons. Reducing some of this friction by isolating each project in its own virtual environment, like each project is walled off from all other projects, makes dependency management a little easier. Virtual environments allow you to manage dependencies for a specific project without the state of those dependencies affecting other projects or your wider system.\nVirtual environments help by:\n\nKeeping dependencies separate for each project.\nAvoiding version conflicts between projects.\nMaking dependency management more predictable and reproducible.\n\nWe will use uv to manage all dependencies, virtual environments, and even versions of Python.\n\n\n\nVersion Control\nVersion control is the practice of tracking and managing changes to code or files over time, allowing you to:\n\nRevert to earlier versions if needed.\nCollaborate with others on the same project easily.\nMaintain a history of changes.\n\nWe are using Git, a version control system, to host our work and GitHub Desktop to manage version control locally.\nVersion control and Git are topics entirely in their own right, and covering them in detail is out of the scope of this session. We hope to cover version control in a future session, but right now, you just need to be able to access materials for these sessions. You can find the materials in the Code Club repository.\nIf you have downloaded GitHub Desktop, the easiest way to access these materials and keep up-to-date is by cloning the Code Club repository (go to File, then Clone Repository, select URL, and paste the Code Club repository link in the URL field). You can then ensure that the materials you are using are the most current by clicking the Fetch Origin button in GitHub Desktop, which grabs the changes we‚Äôve made from the central repository on GitHub.\n\n\nIDE\nIDEs (Integrated Development Environments) are software that simplifies programming and development by combining many of the most common tasks and helpful features for programming into a single tool. These typically include a code editor, debugging functionality, build tools, and features like syntax highlighting and code completion. When you start your code journey, you might not need all these tools, and fully-featured IDEs can be overwhelming. But as you become more comfortable with programming, all these different features will become very valuable.\nSome common IDEs that are used for Python include:\n\nVS Code\nPyCharm\nVim\nJupyter Notebooks/JupyterLab\nPositron\n\nWe will use VS Code or Jupyter Notebooks (which is not exactly an IDE but is similar).",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#project-setup",
    "href": "sessions/01-onboarding/index.html#project-setup",
    "title": "Python Onboarding",
    "section": "Project Setup",
    "text": "Project Setup\nEvery new Python project should start with using uv to set up a virtual environment for the project to keep everything organised and reduce the risk of finding yourself in dependency hell.\nThe entire project setup process can be handled in the command line. We will use PowerShell for consistency.\nWhen you open a PowerShell window, it should open in your C drive (e.g.,¬† C:\\Users\\user.name). If it does not, run cd ~, and it should return to your home directory.\nWe will create a new uv project in the home directory2 using the command uv init. The new project will contain everything we need, including a Python installation, a virtual environment, and the necessary project files for tracking and managing any packages installed in the virtual environment. To set up a new project called test-project, use the following command:\nuv init test-project\nHaving created this new directory, navigate to it using cd test-project. You can check the files in a directory using the command ls. If you run this command, you will see three files in the project directory (hello.py, pyproject.toml, and README.md). The project doesn‚Äôt yet have a Python installation or a virtual environment, but this will be added when we add external Python packages.\nYou can install Python packages using the command uv add. We can add some common Python packages that we will use in most projects (pandas, numpy, seaborn, and ipykernel3) using the following command:\nuv add pandas numpy seaborn ipykernel\nThe output from this command will reference the Python installation used and the creation of a virtual environment directory .venv. Now, if you run ls, you will see two new items in the directory, uv.lock and .venv.\nYour Python project is now set up, and you are ready to start writing some code. You can open VS Code from your PowerShell window by running code ..\nFor more information about creating and managing projects using uv, check out the uv documentation.\n\nOpening your project in VS Code\nTo open your newly-created uv project in VS Code, launch the application and click File &gt; Open Folder.... You‚Äôll want to make sure you select the root level of your project. Once you‚Äôve opened the folder, the file navigation pane in VS Code should display the files that uv has created, including a main.py example file. Click on this to open it.\nOnce VS Code realises you‚Äôve opened a folder with Python code and a virtual environment, it should do the following:\n\nSuggest you install the Python extension (and, once you‚Äôve created a Jupyter notebook, the Jupyter one) offered by Microsoft - go ahead and do this. If this doesn‚Äôt happen, you can install extensions manually from the Extensions pane on the left-hand side.\nSelect the uv-created .venv as the python Environment we‚Äôre going to use to actually run our code. If this doesn‚Äôt happen, press ctrl-shift-P, type ‚Äúpython environment‚Äù to find the Python - Create Environment... option, hit enter, choose ‚ÄúVenv‚Äù and proceed to ‚ÄúUse Existing‚Äù.\n\nIf all has gone well, you should be able to hit the ‚Äúplay‚Äù icon in the top right to execute main.py. The Terminal pane should open up below and display something like Hello from (your-project-name)!.",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/01-onboarding/index.html#footnotes",
    "href": "sessions/01-onboarding/index.html#footnotes",
    "title": "Python Onboarding",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough there are several ways to measure language popularity, the PYPL Index, HackerRank‚Äôs Developer Skills Report, and IEEE Spectrum all rank Python as the most popular language in the world, while Stack Overflow‚Äôs Developer Survey places Python third behind JavaScript and HTML/CSS.‚Ü©Ô∏é\nWe recommend using the C drive for all Python projects, especially if using version control. Storing projects like these on One Drive will create many unnecessary issues.‚Ü©Ô∏é\nStrictly speaking, we should install ipykernel as a development dependency (a dependency that is needed for any development but not when the project is put into production). In this case, we would add it by running uv add --dev ipykernel. However, in this case, it is simpler to just add it as a regular dependency, and it doesn‚Äôt harm.‚Ü©Ô∏é",
    "crumbs": [
      "Sessions",
      "Onboarding",
      "1. Python Onboarding"
    ]
  },
  {
    "objectID": "sessions/schedule.html",
    "href": "sessions/schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This is the schedule for Code Club in FY25/26.\nThe Demonstration, Presentation, and Notebook columns indicate the content to be expected for each session:\n\nDemonstration: A live show-and-tell relating to the discussion topic.\nPresentation: A slide deck covering the discussion topic.\nNotebook: A Jupyter Notebook containing code-along elements or examples for people to work through after the session.\n\nTutorials will be divided into Modules. We recommend that people attend or watch tutorials in the Core module in order to gain a fundamental understanding of coding concepts and resources. Further modules are to be confirmed, but will likely include Automation, Dashboards and Visualisation, and Data Science. People will be able to attend those modules that interest them.\nWe have mapped the contents of the syllabus to competencies in the National Competency Framework for Data Professionals in Health and Care so that you can see which sessions will help you on your way towards them. For full details of the skills in the framework, the self-assessment tool can be found on FutureNHS.\nPlease note that this is a first draft of the mapping of NCF competencies to our syllabus and it is awaiting review.\n\n\n\n\n\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n\n\n  Session Date\n  Module\n  Session Name\n  Description\n  Demonstration\n  Presentation\n  Notebook\n  NCF Competency\n\n\n\n  \n    22/04/2025\n    N/A\n    Nectar Re-Launch\n    Showcasing the re-launch of Code Club\n    üé¨\n    üíª\n    -\n    -\n  \n  \n    01/05/2025\n    On-boarding\n     Python On-boarding\n    What to install, basic virtual environment management, introduction to VS Code\n    üé¨\n    -\n    -\n    SA21 : Python Proficiency\n  \n  \n    15/05/2025\n    On-boarding\n    Jupyter Notebooks\n    Demonstration of Jupyter Notebooks\n    üé¨\n    -\n    üìñ\n    SA21 : Python Proficiency\n  \n  \n    29/05/2025\n    Exploration & Visualisation\n    EDA with Pandas\n    Introduction to exploratory data analysis using pandas\n    üé¨\n    -\n    üìñ\n    SA4 : Descriptive and Explicative Analytics\n  \n  \n    12/06/2025\n    Exploration & Visualisation\n    Visualisation with Seaborn\n    Aesthetically-pleasing visualisations\n    üé¨\n    -\n    üìñ\n    SA1 : Data Visualisation\n  \n  \n    26/06/2025\n    Exploration & Visualisation\n    EDA with Seaborn\n    Using seaborn to visualise and explore data\n    üé¨\n    -\n    üìñ\n    SA4 : Descriptive and Explicative Analytics\n  \n  \n    10/07/2025\n    Core concepts\n    Data Types\n    Introduction to data types\n    -\n    üíª\n    üìñ\n    SA21 : Python Proficiency\n  \n  \n    24/07/2025\n    Core concepts\n    Control Flow\n    Introduction to control flow\n    -\n    üíª\n    üìñ\n    SA21 : Python Proficiency\n  \n  \n    07/08/2025\n    Core concepts\n    Functions & Functional Programming\n    Introduction to functions and functional programming\n    -\n    üíª\n    üìñ\n    SA21 : Python Proficiency\n  \n  \n    21/08/2025\n    Core concepts\n    Object-Oriented Programming\n    Introduction to object-oriented programming\n    -\n    üíª\n    üìñ\n    SA21 : Python Proficiency\n  \n  \n    04/09/2025\n    Exploration & Visualisation\n    Streamlit dashboards\n    How to present data (visualisations) in a Streamlit dashboard\n    üé¨\n    -\n    -\n    SA1 : Data Visualisation\n  \n  \n    18/09/2025\n    Data Science\n    Comparing Samples\n    Understanding data distributions and comparisons between samples\n    -\n    üíª\n    üìñ\n    SA4 : Descriptive and Explicative Analytics\n  \n  \n    02/10/2025\n    Data Science\n    Analysing Relationships\n    Quantifying relationships with hypothesis tests and statistical significance\n    -\n    üíª\n    üìñ\n    SA15 : Hypothesis Testing\n  \n  \n    16/10/2025\n    Data Science\n    Introduction to Linear Regression\n    Introduction to regression concepts and the component parts of linear regression\n    -\n    üíª\n    üìñ\n    SA7 : Advanced Statistics\n  \n  \n    30/10/2025\n    Data Science\n    Implementing Linear Regression\n    Building linear models, assessing model fit, and interpreting coefficients\n    üé¨\n    üíª\n    üìñ\n    SA7 : Advanced Statistics\n  \n  \n    13/11/2025\n    Data Science\n    Beyond Linearity\n    Introduction to generalised linear regression models\n    üé¨\n    üíª\n    üìñ\n    SA7 : Advanced Statistics",
    "crumbs": [
      "Sessions",
      "Session Schedule"
    ]
  },
  {
    "objectID": "resources/glossary.html",
    "href": "resources/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "This page will have a list of definitions for commonly used (and/or commonly misunderstood) terminology and acronyms relating to python or data analysis and manipulation in general.\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n  Term\n  Definition\n\n\n\n  \n    Exploratory Data Analysis (EDA)\n    The process of analysing datasets to summarise their main characteristics, often using visual methods, before formal modeling.\n  \n  \n    Functional programming\n    A programming paradigm that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data.\n  \n  \n    Git\n    A distributed version control system used to track changes in source code during software development.\n  \n  \n    GitHub\n    A cloud-based platform that provides hosting for repositories of files and folders (usually of software code) using git as its backend for version control.\n  \n  \n    Jupyter\n    A software package that allows the creation of python notebooks that can include a mixture of markdown-formatted text and live Python code.\n  \n  \n    Markdown (.md)\n    A simple plain-text markup scheme designed to allow for rapid production of formatted text (with headings, links, etc) within a plaintext file. Used by Quarto (as a Quarto-specific flavour with the .qmd extension).\n  \n  \n    matplotlib\n    The baseline Python library for creating static, animated, and interactive visualisations, offering extensive customisation options for plots and charts. Also used as a framework for more advanced or visually pleasing visualisation packages like seaborn.\n  \n  \n    Object-oriented programming (OOP)\n    A programming paradigm based on the concept of \"objects,\" which can contain data and code to manipulate that data.\n  \n  \n    pandas\n    A python data analysis and manipulation library for working with dataframes (tabular data)\n  \n  \n    Python\n    A general-purpose programming language\n  \n  \n    Regression\n    A method for modeling the relationship between one or more explanatory variables and an outcome. It is used to predict outcomes and understand the impact of changes in predictors (explanatory variables) on the response (outcome).\n  \n  \n    Repository (repo)\n    In git and github, a repository is a self-contained \"project\" of files and folders.\n  \n  \n    Reproducible Analytical Pipelines (RAP)\n    A set of processes and tools designed to ensure that data analysis can be consistently repeated and verified by others.\n  \n  \n    seaborn (sns)\n    A Python data visualisation library based on Matplotlib, providing a high-level interface for drawing attractive and informative statistical graphics.\n  \n  \n    TOML\n    Tom's Obvious Minimal Language - simple, human-readable data serialisation format designed for configuration files, emphasizing readability and ease of use. Used by uv to specify its projects.\n  \n  \n    uv\n    A Python package manager, which can manage python projects (folders) and manage the installation and management of the python environment and libraries within that folder.\n  \n  \n    YAML\n    Yet Another Markup Language - a human-readable data serialisation format often used for configuration files and data exchange between languages."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#what-to-expect",
    "href": "sessions/01-onboarding/slides.html#what-to-expect",
    "title": "Python Onboarding",
    "section": "What to Expect?",
    "text": "What to Expect?\n\nLearning a language is hard. It can be frustrating. Perseverance is key to success.\nThese sessions will introduce you to Python, showing you what is possible and how to achieve some of what might benefit your work.\nBut the real learning comes by doing. You need to run the code yourself, have a play around, and cement what you‚Äôve learned by applying it.\nPractice, repetition, and making mistakes along the way is how real progress is made."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#why-learn-python",
    "href": "sessions/01-onboarding/slides.html#why-learn-python",
    "title": "Python Onboarding",
    "section": "Why Learn Python?",
    "text": "Why Learn Python?\n\nCoding skills, generally, and Python specifically, seem to be a priority in the NHS right now. It‚Äôs a clear direction of travel. Learning now sets you up for the future.\nPython and the applied skills taught in these sessions will enable you to use advanced methods and design flexible, scalable solutions.\nPython is very valuable for career development.\nIt is (hopefully) fun!"
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#the-toolkit",
    "href": "sessions/01-onboarding/slides.html#the-toolkit",
    "title": "Python Onboarding",
    "section": "The Toolkit",
    "text": "The Toolkit\n\nWe will be using the following tools throughout this course:\n\nLanguage: Python\nDependency management: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code/Jupyter Notebooks (or your preferred IDE)\n\nYou can install all these tools by running the following in PowerShell:\n\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop"
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#python",
    "href": "sessions/01-onboarding/slides.html#python",
    "title": "Python Onboarding",
    "section": "Python",
    "text": "Python\n\nPython is an all-purpose programming language that is the most popular worldwide and widely used in almost every industry.\nPython‚Äôs popularity is owed to its flexibility ‚Äì it is the second-best tool for every job.\nIt is a strong choice for data science and analytics, being one of the best languages for data wrangling, data visualisation, statistics, and machine learning.\n\nIt is also well-suited to web development, scientific computing, and automation."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#dependency-management",
    "href": "sessions/01-onboarding/slides.html#dependency-management",
    "title": "Python Onboarding",
    "section": "Dependency Management",
    "text": "Dependency Management\n\nDependency management refers to the process of tracking and managing all of the packages (dependencies) a project needs to run. It ensures:\n\nThe right packages are installed.\nThe correct versions are used.\nConflicts between packages are avoided.\n\nWe are using uv for dependency management."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#virtual-environments",
    "href": "sessions/01-onboarding/slides.html#virtual-environments",
    "title": "Python Onboarding",
    "section": "Virtual Environments",
    "text": "Virtual Environments\n\nVirtual environments are isolated Python environments that allow you to manage dependencies for a specific project without the state of those dependencies affecting other projects or your wider system. They help by:\n\nKeeping dependencies separate for each project.\nAvoiding version conflicts between projects.\nMaking dependency management more predictable and reproducible.\n\nVirtual environments are a part of dependency management, and we will use uv to manage both the dependencies and virtual environments."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#version-control",
    "href": "sessions/01-onboarding/slides.html#version-control",
    "title": "Python Onboarding",
    "section": "Version Control",
    "text": "Version Control\n\nVersion control is the practice of tracking and managing changes to code or files over time, allowing you to:\n\nRevert to earlier versions if needed.\nCollaborate with others on the same project easily.\nMaintain a history of changes.\n\nWe are using Git (the version control system) and GitHub (the platform for hosting our work)."
  },
  {
    "objectID": "sessions/01-onboarding/slides.html#ide",
    "href": "sessions/01-onboarding/slides.html#ide",
    "title": "Python Onboarding",
    "section": "IDE",
    "text": "IDE\n\nAn IDE (Integrated Development Environment) is fully featured software that provides everything you need to write code as conveniently as possible.\nIt typically includes a code editor, debugger, build tools, and features like syntax highlighting and code completion.\nSome common IDEs used for Python include VS Code, PyCharm, Vim, Jupyter Notebooks/JupyterLab, and Positron.\nWe will use VS Code or Jupyter Notebooks (which is not exactly an IDE but is similar)."
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#what-is-a-notebook",
    "href": "sessions/02-jupyter_notebooks/slides.html#what-is-a-notebook",
    "title": "Jupyter Notebooks",
    "section": "What is a notebook",
    "text": "What is a notebook\n\nThe standard for programming in python is the .py file which can hold a block of code which can contain lines of code that allow you to export the results as visualisations or data files.\nJupyter Notebooks have been developed with the data science and analytical community.\nNotebooks are a collection interactive cells which a user can run as a collection or individually, based on the current state of program.\nCells can be denoted as Code, Markdown or Raw Depending on use case.\n\nCode cells use a process called a kernel to run programme elements in the user selected code base (e.g.¬†Python or R).\nMarkdown cells allow the user to include formatted text and other elements (such as links and images).\nRaw cells have no processing attached and output as plain text."
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#a-brief-history-of-jupyter-notebooks",
    "href": "sessions/02-jupyter_notebooks/slides.html#a-brief-history-of-jupyter-notebooks",
    "title": "Jupyter Notebooks",
    "section": "A brief history of Jupyter notebooks",
    "text": "A brief history of Jupyter notebooks\n\nIn 2001, Fernando Perez started development of the iPython project as a way of incorporating prompts and access to previous output, as he continued development he amalgamated iPython with 2 other projects\nIn 2014, Project Jupyter was born out of the initial iPython project. The key aim was to make the project independent of a programming language and allow different code bases to use notebooks. The Name is a reference to the three initial languages: Julia, Python, and R.\nJupyter Notebooks and more recently Jupiter Labs are more than just the notebook, they are interactive development environments launched from the command line.\nJupyter notebooks are used by many online platforms and service providers including: Kaggle, Microsoft Fabric, and the NHS Federated Data Platform."
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#pros-and-cons-of-using-a-notebook",
    "href": "sessions/02-jupyter_notebooks/slides.html#pros-and-cons-of-using-a-notebook",
    "title": "Jupyter Notebooks",
    "section": "Pros and cons of using a notebook",
    "text": "Pros and cons of using a notebook\nOn the plus side‚Ä¶\n\nNotebooks are highly interactive and allow cells to be run in any order.\nYou can re-run each cell separately, so iterative testing is more granular.\nNotebooks can be used to provide a structured report for an end user regardless of coding knowledge.\n\nHaving said that‚Ä¶\n\nIf you are not careful you can save a notebook in a state that cannot run as intended if changes are not checked.\nIt can be harder to understand complex code interactions."
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#the-toolkit",
    "href": "sessions/02-jupyter_notebooks/slides.html#the-toolkit",
    "title": "Jupyter Notebooks",
    "section": "The Toolkit",
    "text": "The Toolkit\n\nYou will need the following pre-installed:\n\nLanguage: Python\nDependency management: uv\nVersion Control: Git, GitHub Desktop\nIDE: VS Code (or your preferred IDE)\n\nYou can install all these tools by running the following in PowerShell:\n\nwinget install astral-sh.uv Microsoft.VisualStudioCode github-desktop"
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#walkthrough-and-demonstration",
    "href": "sessions/02-jupyter_notebooks/slides.html#walkthrough-and-demonstration",
    "title": "Jupyter Notebooks",
    "section": "Walkthrough and demonstration",
    "text": "Walkthrough and demonstration\nif reviewing these slides this section is only available in the recording, though the initial steps used should be available on the associated Code Club site page"
  },
  {
    "objectID": "sessions/02-jupyter_notebooks/slides.html#resources",
    "href": "sessions/02-jupyter_notebooks/slides.html#resources",
    "title": "Jupyter Notebooks",
    "section": "Resources",
    "text": "Resources\n\nCheck out the History of iPython\nYou can find out more about Project Jupyter\nThe demonstration makes use of this markdown cheatsheet\nLikewise this is the Jupyter shortcuts Cheat Sheet"
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html",
    "href": "sessions/03-eda-pandas/index.html",
    "title": "Exploring Data Using Pandas",
    "section": "",
    "text": "This is the first of four sessions looking at how to explore data in Python. This session will focus on introducing the Python library, pandas. We will use pandas to import, inspect, summarise, and transform the data, illustrating a typical exploratory data analysis workflow.\nWe are using Australian weather data, taken from Kaggle. This dataset is used to build machine learning models that predict whether it will rain tomorrow, using data about the weather every day from 2007 to 2017. To download the data, click here.\n# install necessary packages\n# !uv add skimpy\n\n# import packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom skimpy import skim\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#setting-the-scene",
    "href": "sessions/03-eda-pandas/index.html#setting-the-scene",
    "title": "Exploring Data Using Pandas",
    "section": "Setting the Scene",
    "text": "Setting the Scene\nBefore we start to explore any dataset, we need to establish what we are looking to do with the data. This should inform our decisions wwith any exploration, and any analysis that follows.\nQuestions:\n\nWhat are we trying to achieve?\nHow do our goals impact our analysis?\nWhat should we take into consideration before we write any code?\nWhat sort of questions might we be interested in with this dataset?\n\n\nWhat Our Data Can Tell Us (And What it Can‚Äôt)\nWe also need to consider what the data is and where it came from.\nQuestions:\n\nHow was the data collected?\nWhat is it missing?\nWhat do the variables in our dataset actually mean, and are they a good approximation of the concepts we are interested in?",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#exploring-the-dataset",
    "href": "sessions/03-eda-pandas/index.html#exploring-the-dataset",
    "title": "Exploring Data Using Pandas",
    "section": "Exploring the Dataset",
    "text": "Exploring the Dataset\nFirst, we should start with dataset-wide operations.\nQuestions:\n\nWhat do we want to know about a dataset when we first encounter it?\nHow do we get a quick overview of the data that can help us in our next steps?\nWe need to get a ‚Äúfeel‚Äù for the data before we can really make any decisions about how to analyse it. How do we get there with a new dataset?\n\nWe can start by getting a quick glance at the data. The starting point when you have just imported a new dataset is usually the pandas function pd.DataFrame.head(), which shows the top \\(n\\) rows of the dataset (by default it shows the top five rows).\n\n# view the top five rows\ndf.head()\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n0\n2008-12-01\nAlbury\n13.4\n22.9\n0.6\nNaN\nNaN\nW\n44.0\nW\n...\n71.0\n22.0\n1007.7\n1007.1\n8.0\nNaN\n16.9\n21.8\nNo\nNo\n\n\n1\n2008-12-02\nAlbury\n7.4\n25.1\n0.0\nNaN\nNaN\nWNW\n44.0\nNNW\n...\n44.0\n25.0\n1010.6\n1007.8\nNaN\nNaN\n17.2\n24.3\nNo\nNo\n\n\n2\n2008-12-03\nAlbury\n12.9\n25.7\n0.0\nNaN\nNaN\nWSW\n46.0\nW\n...\n38.0\n30.0\n1007.6\n1008.7\nNaN\n2.0\n21.0\n23.2\nNo\nNo\n\n\n3\n2008-12-04\nAlbury\n9.2\n28.0\n0.0\nNaN\nNaN\nNE\n24.0\nSE\n...\n45.0\n16.0\n1017.6\n1012.8\nNaN\nNaN\n18.1\n26.5\nNo\nNo\n\n\n4\n2008-12-05\nAlbury\n17.5\n32.3\n1.0\nNaN\nNaN\nW\n41.0\nENE\n...\n82.0\n33.0\n1010.8\n1006.0\n7.0\n8.0\n17.8\n29.7\nNo\nNo\n\n\n\n\n5 rows √ó 23 columns\n\n\n\nYou can also look at the bottom rows of the dataset, using pd.DataFrame.tail(). This might be useful if you are dealing with time-series data. Below, we specify that we want to look at the bottom ten rows.\n\n# view the bottom ten rows\ndf.tail(10)\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n145450\n2017-06-16\nUluru\n5.2\n24.3\n0.0\nNaN\nNaN\nE\n24.0\nSE\n...\n53.0\n24.0\n1023.8\n1020.0\nNaN\nNaN\n12.3\n23.3\nNo\nNo\n\n\n145451\n2017-06-17\nUluru\n6.4\n23.4\n0.0\nNaN\nNaN\nESE\n31.0\nS\n...\n53.0\n25.0\n1025.8\n1023.0\nNaN\nNaN\n11.2\n23.1\nNo\nNo\n\n\n145452\n2017-06-18\nUluru\n8.0\n20.7\n0.0\nNaN\nNaN\nESE\n41.0\nSE\n...\n56.0\n32.0\n1028.1\n1024.3\nNaN\n7.0\n11.6\n20.0\nNo\nNo\n\n\n145453\n2017-06-19\nUluru\n7.4\n20.6\n0.0\nNaN\nNaN\nE\n35.0\nESE\n...\n63.0\n33.0\n1027.2\n1023.3\nNaN\nNaN\n11.0\n20.3\nNo\nNo\n\n\n145454\n2017-06-20\nUluru\n3.5\n21.8\n0.0\nNaN\nNaN\nE\n31.0\nESE\n...\n59.0\n27.0\n1024.7\n1021.2\nNaN\nNaN\n9.4\n20.9\nNo\nNo\n\n\n145455\n2017-06-21\nUluru\n2.8\n23.4\n0.0\nNaN\nNaN\nE\n31.0\nSE\n...\n51.0\n24.0\n1024.6\n1020.3\nNaN\nNaN\n10.1\n22.4\nNo\nNo\n\n\n145456\n2017-06-22\nUluru\n3.6\n25.3\n0.0\nNaN\nNaN\nNNW\n22.0\nSE\n...\n56.0\n21.0\n1023.5\n1019.1\nNaN\nNaN\n10.9\n24.5\nNo\nNo\n\n\n145457\n2017-06-23\nUluru\n5.4\n26.9\n0.0\nNaN\nNaN\nN\n37.0\nSE\n...\n53.0\n24.0\n1021.0\n1016.8\nNaN\nNaN\n12.5\n26.1\nNo\nNo\n\n\n145458\n2017-06-24\nUluru\n7.8\n27.0\n0.0\nNaN\nNaN\nSE\n28.0\nSSE\n...\n51.0\n24.0\n1019.4\n1016.5\n3.0\n2.0\n15.1\n26.0\nNo\nNo\n\n\n145459\n2017-06-25\nUluru\n14.9\nNaN\n0.0\nNaN\nNaN\nNaN\nNaN\nESE\n...\n62.0\n36.0\n1020.2\n1017.9\n8.0\n8.0\n15.0\n20.9\nNo\nNaN\n\n\n\n\n10 rows √ó 23 columns\n\n\n\nA quick glimpse at the data is useful, but we may also want to get quick descriptions of several aspects of the data. Such as the length of the dataset (len(), which can also be used to get the length of various Python objects), which tells us how many observations we have.\n\n# get the object length\nlen(df)\n\n145460\n\n\nAnother option is pd.DataFrame.shape(), which shows the length (number of rows) and width (number of columns).\n\n# get the object shape (number of rows, number of columns)\ndf.shape\n\n(145460, 23)\n\n\nSpeaking of columns, if we want a quick list of the column names, we can get this using pd.DataFrame.columns().\n\n# get all column names\ndf.columns\n\nIndex(['Date', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation',\n       'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm',\n       'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',\n       'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am',\n       'Temp3pm', 'RainToday', 'RainTomorrow'],\n      dtype='object')\n\n\nA quick and easy way to get some valuable information about the dataset is pd.DataFrame.info(), including the total non-null observations and data type1 of each column.\n\n# get dataframe info (column indices, non-null counts, data types)\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 145460 entries, 0 to 145459\nData columns (total 23 columns):\n #   Column         Non-Null Count   Dtype  \n---  ------         --------------   -----  \n 0   Date           145460 non-null  object \n 1   Location       145460 non-null  object \n 2   MinTemp        143975 non-null  float64\n 3   MaxTemp        144199 non-null  float64\n 4   Rainfall       142199 non-null  float64\n 5   Evaporation    82670 non-null   float64\n 6   Sunshine       75625 non-null   float64\n 7   WindGustDir    135134 non-null  object \n 8   WindGustSpeed  135197 non-null  float64\n 9   WindDir9am     134894 non-null  object \n 10  WindDir3pm     141232 non-null  object \n 11  WindSpeed9am   143693 non-null  float64\n 12  WindSpeed3pm   142398 non-null  float64\n 13  Humidity9am    142806 non-null  float64\n 14  Humidity3pm    140953 non-null  float64\n 15  Pressure9am    130395 non-null  float64\n 16  Pressure3pm    130432 non-null  float64\n 17  Cloud9am       89572 non-null   float64\n 18  Cloud3pm       86102 non-null   float64\n 19  Temp9am        143693 non-null  float64\n 20  Temp3pm        141851 non-null  float64\n 21  RainToday      142199 non-null  object \n 22  RainTomorrow   142193 non-null  object \ndtypes: float64(16), object(7)\nmemory usage: 25.5+ MB\n\n\nIf we wanted to get a better sense of the null values in each column, we could calculate the percentage of null values by capturing whether each row of each column is null (pd.DataFrame.isnull()), summing the total null values in each column (pd.DataFrame.sum()), and then dividing by the length of the dataframe (/len()).\n\n# calculate the percentage of null values in each column\ndf.isnull().sum()/len(df)\n\nDate             0.000000\nLocation         0.000000\nMinTemp          0.010209\nMaxTemp          0.008669\nRainfall         0.022419\nEvaporation      0.431665\nSunshine         0.480098\nWindGustDir      0.070989\nWindGustSpeed    0.070555\nWindDir9am       0.072639\nWindDir3pm       0.029066\nWindSpeed9am     0.012148\nWindSpeed3pm     0.021050\nHumidity9am      0.018246\nHumidity3pm      0.030984\nPressure9am      0.103568\nPressure3pm      0.103314\nCloud9am         0.384216\nCloud3pm         0.408071\nTemp9am          0.012148\nTemp3pm          0.024811\nRainToday        0.022419\nRainTomorrow     0.022460\ndtype: float64\n\n\nIf we want a quick summary of all the numeric columns in the dataset, we can use pd.DataFrame.describe().\n\n# quick summary of numeric variables\ndf.describe()\n\n\n\n\n\n\n\n\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustSpeed\nWindSpeed9am\nWindSpeed3pm\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\n\n\n\n\ncount\n143975.000000\n144199.000000\n142199.000000\n82670.000000\n75625.000000\n135197.000000\n143693.000000\n142398.000000\n142806.000000\n140953.000000\n130395.00000\n130432.000000\n89572.000000\n86102.000000\n143693.000000\n141851.00000\n\n\nmean\n12.194034\n23.221348\n2.360918\n5.468232\n7.611178\n40.035230\n14.043426\n18.662657\n68.880831\n51.539116\n1017.64994\n1015.255889\n4.447461\n4.509930\n16.990631\n21.68339\n\n\nstd\n6.398495\n7.119049\n8.478060\n4.193704\n3.785483\n13.607062\n8.915375\n8.809800\n19.029164\n20.795902\n7.10653\n7.037414\n2.887159\n2.720357\n6.488753\n6.93665\n\n\nmin\n-8.500000\n-4.800000\n0.000000\n0.000000\n0.000000\n6.000000\n0.000000\n0.000000\n0.000000\n0.000000\n980.50000\n977.100000\n0.000000\n0.000000\n-7.200000\n-5.40000\n\n\n25%\n7.600000\n17.900000\n0.000000\n2.600000\n4.800000\n31.000000\n7.000000\n13.000000\n57.000000\n37.000000\n1012.90000\n1010.400000\n1.000000\n2.000000\n12.300000\n16.60000\n\n\n50%\n12.000000\n22.600000\n0.000000\n4.800000\n8.400000\n39.000000\n13.000000\n19.000000\n70.000000\n52.000000\n1017.60000\n1015.200000\n5.000000\n5.000000\n16.700000\n21.10000\n\n\n75%\n16.900000\n28.200000\n0.800000\n7.400000\n10.600000\n48.000000\n19.000000\n24.000000\n83.000000\n66.000000\n1022.40000\n1020.000000\n7.000000\n7.000000\n21.600000\n26.40000\n\n\nmax\n33.900000\n48.100000\n371.000000\n145.000000\n14.500000\n135.000000\n130.000000\n87.000000\n100.000000\n100.000000\n1041.00000\n1039.600000\n9.000000\n9.000000\n40.200000\n46.70000\n\n\n\n\n\n\n\nHowever, I prefer to bring in another package, skimpy, that does all of this very quickly and cleanly. We can get a detailed description of the entire dataset using skim().\n\n# a more informative summary function from the skimpy package\nskim(df)\n\n‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ skimpy summary ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n‚îÇ          Data Summary                Data Types                                                                 ‚îÇ\n‚îÇ ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì                                                          ‚îÇ\n‚îÇ ‚îÉ Dataframe         ‚îÉ Values ‚îÉ ‚îÉ Column Type ‚îÉ Count ‚îÉ                                                          ‚îÇ\n‚îÇ ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î© ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©                                                          ‚îÇ\n‚îÇ ‚îÇ Number of rows    ‚îÇ 145460 ‚îÇ ‚îÇ float64     ‚îÇ 16    ‚îÇ                                                          ‚îÇ\n‚îÇ ‚îÇ Number of columns ‚îÇ 23     ‚îÇ ‚îÇ string      ‚îÇ 7     ‚îÇ                                                          ‚îÇ\n‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                          ‚îÇ\n‚îÇ                                                     number                                                      ‚îÇ\n‚îÇ ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì  ‚îÇ\n‚îÇ ‚îÉ column         ‚îÉ NA     ‚îÉ NA %                ‚îÉ mean  ‚îÉ sd    ‚îÉ p0    ‚îÉ p25  ‚îÉ p50  ‚îÉ p75  ‚îÉ p100 ‚îÉ hist   ‚îÉ  ‚îÇ\n‚îÇ ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©  ‚îÇ\n‚îÇ ‚îÇ MinTemp        ‚îÇ   1485 ‚îÇ  1.0208992162793895 ‚îÇ 12.19 ‚îÇ 6.398 ‚îÇ  -8.5 ‚îÇ  7.6 ‚îÇ   12 ‚îÇ 16.9 ‚îÇ 33.9 ‚îÇ  ‚ñÉ‚ñá‚ñá‚ñÉ  ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ MaxTemp        ‚îÇ   1261 ‚îÇ  0.8669049910628351 ‚îÇ 23.22 ‚îÇ 7.119 ‚îÇ  -4.8 ‚îÇ 17.9 ‚îÇ 22.6 ‚îÇ 28.2 ‚îÇ 48.1 ‚îÇ  ‚ñÅ‚ñá‚ñá‚ñÉ  ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ Rainfall       ‚îÇ   3261 ‚îÇ   2.241853430496356 ‚îÇ 2.361 ‚îÇ 8.478 ‚îÇ     0 ‚îÇ    0 ‚îÇ    0 ‚îÇ  0.8 ‚îÇ  371 ‚îÇ   ‚ñá    ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ Evaporation    ‚îÇ  62790 ‚îÇ    43.1665062560154 ‚îÇ 5.468 ‚îÇ 4.194 ‚îÇ     0 ‚îÇ  2.6 ‚îÇ  4.8 ‚îÇ  7.4 ‚îÇ  145 ‚îÇ   ‚ñá    ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ Sunshine       ‚îÇ  69835 ‚îÇ   48.00976213391998 ‚îÇ 7.611 ‚îÇ 3.785 ‚îÇ     0 ‚îÇ  4.8 ‚îÇ  8.4 ‚îÇ 10.6 ‚îÇ 14.5 ‚îÇ ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñÉ ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ WindGustSpeed  ‚îÇ  10263 ‚îÇ   7.055547916953114 ‚îÇ 40.04 ‚îÇ 13.61 ‚îÇ     6 ‚îÇ   31 ‚îÇ   39 ‚îÇ   48 ‚îÇ  135 ‚îÇ  ‚ñÇ‚ñá‚ñÇ   ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ WindSpeed9am   ‚îÇ   1767 ‚îÇ   1.214766946239516 ‚îÇ 14.04 ‚îÇ 8.915 ‚îÇ     0 ‚îÇ    7 ‚îÇ   13 ‚îÇ   19 ‚îÇ  130 ‚îÇ   ‚ñá‚ñÇ   ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ WindSpeed3pm   ‚îÇ   3062 ‚îÇ   2.105046060772721 ‚îÇ 18.66 ‚îÇ  8.81 ‚îÇ     0 ‚îÇ   13 ‚îÇ   19 ‚îÇ   24 ‚îÇ   87 ‚îÇ  ‚ñÖ‚ñá‚ñÇ   ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ Humidity9am    ‚îÇ   2654 ‚îÇ  1.8245565791282827 ‚îÇ 68.88 ‚îÇ 19.03 ‚îÇ     0 ‚îÇ   57 ‚îÇ   70 ‚îÇ   83 ‚îÇ  100 ‚îÇ  ‚ñÅ‚ñÇ‚ñá‚ñá‚ñÜ ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ Humidity3pm    ‚îÇ   4507 ‚îÇ    3.09844630826344 ‚îÇ 51.54 ‚îÇ  20.8 ‚îÇ     0 ‚îÇ   37 ‚îÇ   52 ‚îÇ   66 ‚îÇ  100 ‚îÇ ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÇ ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ Pressure9am    ‚îÇ  15065 ‚îÇ     10.356799120033 ‚îÇ  1018 ‚îÇ 7.107 ‚îÇ 980.5 ‚îÇ 1013 ‚îÇ 1018 ‚îÇ 1022 ‚îÇ 1041 ‚îÇ   ‚ñÇ‚ñá‚ñÖ  ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ Pressure3pm    ‚îÇ  15028 ‚îÇ  10.331362573903478 ‚îÇ  1015 ‚îÇ 7.037 ‚îÇ 977.1 ‚îÇ 1010 ‚îÇ 1015 ‚îÇ 1020 ‚îÇ 1040 ‚îÇ   ‚ñÇ‚ñá‚ñÖ  ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ Cloud9am       ‚îÇ  55888 ‚îÇ   38.42155919153032 ‚îÇ 4.447 ‚îÇ 2.887 ‚îÇ     0 ‚îÇ    1 ‚îÇ    5 ‚îÇ    7 ‚îÇ    9 ‚îÇ ‚ñá‚ñÇ‚ñÉ‚ñÇ‚ñá‚ñÖ ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ Cloud3pm       ‚îÇ  59358 ‚îÇ   40.80709473394748 ‚îÇ  4.51 ‚îÇ  2.72 ‚îÇ     0 ‚îÇ    2 ‚îÇ    5 ‚îÇ    7 ‚îÇ    9 ‚îÇ ‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñá‚ñÉ ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ Temp9am        ‚îÇ   1767 ‚îÇ   1.214766946239516 ‚îÇ 16.99 ‚îÇ 6.489 ‚îÇ  -7.2 ‚îÇ 12.3 ‚îÇ 16.7 ‚îÇ 21.6 ‚îÇ 40.2 ‚îÇ  ‚ñÇ‚ñá‚ñá‚ñÉ  ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ Temp3pm        ‚îÇ   3609 ‚îÇ  2.4810944589577892 ‚îÇ 21.68 ‚îÇ 6.937 ‚îÇ  -5.4 ‚îÇ 16.6 ‚îÇ 21.1 ‚îÇ 26.4 ‚îÇ 46.7 ‚îÇ  ‚ñÅ‚ñá‚ñá‚ñÉ  ‚îÇ  ‚îÇ\n‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îÇ                                                     string                                                      ‚îÇ\n‚îÇ ‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì  ‚îÇ\n‚îÇ ‚îÉ          ‚îÉ       ‚îÉ          ‚îÉ          ‚îÉ          ‚îÉ          ‚îÉ          ‚îÉ chars per ‚îÉ words    ‚îÉ total     ‚îÉ  ‚îÇ\n‚îÇ ‚îÉ column   ‚îÉ NA    ‚îÉ NA %     ‚îÉ shortest ‚îÉ longest  ‚îÉ min      ‚îÉ max      ‚îÉ row       ‚îÉ per row  ‚îÉ words     ‚îÉ  ‚îÇ\n‚îÇ ‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©  ‚îÇ\n‚îÇ ‚îÇ Date     ‚îÇ     0 ‚îÇ        0 ‚îÇ 2008-12- ‚îÇ 2008-12- ‚îÇ 2007-11- ‚îÇ 2017-06- ‚îÇ        10 ‚îÇ        1 ‚îÇ    145460 ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ          ‚îÇ       ‚îÇ          ‚îÇ 01       ‚îÇ 01       ‚îÇ 01       ‚îÇ 25       ‚îÇ           ‚îÇ          ‚îÇ           ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ Location ‚îÇ     0 ‚îÇ        0 ‚îÇ Sale     ‚îÇ Melbourn ‚îÇ Adelaide ‚îÇ Woomera  ‚îÇ      8.71 ‚îÇ        1 ‚îÇ    145460 ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ          ‚îÇ       ‚îÇ          ‚îÇ          ‚îÇ eAirport ‚îÇ          ‚îÇ          ‚îÇ           ‚îÇ          ‚îÇ           ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ WindGust ‚îÇ 10326 ‚îÇ 7.098858 ‚îÇ W        ‚îÇ WNW      ‚îÇ E        ‚îÇ WSW      ‚îÇ      2.19 ‚îÇ     0.93 ‚îÇ    135134 ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ Dir      ‚îÇ       ‚îÇ 79279527 ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ           ‚îÇ          ‚îÇ           ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ WindDir9 ‚îÇ 10566 ‚îÇ 7.263852 ‚îÇ W        ‚îÇ NNW      ‚îÇ E        ‚îÇ WSW      ‚îÇ      2.18 ‚îÇ     0.93 ‚îÇ    134894 ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ am       ‚îÇ       ‚îÇ 60552729 ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ           ‚îÇ          ‚îÇ           ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ          ‚îÇ       ‚îÇ        2 ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ           ‚îÇ          ‚îÇ           ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ WindDir3 ‚îÇ  4228 ‚îÇ 2.906641 ‚îÇ E        ‚îÇ WNW      ‚îÇ E        ‚îÇ WSW      ‚îÇ      2.21 ‚îÇ     0.97 ‚îÇ    141232 ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ pm       ‚îÇ       ‚îÇ 00096246 ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ           ‚îÇ          ‚îÇ           ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ          ‚îÇ       ‚îÇ        4 ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ           ‚îÇ          ‚îÇ           ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ RainToda ‚îÇ  3261 ‚îÇ 2.241853 ‚îÇ No       ‚îÇ Yes      ‚îÇ No       ‚îÇ Yes      ‚îÇ      2.22 ‚îÇ     0.98 ‚îÇ    142199 ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ y        ‚îÇ       ‚îÇ 43049635 ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ           ‚îÇ          ‚îÇ           ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ          ‚îÇ       ‚îÇ        6 ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ           ‚îÇ          ‚îÇ           ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ RainTomo ‚îÇ  3267 ‚îÇ 2.245978 ‚îÇ No       ‚îÇ Yes      ‚îÇ No       ‚îÇ Yes      ‚îÇ      2.22 ‚îÇ     0.98 ‚îÇ    142193 ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ rrow     ‚îÇ       ‚îÇ 27581465 ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ           ‚îÇ          ‚îÇ           ‚îÇ  ‚îÇ\n‚îÇ ‚îÇ          ‚îÇ       ‚îÇ        7 ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ          ‚îÇ           ‚îÇ          ‚îÇ           ‚îÇ  ‚îÇ\n‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ End ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#exploring-variables-columns-observations-rows",
    "href": "sessions/03-eda-pandas/index.html#exploring-variables-columns-observations-rows",
    "title": "Exploring Data Using Pandas",
    "section": "Exploring Variables (Columns) & Observations (Rows)",
    "text": "Exploring Variables (Columns) & Observations (Rows)\nIf we are going to narrow our focus to specific variables or groups of observations, we need to know how to select columns, filter values, and group the data. There are lots of different ways we can slice up the data. We won‚Äôt cover all of them here2, but we will try to cover a range that helps illustrate how pandas works and will help you build the intuition for working with data in pandas.\nWe can select columns in a variety of ways, but the ‚Äúcorrect‚Äù way to select columns in most circumstances is using selection brackets (the square brackets []), also known as the indexing operator.\n\n# selecting a single column by name\ndf['Date']\n\n# alternative ways to select columns\n# df.loc[:, 'Date']\n# df.Date\n\n0         2008-12-01\n1         2008-12-02\n2         2008-12-03\n3         2008-12-04\n4         2008-12-05\n             ...    \n145455    2017-06-21\n145456    2017-06-22\n145457    2017-06-23\n145458    2017-06-24\n145459    2017-06-25\nName: Date, Length: 145460, dtype: object\n\n\nIf we want to select multiple columns, we can use double squared brackets ([[ ]]). This is the same process as before, but the inner brackets define a list, and the outer are the selection brackets.\n\n# selecting multiple columns (and all rows) by name\ndf[['Date', 'Location', 'Rainfall']]\n# df.loc[:, ['Date', 'Location', 'Rainfall']]\n\n\n\n\n\n\n\n\nDate\nLocation\nRainfall\n\n\n\n\n0\n2008-12-01\nAlbury\n0.6\n\n\n1\n2008-12-02\nAlbury\n0.0\n\n\n2\n2008-12-03\nAlbury\n0.0\n\n\n3\n2008-12-04\nAlbury\n0.0\n\n\n4\n2008-12-05\nAlbury\n1.0\n\n\n...\n...\n...\n...\n\n\n145455\n2017-06-21\nUluru\n0.0\n\n\n145456\n2017-06-22\nUluru\n0.0\n\n\n145457\n2017-06-23\nUluru\n0.0\n\n\n145458\n2017-06-24\nUluru\n0.0\n\n\n145459\n2017-06-25\nUluru\n0.0\n\n\n\n\n145460 rows √ó 3 columns\n\n\n\nWhile selection brackets are a quick and easy solution if we want to grab a subset of variables in the dataset, it is realy only intended to be used for simple operations using only column selection.\nFor row selection, we should use pd.DataFrame.iloc[]. The iloc function is used for ‚Äúinteger position‚Äù selection, which means you can select rows or columns using their integer position. For rows 10-15, you can select them using the following:\n\n# slicing by rows\ndf.iloc[10:16]\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n10\n2008-12-11\nAlbury\n13.4\n30.4\n0.0\nNaN\nNaN\nN\n30.0\nSSE\n...\n48.0\n22.0\n1011.8\n1008.7\nNaN\nNaN\n20.4\n28.8\nNo\nYes\n\n\n11\n2008-12-12\nAlbury\n15.9\n21.7\n2.2\nNaN\nNaN\nNNE\n31.0\nNE\n...\n89.0\n91.0\n1010.5\n1004.2\n8.0\n8.0\n15.9\n17.0\nYes\nYes\n\n\n12\n2008-12-13\nAlbury\n15.9\n18.6\n15.6\nNaN\nNaN\nW\n61.0\nNNW\n...\n76.0\n93.0\n994.3\n993.0\n8.0\n8.0\n17.4\n15.8\nYes\nYes\n\n\n13\n2008-12-14\nAlbury\n12.6\n21.0\n3.6\nNaN\nNaN\nSW\n44.0\nW\n...\n65.0\n43.0\n1001.2\n1001.8\nNaN\n7.0\n15.8\n19.8\nYes\nNo\n\n\n14\n2008-12-15\nAlbury\n8.4\n24.6\n0.0\nNaN\nNaN\nNaN\nNaN\nS\n...\n57.0\n32.0\n1009.7\n1008.7\nNaN\nNaN\n15.9\n23.5\nNo\nNaN\n\n\n15\n2008-12-16\nAlbury\n9.8\n27.7\nNaN\nNaN\nNaN\nWNW\n50.0\nNaN\n...\n50.0\n28.0\n1013.4\n1010.3\n0.0\nNaN\n17.3\n26.2\nNaN\nNo\n\n\n\n\n6 rows √ó 23 columns\n\n\n\nWe can do similar using a column‚Äôs integer position, but we have to select all rows (:) first:\n\n# using iloc with columns\ndf.iloc[:, 20]\n\n0         21.8\n1         24.3\n2         23.2\n3         26.5\n4         29.7\n          ... \n145455    22.4\n145456    24.5\n145457    26.1\n145458    26.0\n145459    20.9\nName: Temp3pm, Length: 145460, dtype: float64\n\n\nFinally, we can put both together to take a subset of both rows and columns:\n\n# using iloc with rows and columns\ndf.iloc[10:16, 20]\n\n10    28.8\n11    17.0\n12    15.8\n13    19.8\n14    23.5\n15    26.2\nName: Temp3pm, dtype: float64\n\n\nHowever, selecting by integer position is relatively limited. It is more likely we would want to subset the data based on the values of certain columns. We can filter rows by condition using pd.DataFrame.loc[]. The loc function slices by label, instead of integer position.\nFor example, we might want to look at a subset of the data based on location.\n\n# select all observations in Perth\ndf.loc[df['Location'] == 'Perth']\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n120638\n2008-07-01\nPerth\n2.7\n18.8\n0.0\n0.8\n9.1\nENE\n20.0\nNaN\n...\n97.0\n53.0\n1027.6\n1024.5\n2.0\n3.0\n8.5\n18.1\nNo\nNo\n\n\n120639\n2008-07-02\nPerth\n6.4\n20.7\n0.0\n1.8\n7.0\nNE\n22.0\nESE\n...\n80.0\n39.0\n1024.1\n1019.0\n0.0\n6.0\n11.1\n19.7\nNo\nNo\n\n\n120640\n2008-07-03\nPerth\n6.5\n19.9\n0.4\n2.2\n7.3\nNE\n31.0\nNaN\n...\n84.0\n71.0\n1016.8\n1015.6\n1.0\n3.0\n12.1\n17.7\nNo\nYes\n\n\n120641\n2008-07-04\nPerth\n9.5\n19.2\n1.8\n1.2\n4.7\nW\n26.0\nNNE\n...\n93.0\n73.0\n1019.3\n1018.4\n6.0\n6.0\n13.2\n17.7\nYes\nYes\n\n\n120642\n2008-07-05\nPerth\n9.5\n16.4\n1.8\n1.4\n4.9\nWSW\n44.0\nW\n...\n69.0\n57.0\n1020.4\n1022.1\n7.0\n5.0\n15.9\n16.0\nYes\nYes\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n123826\n2017-06-21\nPerth\n10.3\n19.9\n0.2\n1.8\n7.5\nNW\n37.0\nNNE\n...\n89.0\n60.0\n1017.1\n1013.8\n5.0\n6.0\n13.0\n18.5\nNo\nYes\n\n\n123827\n2017-06-22\nPerth\n13.0\n16.8\n61.2\n3.6\n0.0\nSSW\n46.0\nW\n...\n90.0\n75.0\n1005.6\n1008.9\n7.0\n7.0\n16.4\n15.6\nYes\nNo\n\n\n123828\n2017-06-23\nPerth\n13.3\n18.9\n0.4\n1.8\n6.5\nSE\n37.0\nSE\n...\n85.0\n65.0\n1019.2\n1019.4\n6.0\n6.0\n15.1\n18.0\nNo\nNo\n\n\n123829\n2017-06-24\nPerth\n11.5\n18.2\n0.0\n3.8\n9.3\nSE\n30.0\nESE\n...\n62.0\n47.0\n1025.9\n1023.4\n1.0\n3.0\n14.0\n17.6\nNo\nNo\n\n\n123830\n2017-06-25\nPerth\n6.3\n17.0\n0.0\n1.6\n7.9\nE\n26.0\nSE\n...\n75.0\n49.0\n1028.6\n1026.0\n1.0\n3.0\n11.5\n15.6\nNo\nNo\n\n\n\n\n3193 rows √ó 23 columns\n\n\n\nWe can also filter by multiple values, such as location and rainfall.\n\ndf.loc[(df['Rainfall'] == 0) & (df['Location'] == 'Perth')]\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n120638\n2008-07-01\nPerth\n2.7\n18.8\n0.0\n0.8\n9.1\nENE\n20.0\nNaN\n...\n97.0\n53.0\n1027.6\n1024.5\n2.0\n3.0\n8.5\n18.1\nNo\nNo\n\n\n120639\n2008-07-02\nPerth\n6.4\n20.7\n0.0\n1.8\n7.0\nNE\n22.0\nESE\n...\n80.0\n39.0\n1024.1\n1019.0\n0.0\n6.0\n11.1\n19.7\nNo\nNo\n\n\n120644\n2008-07-07\nPerth\n0.7\n18.3\n0.0\n0.8\n9.3\nN\n37.0\nNE\n...\n72.0\n36.0\n1028.9\n1024.2\n1.0\n5.0\n8.7\n17.9\nNo\nNo\n\n\n120645\n2008-07-08\nPerth\n3.2\n20.4\n0.0\n1.4\n6.9\nNNW\n24.0\nNE\n...\n58.0\n42.0\n1023.9\n1021.1\n6.0\n5.0\n10.2\n19.3\nNo\nYes\n\n\n120651\n2008-07-14\nPerth\n7.9\n19.7\n0.0\n0.2\n6.5\nNE\n31.0\nNE\n...\n86.0\n41.0\n1026.0\n1021.9\n6.0\n5.0\n11.7\n18.7\nNo\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n123823\n2017-06-18\nPerth\n7.5\n23.4\n0.0\n1.8\n9.2\nNNE\n28.0\nENE\n...\n67.0\n41.0\n1026.9\n1022.9\n0.0\n0.0\n14.2\n22.2\nNo\nNo\n\n\n123824\n2017-06-19\nPerth\n5.5\n23.0\n0.0\n3.0\n9.1\nSW\n19.0\nENE\n...\n84.0\n55.0\n1023.0\n1020.3\n1.0\n2.0\n11.5\n22.0\nNo\nNo\n\n\n123825\n2017-06-20\nPerth\n7.8\n22.5\n0.0\n2.8\n9.1\nNW\n26.0\nW\n...\n98.0\n59.0\n1019.3\n1015.9\n1.0\n1.0\n13.5\n21.6\nNo\nNo\n\n\n123829\n2017-06-24\nPerth\n11.5\n18.2\n0.0\n3.8\n9.3\nSE\n30.0\nESE\n...\n62.0\n47.0\n1025.9\n1023.4\n1.0\n3.0\n14.0\n17.6\nNo\nNo\n\n\n123830\n2017-06-25\nPerth\n6.3\n17.0\n0.0\n1.6\n7.9\nE\n26.0\nSE\n...\n75.0\n49.0\n1028.6\n1026.0\n1.0\n3.0\n11.5\n15.6\nNo\nNo\n\n\n\n\n2293 rows √ó 23 columns\n\n\n\nFor any complex process for subsetting the data, including multiple conditions, pd.DataFrame.loc[] is the best bet.\n\nSummarising Data\nNow that we know how to select the variables or observations we are interested in, we can start doing some descriptive analysis. The operations we use will depend on the questions we are trying to answer, and the possibilities will be almost endless.\nQuestions:\n\nWhat ‚Äúfunctions‚Äù might we need to carry out on our data when we are exploring it?\n\nWe know that the weather data includes observations from all over the country, but we might want to check exactly how many different locations there are. We can use pd.DataFrame.nunique() to do this.\n\n# count unique values\ndf['Location'].nunique()\n\n49\n\n\nWe may also be interested in the locations themselves, which may tell us more about the spatial distribution of our data. In this case, we can use pd.DataFrame.unique().\n\n# get unique values\ndf['Location'].unique()\n\narray(['Albury', 'BadgerysCreek', 'Cobar', 'CoffsHarbour', 'Moree',\n       'Newcastle', 'NorahHead', 'NorfolkIsland', 'Penrith', 'Richmond',\n       'Sydney', 'SydneyAirport', 'WaggaWagga', 'Williamtown',\n       'Wollongong', 'Canberra', 'Tuggeranong', 'MountGinini', 'Ballarat',\n       'Bendigo', 'Sale', 'MelbourneAirport', 'Melbourne', 'Mildura',\n       'Nhil', 'Portland', 'Watsonia', 'Dartmoor', 'Brisbane', 'Cairns',\n       'GoldCoast', 'Townsville', 'Adelaide', 'MountGambier', 'Nuriootpa',\n       'Woomera', 'Albany', 'Witchcliffe', 'PearceRAAF', 'PerthAirport',\n       'Perth', 'SalmonGums', 'Walpole', 'Hobart', 'Launceston',\n       'AliceSprings', 'Darwin', 'Katherine', 'Uluru'], dtype=object)\n\n\nAnother common operation we might look to do is calculating the mean value (pd.DataFrame.mean()) of a certain variable. What is the average value of sunshine across the entire dataset?\n\n# calculate variable mean\ndf['Sunshine'].mean()\n\nnp.float64(7.6111775206611565)\n\n\nThis gives us the mean to many decimal places, and we probably don‚Äôt need to know the average sunshine hours to this level of precision. We can use the pd.DataFrame.round() function to round to two decimal places.\n\n# round mean value\ndf['Sunshine'].mean().round(2)\n\nnp.float64(7.61)\n\n\nMany operations will return the value with information about the object‚Äôs type included. The above values are wrapped in np.float64() because pd.DataFrame.mean() uses numpy to calculate the mean value. However, if you want to strip this information out so you only see the value itself, you can use print().\n\n# print mean value\nprint(df['Sunshine'].mean().round(2))\n\n7.61\n\n\nWhile we are often interested in the mean value when we talk about averages, we might want to know the median instead (pd.DataFrame.median()).\n\n# calculate other summary statistics\nprint(df['Sunshine'].median())\n\n8.4\n\n\nAnother common calculation is summing values (pd.DataFrame.sum()). We can use sum() to see the total hours of sunshine in our dataset, and we can use int() to convert this value to an integer (which also means we don‚Äôt need to use print()3).\n\n# calculate sum value and return an integer\nint(df['Sunshine'].sum())\n\n575595\n\n\nWe can also apply these summary operations on multiple variables, using the same selection logic as before (using double squared brackets).\n\nprint(df[['Sunshine', 'Rainfall']].mean())\n\nSunshine    7.611178\nRainfall    2.360918\ndtype: float64\n\n\nAnd we can apply multiple functions, using pd.DataFrame.agg().\n\ndf['Sunshine'].agg(['mean', 'median', 'sum']).round(1)\n\nmean           7.6\nmedian         8.4\nsum       575595.3\nName: Sunshine, dtype: float64\n\n\nThe next step when exploring specific variables will often be group-level summaries. The average amount of sunshine across the whole dataset has limited utility, but the average hours of sunshine in each location allows us to compare between locations and start to understand how different variables are related to each other. If we want to do a group-level operation, we have to use pd.DataFrame.groupby().\n\n# calculate group means\ndf.groupby(by='Location')['Sunshine'].mean().round(1)\n\nLocation\nAdelaide            7.7\nAlbany              6.7\nAlbury              NaN\nAliceSprings        9.6\nBadgerysCreek       NaN\nBallarat            NaN\nBendigo             NaN\nBrisbane            8.1\nCairns              7.6\nCanberra            7.4\nCobar               8.7\nCoffsHarbour        7.4\nDartmoor            6.5\nDarwin              8.5\nGoldCoast           NaN\nHobart              6.6\nKatherine           NaN\nLaunceston          NaN\nMelbourne           6.4\nMelbourneAirport    6.4\nMildura             8.5\nMoree               8.9\nMountGambier        6.5\nMountGinini         NaN\nNewcastle           NaN\nNhil                NaN\nNorahHead           NaN\nNorfolkIsland       7.0\nNuriootpa           7.7\nPearceRAAF          8.8\nPenrith             NaN\nPerth               8.8\nPerthAirport        8.8\nPortland            6.5\nRichmond            NaN\nSale                6.7\nSalmonGums          NaN\nSydney              7.2\nSydneyAirport       7.2\nTownsville          8.5\nTuggeranong         NaN\nUluru               NaN\nWaggaWagga          8.2\nWalpole             NaN\nWatsonia            6.4\nWilliamtown         7.2\nWitchcliffe         NaN\nWollongong          NaN\nWoomera             9.0\nName: Sunshine, dtype: float64\n\n\nThe groupby(by='Location') function tells us the grouping variable (location), then we select the variable we want to summarise by location (sunshine), and then we specify the operation (mean).\nThere are multiple locations that return NaN (Not a Number). This indicates that numpy was unable to calculate a mean value for those locations. This is likely to be because all sunshine values for those locations are null.\nWe can check this using pd.DataFrame.count(), which counts the total non-null values (whereas pd.DataFrame.size() counts the total values).\n\n# group by location and count non-null sunshine values\ndf.groupby('Location')['Sunshine'].count()\n\nLocation\nAdelaide            1769\nAlbany              2520\nAlbury                 0\nAliceSprings        2520\nBadgerysCreek          0\nBallarat               0\nBendigo                0\nBrisbane            3144\nCairns              2564\nCanberra            1521\nCobar                550\nCoffsHarbour        1494\nDartmoor            2566\nDarwin              3189\nGoldCoast              0\nHobart              3179\nKatherine              0\nLaunceston             0\nMelbourne           3192\nMelbourneAirport    3008\nMildura             2876\nMoree               2055\nMountGambier        2597\nMountGinini            0\nNewcastle              0\nNhil                   0\nNorahHead              0\nNorfolkIsland       2570\nNuriootpa           2848\nPearceRAAF          3004\nPenrith                0\nPerth               3188\nPerthAirport        3004\nPortland            2566\nRichmond               0\nSale                1818\nSalmonGums             0\nSydney              3328\nSydneyAirport       2993\nTownsville          2617\nTuggeranong            0\nUluru                  0\nWaggaWagga          2575\nWalpole                0\nWatsonia            3008\nWilliamtown         1355\nWitchcliffe            0\nWollongong             0\nWoomera             2007\nName: Sunshine, dtype: int64\n\n\nThe results show that all the locations that return NaN in our group mean calculation have zero non-null values.",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#transforming-data",
    "href": "sessions/03-eda-pandas/index.html#transforming-data",
    "title": "Exploring Data Using Pandas",
    "section": "Transforming Data",
    "text": "Transforming Data\nDatasets are rarely perfectly clean and tidy. We often need to transform the data before we can get the most out of it.\nQuestions:\n\nWhat sort of transformations would help us get the most out of the analysis of the Australian weather data?\n\nThe first step with any analysis is often converting columns to the correct types. With a longitudinal (time-series) dataset,the date column is a good place to start. We can use pd.DataFrame.dtypes to check the data type, either of a single column (using the selector brackets) or all columns in the dataset.\n\nprint(df.dtypes)\n\nDate              object\nLocation          object\nMinTemp          float64\nMaxTemp          float64\nRainfall         float64\nEvaporation      float64\nSunshine         float64\nWindGustDir       object\nWindGustSpeed    float64\nWindDir9am        object\nWindDir3pm        object\nWindSpeed9am     float64\nWindSpeed3pm     float64\nHumidity9am      float64\nHumidity3pm      float64\nPressure9am      float64\nPressure3pm      float64\nCloud9am         float64\nCloud3pm         float64\nTemp9am          float64\nTemp3pm          float64\nRainToday         object\nRainTomorrow      object\ndtype: object\n\n\nAll columns are either stored as object or float64. The object data type is for generic non-numeric data, but from the columns that are stored as objects, we can tell this is mostly categorical variables where the categories are represented as text. The float64 data type refers to data that is numeric and includes decimals (float64 = 64-bit floating point number).\nThe date column is stored as an object, but pandas can store dates as datetime64. We can convert dates using pd.to_datetime(). When transforming data, if we want to keep those transformations, we have to store those changes, using =. In this case, we want to convert the date column but we don‚Äôt want to create an entirely new dataframe to handle this change, so we can overwrite the current date column by using the selection brackets to identify the column we want to apply this change to.\n\n# convert date column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\nThe remaining object columns can be converted to categorical, which makes them easier to work with in subsequent analyses. We can use pd.DataFrame.astype() to convert column data types.\n\n# create a list of all object columns\nobject_cols = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\n\n# convert object columns to category\ndf[object_cols] = df[object_cols].astype('category')\n\nA more efficient, though synactically more complex, way of doing this is using lamda functions. We won‚Äôt cover lambda functons in this session (they will be discussed in detail in a future session), but below is how we can use them to convert objects to categories.\n\n# convert object columns to category data type\ndf = df.apply(lambda x: x.astype('category') if x.dtype == 'object' else x)\n\nAnother choice we might make is to remove missing values, using pd.DataFrame.dropna() to filter the null values and keep only the non-null values. We can use this to drop all null values across the entire dataset, or we can apply it to a subset of columns, using the subset argument.\n\n# filter observations where sunshine is NA\ndf.dropna(subset='Sunshine')\n\n\n\n\n\n\n\n\nDate\nLocation\nMinTemp\nMaxTemp\nRainfall\nEvaporation\nSunshine\nWindGustDir\nWindGustSpeed\nWindDir9am\n...\nHumidity9am\nHumidity3pm\nPressure9am\nPressure3pm\nCloud9am\nCloud3pm\nTemp9am\nTemp3pm\nRainToday\nRainTomorrow\n\n\n\n\n6049\n2009-01-01\nCobar\n17.9\n35.2\n0.0\n12.0\n12.3\nSSW\n48.0\nENE\n...\n20.0\n13.0\n1006.3\n1004.4\n2.0\n5.0\n26.6\n33.4\nNo\nNo\n\n\n6050\n2009-01-02\nCobar\n18.4\n28.9\n0.0\n14.8\n13.0\nS\n37.0\nSSE\n...\n30.0\n8.0\n1012.9\n1012.1\n1.0\n1.0\n20.3\n27.0\nNo\nNo\n\n\n6051\n2009-01-03\nCobar\n15.5\n34.1\n0.0\n12.6\n13.3\nSE\n30.0\nNaN\n...\nNaN\n7.0\nNaN\n1011.6\nNaN\n1.0\nNaN\n32.7\nNo\nNo\n\n\n6052\n2009-01-04\nCobar\n19.4\n37.6\n0.0\n10.8\n10.6\nNNE\n46.0\nNNE\n...\n42.0\n22.0\n1012.3\n1009.2\n1.0\n6.0\n28.7\n34.9\nNo\nNo\n\n\n6053\n2009-01-05\nCobar\n21.9\n38.4\n0.0\n11.4\n12.2\nWNW\n31.0\nWNW\n...\n37.0\n22.0\n1012.7\n1009.1\n1.0\n5.0\n29.1\n35.6\nNo\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n142298\n2017-06-20\nDarwin\n19.3\n33.4\n0.0\n6.0\n11.0\nENE\n35.0\nSE\n...\n63.0\n32.0\n1013.9\n1010.5\n0.0\n1.0\n24.5\n32.3\nNo\nNo\n\n\n142299\n2017-06-21\nDarwin\n21.2\n32.6\n0.0\n7.6\n8.6\nE\n37.0\nSE\n...\n56.0\n28.0\n1014.6\n1011.2\n7.0\n0.0\n24.8\n32.0\nNo\nNo\n\n\n142300\n2017-06-22\nDarwin\n20.7\n32.8\n0.0\n5.6\n11.0\nE\n33.0\nE\n...\n46.0\n23.0\n1015.3\n1011.8\n0.0\n0.0\n24.8\n32.1\nNo\nNo\n\n\n142301\n2017-06-23\nDarwin\n19.5\n31.8\n0.0\n6.2\n10.6\nESE\n26.0\nSE\n...\n62.0\n58.0\n1014.9\n1010.7\n1.0\n1.0\n24.8\n29.2\nNo\nNo\n\n\n142302\n2017-06-24\nDarwin\n20.2\n31.7\n0.0\n5.6\n10.7\nENE\n30.0\nENE\n...\n73.0\n32.0\n1013.9\n1009.7\n6.0\n5.0\n25.4\n31.0\nNo\nNo\n\n\n\n\n75625 rows √ó 23 columns\n\n\n\nWe haven‚Äôt stored this transformation, because filtering nulls without careful consideration is a bad idea, but it‚Äôs useful to know, nonetheless.\nThere are lots of ways we could transform the data, but the final example we will consider here is reshaping the data using pd.DataFrame.pivot(), which transforms the data from long to wide format data, and pd.DataFrame.melt(), which transforms it from wide to long format.\nPerhaps we want to focus on the maximum temperature per day in each location in 2015. We can use pd.Series.dt.year to get the year from the date column, and filter for the year 2015, before reshaping the data.\n\ndf2015 = df.loc[df['Date'].dt.year == 2015]\ndf_wide = df2015.pivot(index='Date', columns='Location', values='MaxTemp')\n\ndf_wide.head()\n\n\n\n\n\n\n\nLocation\nAdelaide\nAlbany\nAlbury\nAliceSprings\nBadgerysCreek\nBallarat\nBendigo\nBrisbane\nCairns\nCanberra\n...\nTownsville\nTuggeranong\nUluru\nWaggaWagga\nWalpole\nWatsonia\nWilliamtown\nWitchcliffe\nWollongong\nWoomera\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2015-01-01\n37.0\n21.9\n33.5\n40.3\n34.7\n27.4\n31.2\n31.3\n33.7\n32.6\n...\n32.6\n32.1\n42.0\n35.2\n23.6\n28.3\n33.7\n25.0\n25.3\n39.2\n\n\n2015-01-02\n44.1\n21.2\n39.6\n41.4\n30.5\n38.2\n39.8\n30.5\n33.7\n35.2\n...\n33.0\n34.1\n42.4\n38.9\n21.1\n40.6\n29.3\n23.6\n24.6\n43.3\n\n\n2015-01-03\n38.2\n21.5\n38.3\n36.4\n34.3\n37.5\n40.3\n28.9\n33.6\n34.7\n...\n28.1\n33.7\n39.8\n37.5\n21.8\n39.5\n32.8\n23.0\n25.7\n44.7\n\n\n2015-01-04\n30.5\n23.3\n33.1\n29.0\n34.8\n23.5\n29.0\n30.2\n29.4\n32.5\n...\n31.6\n32.8\n36.1\n33.8\n24.4\n25.1\n34.5\n29.8\n25.3\n37.6\n\n\n2015-01-05\n34.9\n24.9\n35.2\n27.1\n27.2\n26.6\n33.6\n28.1\n31.4\n29.6\n...\n31.6\n28.9\n38.8\n34.9\n29.5\n25.7\n27.0\n31.7\n23.1\n38.3\n\n\n\n\n5 rows √ó 49 columns\n\n\n\nPerhaps we want to look at the maximum and minimum temperatures in each location, together. We can reshape the data to support this4.\n\ndf_long = df2015.melt(\n    id_vars=['Date', 'Location'],\n    value_vars=['MaxTemp', 'MinTemp'],\n    var_name='Variable',\n    value_name='Value'\n)\n\ndf_long.head()\n\n\n\n\n\n\n\n\nDate\nLocation\nVariable\nValue\n\n\n\n\n0\n2015-01-01\nAlbury\nMaxTemp\n33.5\n\n\n1\n2015-01-02\nAlbury\nMaxTemp\n39.6\n\n\n2\n2015-01-03\nAlbury\nMaxTemp\n38.3\n\n\n3\n2015-01-04\nAlbury\nMaxTemp\n33.1\n\n\n4\n2015-01-05\nAlbury\nMaxTemp\n35.2",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#exercises",
    "href": "sessions/03-eda-pandas/index.html#exercises",
    "title": "Exploring Data Using Pandas",
    "section": "Exercises",
    "text": "Exercises\nSome of these questions are easily answered by scrolling up and finding the answer in the output of the above code, however, the goal is to find the answer using code. No one actually cares what the answer to any of these questions is, it‚Äôs the process that matters!\nRemember, if you don‚Äôt know the answer, it‚Äôs okay to Google it (or speak to others, including me, for help)!\n\n\nImport Data (to Reset)\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')\n\n\n\nWhat is the ‚ÄòSunshine‚Äô column‚Äôs data type?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# What is the 'Sunshine' column's data type?\nprint(df['Sunshine'].dtypes)\n\nfloat64\n\n\n\n\n\n\nIdentify all the columns that are of dtype ‚Äòobject‚Äô.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Identify all the columns that are of dtype 'object'\nprint(list(df.select_dtypes(include=['object'])))\n\n['Date', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\n\n\n\n\n\n\nHow many of the dataframe‚Äôs columns are of dtype ‚Äòobject‚Äô?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# How many of the dataframe's columns are of dtype 'object'?\nlen(list(df.select_dtypes(include=['object'])))\n\n7\n\n\n\n\n\n\nHow many of the ‚ÄòRainfall‚Äô column values are NAs?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# How many of the 'Rainfall' column values are NAs?\nprint(df['Rainfall'].isna().sum())\n\n3261\n\n\n\n\n\n\nCreate a new dataframe which only includes the ‚ÄòDate‚Äô, ‚ÄòLocation, ‚ÄôSunshine‚Äô, ‚ÄòRainfall‚Äô, and ‚ÄòRainTomorrow‚Äô columns.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nnew_df = df[['Date', 'Location', 'Sunshine', 'Rainfall', 'RainTomorrow']]\nnew_df.head()\n\n\n\n\n\n\n\n\nDate\nLocation\nSunshine\nRainfall\nRainTomorrow\n\n\n\n\n0\n2008-12-01\nAlbury\nNaN\n0.6\nNo\n\n\n1\n2008-12-02\nAlbury\nNaN\n0.0\nNo\n\n\n2\n2008-12-03\nAlbury\nNaN\n0.0\nNo\n\n\n3\n2008-12-04\nAlbury\nNaN\n0.0\nNo\n\n\n4\n2008-12-05\nAlbury\nNaN\n1.0\nNo\n\n\n\n\n\n\n\n\n\n\n\nConvert ‚ÄòRainTomorrow‚Äô to a numeric variable, where ‚ÄòYes‚Äô = 1 and ‚ÄòNo‚Äô = 0.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# df['Location'].astype('category').cat.codes\n# df['RainTomorrow'].astype('category').cat.codes\ndf['RainTomorrow'].map({'Yes': 1, 'No': 0})\n\n0         0.0\n1         0.0\n2         0.0\n3         0.0\n4         0.0\n         ... \n145455    0.0\n145456    0.0\n145457    0.0\n145458    0.0\n145459    NaN\nName: RainTomorrow, Length: 145460, dtype: float64\n\n\n\n\n\n\nWhat is the average amount of rainfall for each location?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# average rainfall by location, sorted by value\ndf.groupby('Location')['Rainfall'].mean().sort_values(ascending=False)\n\nLocation\nCairns              5.742035\nDarwin              5.092452\nCoffsHarbour        5.061497\nGoldCoast           3.769396\nWollongong          3.594903\nWilliamtown         3.591108\nTownsville          3.485592\nNorahHead           3.387299\nSydney              3.324543\nMountGinini         3.292260\nKatherine           3.201090\nNewcastle           3.183892\nBrisbane            3.144891\nNorfolkIsland       3.127665\nSydneyAirport       3.009917\nWalpole             2.906846\nWitchcliffe         2.895664\nPortland            2.530374\nAlbany              2.263859\nBadgerysCreek       2.193101\nPenrith             2.175304\nTuggeranong         2.164043\nDartmoor            2.146567\nRichmond            2.138462\nMountGambier        2.087562\nLaunceston          2.011988\nAlbury              1.914115\nPerth               1.906295\nMelbourne           1.870062\nWatsonia            1.860820\nPerthAirport        1.761648\nCanberra            1.741720\nBallarat            1.740026\nWaggaWagga          1.709946\nPearceRAAF          1.669080\nMoree               1.630203\nBendigo             1.619380\nHobart              1.601819\nAdelaide            1.566354\nSale                1.510167\nMelbourneAirport    1.451977\nNuriootpa           1.390343\nCobar               1.127309\nSalmonGums          1.034382\nMildura             0.945062\nNhil                0.934863\nAliceSprings        0.882850\nUluru               0.784363\nWoomera             0.490405\nName: Rainfall, dtype: float64\n\n\n\n\n\n\nWhat is the average amount of rainfall for days that it will rain tomorrow?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# average rainfall depending on whether it will rain tomorrow or not\ndf.groupby('RainTomorrow')['Rainfall'].mean()\n\nRainTomorrow\nNo     1.270290\nYes    6.142104\nName: Rainfall, dtype: float64\n\n\n\n\n\n\nWhat is the average amount of sunshine in Perth when it will not rain tomorrow?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# average sunshine in Perth when it won't rain tomorrow\ndf.loc[(df['Location'] == 'Perth') & (df['RainTomorrow'] == 'No'), 'Sunshine'].mean()\n# df[(df['Location']=='Perth') & (df['RainTomorrow']=='No')]['Sunshine'].mean()\n\nnp.float64(9.705306603773584)\n\n\n\n\n\n\nWe want to understand the role that time plays in the dataset. Using the original dataframe, carry the following tasks and answer the corresponding questions:\n\nCreate columns representing the year and month from the ‚ÄòDate‚Äô column. How many years of data are in the dataset?\nExamine the distribution of the ‚ÄòSunshine‚Äô NAs over time. Is time a component in the ‚ÄòSunshine‚Äô data quality issues?\nCalculate the average rainfall and sunshine by month. How do rainfall and sunshine vary through the year?\nCalculate the average rainfall and sunshine by year. How have rainfall and sunshine changed over time?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# get year and month columns\ndf = (\n    df.assign(Date=pd.to_datetime(df['Date']))\n    .assign(\n        Year=lambda x: x['Date'].dt.year,\n        Month=lambda x: x['Date'].dt.month\n    )\n)\n\n# count unique years\ndf['Year'].nunique()\n\n11\n\n\n\n# lambda function counting nulls by year\ndf.groupby('Year')['Sunshine'].apply(lambda x: x.isna().sum())\n\nYear\n2007        0\n2008      323\n2009     6146\n2010     6220\n2011     6053\n2012     6539\n2013     7570\n2014     9157\n2015     9441\n2016    11994\n2017     6392\nName: Sunshine, dtype: int64\n\n\n\n# rainfall and sunshine by month\ndf.groupby('Month')[['Rainfall', 'Sunshine']].mean().round(1)\n\n\n\n\n\n\n\n\nRainfall\nSunshine\n\n\nMonth\n\n\n\n\n\n\n1\n2.7\n9.2\n\n\n2\n3.2\n8.6\n\n\n3\n2.8\n7.6\n\n\n4\n2.3\n7.1\n\n\n5\n2.0\n6.3\n\n\n6\n2.8\n5.6\n\n\n7\n2.2\n6.1\n\n\n8\n2.0\n7.1\n\n\n9\n1.9\n7.7\n\n\n10\n1.6\n8.5\n\n\n11\n2.3\n8.7\n\n\n12\n2.5\n9.0\n\n\n\n\n\n\n\n\n# rainfall and sunshine by year\ndf.groupby('Year')[['Rainfall', 'Sunshine']].mean().round(1)\n\n\n\n\n\n\n\n\nRainfall\nSunshine\n\n\nYear\n\n\n\n\n\n\n2007\n3.2\n8.1\n\n\n2008\n2.3\n7.8\n\n\n2009\n2.2\n7.9\n\n\n2010\n2.7\n7.3\n\n\n2011\n2.8\n7.3\n\n\n2012\n2.4\n7.6\n\n\n2013\n2.3\n7.7\n\n\n2014\n2.0\n7.8\n\n\n2015\n2.2\n7.7\n\n\n2016\n2.4\n7.6\n\n\n2017\n2.5\n7.7",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/03-eda-pandas/index.html#footnotes",
    "href": "sessions/03-eda-pandas/index.html#footnotes",
    "title": "Exploring Data Using Pandas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor more information about pandas data types, check out the pandas documentation on dtypes.‚Ü©Ô∏é\nFor more information, I‚Äôd recommend the pandas documentation, and this pandas tutorial on subsetting data.‚Ü©Ô∏é\nSome functions should be wrapped in print() in order to return a value that is easy to read, but others won‚Äôt. There will be an internal logic for which is which, but it‚Äôs not of huge importance to us. You are better off just testing functions out and wrapping them in print() if necessary.‚Ü©Ô∏é\nThis is often very useful when we need to visualise data, for example plotting the max and min temp for each location, is easier if the values are organised in the same column and differentiated using another column.‚Ü©Ô∏é",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "3. EDA with Pandas"
    ]
  },
  {
    "objectID": "sessions/04-seaborn-visualisation/index.html",
    "href": "sessions/04-seaborn-visualisation/index.html",
    "title": "Visualisation with Seaborn",
    "section": "",
    "text": "Python has a rich ecosystem of libraries for data visualisation, each with different strengths. Some popular options include matplotlib for fine control over plots, plotly for interactive visualisations, and bokeh for web-ready dashboards. In this session, we‚Äôll be using seaborn. It‚Äôs built on top of matplotlib but offers a simpler, high-level interface and nice looking default styles ‚Äî it‚Äôs therefore a good choice when you who want to quickly create clear and informative plots without needing to tweak every detail.\nWe are using Australian weather data, taken from Kaggle. This dataset is used to build machine learning models that predict whether it will rain tomorrow, using data about the weather every day from 2007 to 2017. To download the data, click here.\nOne final note before we get started - This page is a combination of text and python code. We‚Äôve tried to explain clearly what we‚Äôre about to do before we do it, but do also note the # comments within the python code cells themselves that occasionally explain a specific line of code in more detail.",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "4. Visualisations with Seaborn"
    ]
  },
  {
    "objectID": "sessions/04-seaborn-visualisation/index.html#initial-setup",
    "href": "sessions/04-seaborn-visualisation/index.html#initial-setup",
    "title": "Visualisation with Seaborn",
    "section": "Initial setup",
    "text": "Initial setup\nWe‚Äôre just going to import some python packages. Remember that the plt, np, sns aliases are just for convenience - we could omit this completely or use different aliases if we prefer.\n\n# install necessary packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# suppress some annoying warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning) \n\nsns.set_theme(style='darkgrid') # if we don't call this, we just get default matplotlib styling\nsns.set_context(\"notebook\") # set an overall scale. Notebook is the default. In increasing size: paper, notebook, talk, poster.\nplt.rcParams['font.sans-serif'] = ['Calibri','Segoe UI','Arial'] # use a nicer font in matplotlib (if available)\n\nAs before, we need to import our dataset. We‚Äôre importing the csv file into an initial dataframe called df to start with.\n\n# import the dataset\ndf = pd.read_csv('data/weatherAUS.csv')",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "4. Visualisations with Seaborn"
    ]
  },
  {
    "objectID": "sessions/04-seaborn-visualisation/index.html#data-manipulation",
    "href": "sessions/04-seaborn-visualisation/index.html#data-manipulation",
    "title": "Visualisation with Seaborn",
    "section": "Data manipulation",
    "text": "Data manipulation\n\nColumn conversions\nBefore we start actually generating some visuals, we need to make sure our Date column contains proper datetimes. We‚Äôre also going to drop the years with partial data so that our dataset only has full years. Finally we‚Äôre going to change the RainTomorrow field to contain a 0 or a 1 rather than yes/no.\n\n# convert date column to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# create a column that just contains the year\ndf['Year'] = df['Date'].dt.year\n\n# drop the partial years (2007,2017)\ndf = df[~df['Year'].isin([2007,2017])]\n\n# convert 'RainTomorrow' to a numeric variable, where 'Yes' = 1 and 'No' = 0.\ndf['RainToday']=df['RainToday'].replace({'Yes': 1, 'No': 0, 'NA':0}).fillna(0).astype(int)\ndf['RainTomorrow']=df['RainTomorrow'].map({'Yes': 1, 'No': 0,'NA': 0}).fillna(0).astype(int); \n\n# little tip: the semicolon suppresses textual output when we don't want it\n\n\n\nSort order and other helper tables\nWe need a month order for our ‚Äúmmm‚Äù months - there is probably an official way of doing this‚Ä¶\n\nmonth_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\nWe also need a sort order for our city names to use as a column order for some of our charts later. We‚Äôll just arrange them alphabetically.\n\ncolOrder_top5Cities=['Adelaide','Brisbane','Melbourne','Perth','Sydney']\n\nTo enhance a chart we‚Äôre going to build later, we‚Äôre going to dynamically calculate some text describing our data range.\n\n# Calculate the date range dynamically; we're going to use this later...\ndate_min = df['Date'].min().strftime('%Y')\ndate_max = df['Date'].max().strftime('%Y')\ndate_range = f\"{date_min} - {date_max}\"\n\nprint(date_range)\n\n2008 - 2016\n\n\n\n\nPivoting and grouping\nNext, we‚Äôre going to create some helper dataframes by filtering, grouping and pivoting the data. These will be used for different types of visuals later. Of course, we could have just created these groupings and pivots inline when we do the actual visualisation, but we‚Äôre doing it this way because:\n\nIt‚Äôs easier to follow\nIt‚Äôs tidier (and probably faster) to create these dataframes once as we‚Äôre going to be using them multiple times.\n\n\n# build a month column\ndf['Month'] = df['Date'].dt.strftime('%b') # Add a column that just contains the month in mmm format\ndf['Month'] = pd.Categorical(df['Month'], categories=month_order, ordered=True) # Make it categorical using our custom order so that it appears in the right order\n\n# we're going to filter to top 5 cities from now on\ndf_top5Cities = df[df['Location'].isin(['Perth','Adelaide','Sydney','Melbourne','Brisbane'])]\n\n# a dataframe with the number of rainy days per year and month, and location\ndf_top5Cities_rainyDays = df_top5Cities.groupby(['Location','Year', 'Month'])['RainToday'].sum().reset_index()\n\n# finally, we're going to create some grouped and pivoted dataframes. Picture these as PivotTables in Excel.\ndf_top5Cities_Rainfall_grouped = df_top5Cities.groupby(['Location', 'Month'])['Rainfall'].mean().reset_index()\ndf_top5Cities_Rainfall_pivoted = df_top5Cities_Rainfall_grouped.pivot(index=\"Location\",columns=\"Month\", values=\"Rainfall\")\ndf_top5Cities_monthly_rainyDays_pivoted = df_top5Cities.groupby(['Location', 'Month','Year'])['RainToday'].sum().reset_index().groupby(['Location','Month'])['RainToday'].mean().reset_index().pivot(index=\"Location\",columns=\"Month\", values=\"RainToday\")\n\nLet‚Äôs use head() to make sure we understand what each grouped/pivoted DF is for.\n\ndf_top5Cities_Rainfall_grouped.head(2)\n\n\n\n\n\n\n\n\nLocation\nMonth\nRainfall\n\n\n\n\n0\nAdelaide\nJan\n0.672199\n\n\n1\nAdelaide\nFeb\n0.973604\n\n\n\n\n\n\n\n\ndf_top5Cities_Rainfall_pivoted.head(2)\n\n\n\n\n\n\n\nMonth\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\nLocation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelaide\n0.672199\n0.973604\n1.171667\n1.461165\n2.293860\n2.447009\n2.873606\n2.399237\n1.809125\n0.906273\n0.730827\n1.192500\n\n\nBrisbane\n6.415574\n5.325389\n4.442276\n3.165385\n3.126446\n2.516318\n1.000000\n1.273381\n1.314498\n2.419424\n3.347761\n4.551613\n\n\n\n\n\n\n\n\ndf_top5Cities_monthly_rainyDays_pivoted.head(2)\n\n\n\n\n\n\n\nMonth\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\nLocation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelaide\n2.555556\n1.555556\n3.666667\n4.000000\n7.666667\n8.555556\n13.444444\n11.444444\n7.888889\n4.222222\n4.000000\n4.555556\n\n\nBrisbane\n8.000000\n7.111111\n10.000000\n5.333333\n5.555556\n6.222222\n4.111111\n3.555556\n4.111111\n5.888889\n6.888889\n8.777778\n\n\n\n\n\n\n\n\nAside: why df[df[...?\ndf_top5Cities = df[df['Location'].isin(['Perth','Adelaide','Sydney','Melbourne','Brisbane'])]\n\nThe first (outer) df[ tells pandas that we want to select a subset of rows based on some condition.\nThe second (inner) df[ is going to tell pandas this condition. In this case, we‚Äôre using isin to return a dataframe that contains a series of True and False rows corresponding to whether the rows in our original dataframe had the Location column as one of our 5 cities.\nThe final dataframe is then a filtered copy where the inner condition is True.\n\nYes, there are other ways of doing this! For example by using .query() to specify our conditions.",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "4. Visualisations with Seaborn"
    ]
  },
  {
    "objectID": "sessions/04-seaborn-visualisation/index.html#doing-some-actual-plotting",
    "href": "sessions/04-seaborn-visualisation/index.html#doing-some-actual-plotting",
    "title": "Visualisation with Seaborn",
    "section": "Doing some actual plotting",
    "text": "Doing some actual plotting\nThe Seaborn home page has a very good introductory tutorial, reference documentation, and a nice collection of examples. You should familiarise yourself with the documentation; it‚Äôll pay off massively if you actually grasp what each function and argument is for, rather than just copy/pasting examples and tweaking them until they work (without really understanding what they‚Äôre doing).\n\nA basic one-line line chart\n\nsns.lineplot(\n  data=df_top5Cities_Rainfall_pivoted.T #.T gives the transpose (flips rows and columns)\n  ) \n\n\n\n\n\n\n\n\n\n\nJust a little bit of customisation - a bar chart\nhttps://seaborn.pydata.org/generated/seaborn.barplot.html\n\nour_barplot = sns.barplot(\n  data=df_top5Cities_Rainfall_grouped \n  ,x=\"Month\"\n  ,y=\"Rainfall\"\n  ,hue=\"Location\" # read \"hue\" as \"series\"\n  ,palette=\"tab10\" # https://matplotlib.org/stable/users/explain/colors/colormaps.html\n  )\n\nour_barplot.set(title='Average daily rainfall by month and city')\n\nsns.move_legend(our_barplot,\"upper left\") # https://seaborn.pydata.org/generated/seaborn.move_legend.html\n\n\n\n\n\n\n\n\n\nAn aside - why did we need to set the title after the sns.barplot call?\nThe barplot function provided by Seaborn doesn‚Äôt actually allow setting of a title - it just generates a plot (including its axes) and returns this as a matplotlib Axes object (recall we mentioned earlier that Seaborn is a layer on top of the matplotlib library). By using the .set(...) method on our barplot object, we can modify this returned object to give it a title. We also could have used this to customise our axis labels (the defaults are fine here), set axis limits, or things like tick labels.\n\n\n\nHeatmaps\n\n# We need to use some matplotlib code to set our output size, add a title, and capitalise our x-axis label\nf,ax = plt.subplots(figsize=(10, 5)) \nax.set_title(f\"Average daily rainfall (/mm) each month ({date_range}) for Australia's top 5 cities\", fontsize=16, fontweight=\"bold\", pad=10)\n\nsns.heatmap(df_top5Cities_Rainfall_pivoted # Heatmap expects rectangular (pivot-like) data\n            ,annot=True # Put numbers inside the cells\n            ,fmt=\".1f\" # Make the numbers have 1 decimal place\n            ,square=True # Square vs rectangular cells\n            ,cbar=False # Get rid of the colourbar legend on the side\n            ,cmap=\"Blues\" # Seems appropriate for rainfall. Colourmaps reference: https://matplotlib.org/stable/users/explain/colors/colormaps.html \n            ,ax=ax # Tell it to use the matplotlib axes we created earlier\n           )\n\nax.set(xlabel=\"Month\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAnother heatmap with some further tweaks\n\n# Again setting up matplotlib subplots so that we can make some changes later\nf,ax = plt.subplots(figsize=(10, 5)) \n\nsns.heatmap(df_top5Cities_monthly_rainyDays_pivoted # Heatmap expects rectangular (pivot-like) data\n            ,annot=True # Put numbers inside the cells\n            ,fmt=\".0f\" # Force the number format\n            ,square=True # Square vs rectangular cells\n            ,cbar=False # Get rid of the colourbar legend on the side\n            ,cmap=\"crest\" # Colourmaps reference: https://matplotlib.org/stable/users/explain/colors/colormaps.html \n            ,ax=ax # Tell it to use the matplotlib axes we created earlier\n           )\n\n# We need to use some matplotlib code to set our output size, add a title, and capitalise our x-axis label\nax.tick_params(axis='x', labelsize=11, rotation=45) # I think 45-degree month labels look nicer, but this is a matter of taste.\nax.tick_params(axis='y', labelsize=11)\n\n# Manually changing our axis labels for more control\nax.set_xlabel(\"Month\",fontweight=\"bold\",fontsize=12) \nax.set_ylabel(\"City\",fontweight=\"bold\",fontsize=12)\n\n# Set our title dynamically\nax.set_title(f\"Mean number of rainy days by month between {date_min} and {date_max} for Australia's top 5 cities\", fontsize=16, fontweight=\"bold\", pad=15);\n\n\n\n\n\n\n\n\n\n\nA fancy multi-chart visual\n\n# Setting up the grid of box plots\n# Box plots are a bit of a rabbit hole and are extremely customisable; we're mostly using defaults here\nboxgrid = sns.FacetGrid(df_top5Cities \n                        ,col=\"Location\" # Defining the different facets\n                        ,col_wrap=5, height=4.5 # Layout and sizing for our facet grid\n                        ,col_order=colOrder_top5Cities  # Using our alphabetical order of city names to arrange our facets\n)\nboxgrid.map(sns.boxplot # This is what tells sns what sort of plots we want in our grid\n            ,\"Month\" # X\n            ,\"MaxTemp\" # Y\n            ,linewidth=1.5\n            ,color=\"skyblue\"\n            ,order=month_order\n            ,fliersize=0 # Seaborn boxplots by default include markers for outliers, which it calls \"fliers\". For this chart we'd like to disable these.\n            ) \n\n# Setting up the grid of line plots\n# Using the default matplotlib plot here\nlinegrid = sns.FacetGrid(df_top5Cities.groupby(['Location', 'Month'])['Rainfall'].mean().reset_index()\n                         ,col=\"Location\" \n                         ,hue=\"Location\",palette=\"Set2\" \n                         ,col_wrap=5, height=4.5 \n                         ,col_order=colOrder_top5Cities\n)\nlinegrid.map(plt.plot, \"Month\", \"Rainfall\",marker=\"o\")\n\n# Formatting axes\nfor ax in boxgrid.axes.flat:\n    ax.tick_params(axis='x', labelsize=9, rotation=45)\n    ax.tick_params(axis='y', labelsize=9)\nfor ax in linegrid.axes.flat:\n    ax.tick_params(axis='x', labelsize=9, rotation=45)\n    ax.tick_params(axis='y', labelsize=9)\n\nlinegrid.set_titles(col_template=\"{col_name}\",fontweight=\"bold\",fontsize=16)\nlinegrid.set_axis_labels(\"Month\",\"Average rainfall /mm\",fontweight=\"bold\",fontsize=10)\nboxgrid.set_titles(col_template=\"{col_name}\",fontweight=\"bold\",fontsize=16)\nboxgrid.set_axis_labels(\"Month\",\"Max temp /$\\\\degree$C\",fontweight=\"bold\",fontsize=10)\n\n# Setting overall titles and spacing\nlinegrid.fig.suptitle(f\"Mean daily rainfall by month for top 5 cities, {date_range}\", fontsize=16, color='black',fontweight='bold') \nboxgrid.fig.suptitle(f\"Max temperature by month for top 5 cities, {date_range}\", fontsize=16, color='black',fontweight='bold') \nlinegrid.fig.subplots_adjust(top=0.85)  \nboxgrid.fig.subplots_adjust(top=0.85);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚ÄúSmall multiples‚Äù-type time-series grid\nThis is mostly based on the example given at https://seaborn.pydata.org/examples/timeseries_facets.html.\nFirstly, for this one we need another dataframe that just contains the Sydney data.\n\ndf_Sydney = df[df['Location']==\"Sydney\"].groupby(['Month', 'Location','Year'], as_index=False)['MaxTemp'].max()\n\n\n# plot each year's time series in its own facet\ng = sns.relplot(data=df_Sydney\n                ,x=\"Month\"\n                ,y=\"MaxTemp\"\n                ,col=\"Year\"\n                ,hue=\"Year\"\n                ,kind=\"line\"\n                ,palette=\"viridis\"\n                ,linewidth=4\n                ,zorder=5\n                ,col_wrap=3, height=2, aspect=1.5, legend=False,\n)\n\n# iterate over each subplot to customize further\nfor year, ax in g.axes_dict.items():\n\n    # Add the title as an annotation within the plot\n    ax.text(.8\n            ,.85\n            ,year\n            ,transform=ax.transAxes\n            ,fontweight=\"bold\",fontsize=9)\n\n    # Plot every year's time series in the background\n    sns.lineplot(\n        data=df_Sydney, x=\"Month\", y=\"MaxTemp\", units=\"Year\",\n        estimator=None, color=\".7\", linewidth=1, ax=ax\n    )\n\n# reduce the frequency of the x axis ticks\nax.set_xticks(ax.get_xticks()[::2])\n\n# tweak the supporting aspects of the plot\ng.set_titles(\"\")\ng.fig.suptitle(f\"Max temperature by month in Sydney, {date_range}\", fontsize=16, color='black',fontweight='bold') \ng.set_axis_labels(\"\", \"Max Temp /$\\\\degree \\\\mathrm{C}$\");\ng.tight_layout();",
    "crumbs": [
      "Sessions",
      "Exploration & Visualisation",
      "4. Visualisations with Seaborn"
    ]
  }
]