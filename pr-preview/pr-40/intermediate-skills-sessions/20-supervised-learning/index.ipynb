{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding Supervised Learning\n",
        "\n",
        "This is the second session in our machine learning series. In the first\n",
        "session, we introduced machine learning conceptually and demonstrated a\n",
        "basic supervised learning workflow using logistic regression to predict\n",
        "survivors on the Titanic. This session builds on what was covered in the\n",
        "first session, exploring supervised learning in more detail,\n",
        "understanding the different approaches to supervising learning, and\n",
        "looking at some of the most common algorithms for supervised tasks.\n",
        "\n",
        "The goal for this session is to build on the first session by gaining a\n",
        "stronger understanding of what supervised learning is and exploring\n",
        "common models used in supervised learning.\n",
        "\n",
        "## Slides\n",
        "\n",
        "Use the left ⬅️ and right ➡️ arrow keys to navigate through the slides\n",
        "below. To view in a separate tab/window,\n",
        "<a href=\"slides.html\" target=\"_blank\">follow this link</a>."
      ],
      "id": "bc626070-e2e9-46eb-85ea-f2298d7fc93b"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<iframe src=\"slides.html\" height=\"500\" width=\"100%\">"
      ],
      "id": "2c6b30b0-8db3-4b48-ad1d-619002dacf7a"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</iframe>"
      ],
      "id": "9613c0d1-9484-4e76-92e6-d23dd7c95d97"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Supervised Learning?\n",
        "\n",
        "Supervised learning is a type of machine learning where a model is\n",
        "trained on data where the outcome is already known, in order to predict\n",
        "outcomes on new data where the outcome isn’t known. You provide features\n",
        "(input variables) and a target (the outcome to predict), and the model\n",
        "learns the relationship between them. This differs from unsupervised\n",
        "learning, where no target exists and the goal is finding patterns or\n",
        "groupings, and from reinforcement learning, where the model learns\n",
        "through trial and error.\n",
        "\n",
        "The fundamental structure of supervised learning involves features and\n",
        "targets. Features are the input variables that describe each\n",
        "observation: patient age, diagnosis codes, medication count, previous\n",
        "admissions. The target is what you want to predict: whether the patient\n",
        "will be readmitted, how long they’ll stay, which treatment will work\n",
        "best. Features can be numerical (age, test results) or categorical\n",
        "(diagnosis, sex), and most algorithms handle both types.\n",
        "\n",
        "There are two different kinds of supervised learning task, based on the\n",
        "target type:\n",
        "\n",
        "-   Classification - Predicting categories (often binary outcomes).\n",
        "    -   Will this patient be readmitted? Yes/No\n",
        "    -   Which diagnosis applies? A/B/C/D\n",
        "    -   Is this scan normal or abnormal? Yes/No\n",
        "-   Regression - Predicting continuous numbers.\n",
        "    -   How many days will the patient stay in hospital?\n",
        "    -   What will next month’s A&E attendances be?\n",
        "    -   What is a patient’s predicted blood pressure?\n",
        "\n",
        "The central challenge in supervised learning is generalisation. Models\n",
        "must learn patterns that work on new data, not just memorise the\n",
        "training set. Underfitting occurs when the model is too simple and\n",
        "misses important patterns, performing poorly on both training and test\n",
        "data. Overfitting occurs when the model is too complex and learns noise\n",
        "in the training data, performing well on training data but poorly on\n",
        "test data. Finding the right complexity level is fundamental to building\n",
        "useful models.\n",
        "\n",
        "## Understanding How Models Learn\n",
        "\n",
        "Understanding what happens during training transforms machine learning\n",
        "from magic into a tool you can control and apply effectively.\n",
        "\n",
        "Different algorithms approach learning in fundamentally different ways.\n",
        "Logistic regression finds linear boundaries that separate classes.\n",
        "Decision trees ask sequential yes/no questions. Random forests combine\n",
        "many trees to get more stable predictions. Gradient boosting builds\n",
        "trees that learn from previous mistakes. Each approach has strengths and\n",
        "weaknesses that make it better suited to different types of problems.\n",
        "\n",
        "This matters in practice because there is no single best algorithm. This\n",
        "is known as the No Free Lunch Theorem. The model that works best depends\n",
        "on your data structure, how much data you have, and what types of\n",
        "patterns exist in your features. Sometimes it might even be better to\n",
        "pick an algorithm that performs worse because it is more computationally\n",
        "efficient or because it is easier to interpet/explain its outputs.\n",
        "\n",
        "## Comparing Multiple Models\n",
        "\n",
        "We’ll train four different models on the Titanic dataset and compare\n",
        "their performance. This demonstrates how different algorithms learn\n",
        "different patterns from the same data.\n",
        "\n",
        "### Setup"
      ],
      "id": "58f700bd-70a3-4626-8055-98c7a5081646"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# set random seed for reproducibility\n",
        "np.random.seed(42)"
      ],
      "id": "setup"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preparation\n",
        "\n",
        "We will use the same data preparation steps as used in the first\n",
        "session."
      ],
      "id": "270c5b69-17f0-452c-8f13-55ffdbc9321d"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load titanic data\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "# select features and target\n",
        "X = df[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'alone', 'embark_town']]\n",
        "y = df['survived']\n",
        "\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# prepare features\n",
        "def prepare_features(data):\n",
        "    data = data.copy()\n",
        "\n",
        "    # convert sex to integer\n",
        "    data['sex'] = (data['sex'] == 'female').astype(int)\n",
        "\n",
        "    # fill missing age with median values by sex/passenger class\n",
        "    data['age'] = (data.groupby(['sex','pclass'])['age'].transform(lambda x: x.fillna(x.median())))\n",
        "\n",
        "    # create dummy variables for embark town\n",
        "    data['embark_town'] = data['embark_town'].str.lower()\n",
        "    data = pd.get_dummies(data, columns=['embark_town'], prefix=\"embark\")\n",
        "    \n",
        "    # keep only Southampton and Cherbourg for simplicity\n",
        "    data = data.drop(['embark_queenstown'], axis=1)\n",
        "\n",
        "    return data\n",
        "\n",
        "X_train = prepare_features(X_train)\n",
        "X_test = prepare_features(X_test)"
      ],
      "id": "load-and-prep-data"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logistic Regression\n",
        "\n",
        "Logistic regression works well when the relationship between features\n",
        "and outcomes is approximately linear, and it produces interpretable\n",
        "coefficients that show how each feature influences predictions."
      ],
      "id": "303e9b03-ad33-4c74-bd57-5edcb025ae8e"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 81.7%"
          ]
        }
      ],
      "source": [
        "# train logistic regression\n",
        "log_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "log_model.fit(X_train, y_train)\n",
        "\n",
        "# evaluate on test set\n",
        "log_pred = log_model.predict(X_test)\n",
        "log_accuracy = accuracy_score(y_test, log_pred)\n",
        "\n",
        "print(f\"Logistic Regression Accuracy: {log_accuracy:.1%}\")"
      ],
      "id": "logistic-regression"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decision Trees\n",
        "\n",
        "Decision trees learn by recursively splitting the data based on feature\n",
        "values. At each node, the tree asks a yes/no question about a feature\n",
        "and splits passengers into two groups. The algorithm chooses splits that\n",
        "best separate survivors from non-survivors, measured by metrics like\n",
        "Gini impurity[1].\n",
        "\n",
        "The tree continues splitting until it reaches one of the following\n",
        "stopping criteria:\n",
        "\n",
        "-   All passengers in a node have the same outcome\n",
        "-   The node contains too few passengers to split further\n",
        "-   The tree reaches its maximum depth\n",
        "\n",
        "[1] Gini impurity measures how mixed the classes are in a node. A pure\n",
        "node (all survivors or all deaths) has Gini = 0. A 50/50 split has Gini\n",
        "= 0.5. The algorithm chooses splits that minimise Gini impurity."
      ],
      "id": "7042e618-a8fb-4c61-b8e0-d0c81b35f7a3"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 81.0%"
          ]
        }
      ],
      "source": [
        "# train decision tree\n",
        "tree_model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_model.fit(X_train, y_train)\n",
        "\n",
        "# evaluate on test set\n",
        "tree_pred = tree_model.predict(X_test)\n",
        "tree_accuracy = accuracy_score(y_test, tree_pred)\n",
        "\n",
        "print(f\"Decision Tree Accuracy: {tree_accuracy:.1%}\")"
      ],
      "id": "decision-tree"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualise the tree structure to see exactly what rules it\n",
        "learned."
      ],
      "id": "df26fa65-a8a4-461a-86c5-2469e20b6f35"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(\n",
        "    tree_model, \n",
        "    feature_names=X_train.columns,\n",
        "    class_names=['Died', 'Survived'],\n",
        "    filled=True,\n",
        "    fontsize=10\n",
        ")\n",
        "plt.title(\"Decision Tree Structure (max_depth=3)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "cell-visualise-tree"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each box contains the splitting rule, the Gini impurity, the number of\n",
        "samples, and the predicted class. Following any path from root to leaf\n",
        "gives you the sequence of decisions that lead to a prediction.\n",
        "\n",
        "Decision trees are powerful because they are clear and simple. However,\n",
        "they tend to overfit the training data by learning overly specific rules\n",
        "that don’t generalise well.\n",
        "\n",
        "### Random Forests\n",
        "\n",
        "Decision trees are extremely effective at learning patterns from data.\n",
        "Too effective, in fact. The problem with decision trees is that they\n",
        "have a tendency to follow a rabbit hole and miss all the other signal in\n",
        "the data. But what if you could combine lots of decision trees together?\n",
        "This is a method called **ensemble learning**, which relies on the\n",
        "principle of “the wisdom of the crowds”. Ensembling combines multiple\n",
        "models, and the idea is that each model should contribute something\n",
        "different, making the final model stronger than the sum of its parts.\n",
        "\n",
        "Random forests are a type of ensemble learning called **bagging**, which\n",
        "addresses the overfitting problem with decision trees by training many\n",
        "trees and averaging their predictions[1]. Each tree in the forest is\n",
        "trained on a random subset of the training data (called a bootstrap\n",
        "sample), and at each split, only a random subset of features is\n",
        "considered. This randomness ensures each tree learns slightly different\n",
        "patterns.\n",
        "\n",
        "When making predictions, each tree votes for a class, and the forest\n",
        "returns the majority vote. Because individual trees make different\n",
        "mistakes, averaging across trees produces more reliable predictions that\n",
        "generalise better to new data.\n",
        "\n",
        "[1] The way random forests combine predictions from each decision tree\n",
        "depends on the task. If it is a classification problem, each individual\n",
        "tree votes on the prediction and the majority vote wins. For regression\n",
        "problems, it is the mean predictions across all trees."
      ],
      "id": "94ec7757-9654-43e7-9e19-1e115a704bbd"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 82.5%"
          ]
        }
      ],
      "source": [
        "# train random forest\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=1000,  # number of trees\n",
        "    max_depth=3,       # keep trees shallow to match single tree\n",
        "    random_state=42\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# evaluate on test set\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy:.1%}\")"
      ],
      "id": "random-forest"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random forests typically outperform single decision trees because the\n",
        "ensemble smooths out individual tree errors. The trade-off is reduced\n",
        "interpretability. You can’t easily visualise 100 trees or explain a\n",
        "prediction as a single decision path.\n",
        "\n",
        "The other risk with random forests is that they reduce variance\n",
        "(overfitting) but increase bias (underfitting)[1]. Random forests reduce\n",
        "variance by training many high-variance decision trees (complex trees\n",
        "that each overft on something different) and averaging their\n",
        "predictions. However, if not tuned correctly, this can lead to random\n",
        "forests missing important patterns in the data.\n",
        "\n",
        "### Gradient Boosting\n",
        "\n",
        "Gradient boosting takes a different approach to ensemble learning.\n",
        "Instead of training trees independently and averaging, it trains them\n",
        "sequentially. This is an ensembling method called **boosting**. Each new\n",
        "tree focuses on the mistakes made by previous trees, gradually improving\n",
        "the model’s performance.\n",
        "\n",
        "The process works as follows:\n",
        "\n",
        "1.  Train the first tree on the original data\n",
        "2.  Identify where it makes prediction errors\n",
        "3.  Train the second tree to correct those errors\n",
        "4.  Repeat\n",
        "\n",
        "Each tree contributes to the final prediction, with later trees\n",
        "typically having more influence because they’ve learned from more\n",
        "mistakes.\n",
        "\n",
        "[1] The bias-variance tradeoff is the idea that a model needs to balance\n",
        "the two types of errors that weaken model performance: bias and\n",
        "variance. Bias occurs when models are too simplistic and miss important\n",
        "patterns in the data, causing underfitting. Variance occurs when models\n",
        "are too complex and learn from noise, causing overfitting on the\n",
        "taraining data. The ideal model finds the balance between the two in\n",
        "order to minimise prediction error (and it’s not always true that there\n",
        "is a tradeoff between them)."
      ],
      "id": "521326d6-9a29-45f4-8daf-7ef93adebc0b"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Accuracy: 82.1%"
          ]
        }
      ],
      "source": [
        "# train gradient boosting model\n",
        "gb_model = GradientBoostingClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=3,\n",
        "    learning_rate=0.01,  # how much each tree contributes\n",
        "    min_samples_leaf=30,\n",
        "    random_state=42\n",
        ")\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# evaluate on test set\n",
        "gb_pred = gb_model.predict(X_test)\n",
        "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
        "\n",
        "print(f\"Gradient Boosting Accuracy: {gb_accuracy:.1%}\")"
      ],
      "id": "gradient-boosting"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gradient boosting often outperforms other algorithms, particularly on\n",
        "tabular data. There are lots of different gradient boosting algorithms,\n",
        "the most popular of which are\n",
        "[XGBoost](https://xgboost.readthedocs.io/en/release_3.2.0/python/index.html),\n",
        "[LightGBM](https://lightgbm.readthedocs.io/en/stable/), and\n",
        "[CatBoost](https://catboost.ai/docs/en/). The sequential learning\n",
        "process allows it to capture complex patterns that other models miss[1].\n",
        "\n",
        "While random forests are designed to reduce the variance (overfitting)\n",
        "that can occur with decision trees, gradient boosting comes at the\n",
        "problem from the other angle: bias. Gradient boosting trains lots of\n",
        "simple, high-bias trees that on their own will underfit the data, but\n",
        "each tree correcting the previous mistakes means the final model should\n",
        "benefit from the sequential process gradually reducing systematic\n",
        "errors. However, this does mean that gradient boosting models are prone\n",
        "to overfitting the training data if they are not tuned correctly.\n",
        "\n",
        "## Comparing Model Performance\n",
        "\n",
        "Different models learned different patterns from the same training data.\n",
        "Comparing their test set performance shows which patterns generalised\n",
        "best to unseen data.\n",
        "\n",
        "[1] Like random forests, the trade-off is reduced interpretability\n",
        "compared to single trees."
      ],
      "id": "05b2c0cf-2f94-4644-a2d2-3708ab88e02f"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Model  Accuracy\n",
            "      Random Forest  0.824627\n",
            "  Gradient Boosting  0.820896\n",
            "Logistic Regression  0.817164\n",
            "      Decision Tree  0.809701"
          ]
        }
      ],
      "source": [
        "# create comparison dataframe\n",
        "comparison = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest', 'Gradient Boosting'],\n",
        "    'Accuracy': [log_accuracy, tree_accuracy, rf_accuracy, gb_accuracy]\n",
        "})\n",
        "\n",
        "# sort by accuracy\n",
        "comparison = comparison.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(comparison.to_string(index=False))"
      ],
      "id": "model-comparison"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It’s worth noting that all four models have similar accuracy scores.\n",
        "This is primarily because the dataset is pretty simple so the simple\n",
        "models (logistic regression and decision trees) can easily compete with\n",
        "more complex models.\n",
        "\n",
        "It’s also because we are not tuning the model hyperparameters to\n",
        "optimise performance. We will cover this in a future session.\n",
        "\n",
        "We can use methods like feature importance plots to understand tree-base\n",
        "models. This tells us which features are contributing the most to the\n",
        "model’s predictions."
      ],
      "id": "3c556ec0-9dd2-49db-b046-c39ed675478c"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [
        "# create dataframe for plotting\n",
        "feature_names = X_train.columns\n",
        "importance_data = pd.DataFrame({\n",
        "    'Feature': list(feature_names) * 3,\n",
        "    'Importance': list(tree_model.feature_importances_) + \n",
        "                  list(rf_model.feature_importances_) + \n",
        "                  list(gb_model.feature_importances_),\n",
        "    'Model': ['Decision Tree'] * len(feature_names) + \n",
        "             ['Random Forest'] * len(feature_names) + \n",
        "             ['Gradient Boosting'] * len(feature_names)\n",
        "})\n",
        "\n",
        "# create faceted plot\n",
        "g = sns.FacetGrid(importance_data, col='Model', height=4, aspect=0.8)\n",
        "g.map_dataframe(sns.barplot, y='Feature', x='Importance', order=feature_names)\n",
        "g.set_axis_labels('Importance', '')\n",
        "g.set_titles(col_template='{col_name}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "cell-feature-importance"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The feaure importance plots confirm that the simplicity of the dataset\n",
        "results in all of the models converging on the same findings. Features\n",
        "are approximately equally important across all three models.\n",
        "\n",
        "## Overfitting and Model Complexity\n",
        "\n",
        "Model complexity is controlled through hyperparameters. For decision\n",
        "trees, the most important hyperparameter is `max_depth`, which limits\n",
        "how many sequential questions the tree can ask. Shallow trees underfit\n",
        "by not learning enough patterns. Deep trees overfit by learning noise in\n",
        "the training data."
      ],
      "id": "c0e0a2f1-2253-4872-9999-a3bd10ec4ecb"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [
        "# test different tree depths\n",
        "depths = range(1, 16)\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for depth in depths:\n",
        "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    tree.fit(X_train, y_train)\n",
        "    \n",
        "    train_scores.append(accuracy_score(y_train, tree.predict(X_train)))\n",
        "    test_scores.append(accuracy_score(y_test, tree.predict(X_test)))\n",
        "\n",
        "# create dataframe for plotting\n",
        "overfitting_data = pd.DataFrame({\n",
        "    'Tree Depth': list(depths) * 2,\n",
        "    'Accuracy': train_scores + test_scores,\n",
        "    'Dataset': ['Training Accuracy'] * len(depths) + ['Test Accuracy'] * len(depths)\n",
        "})\n",
        "\n",
        "# plot results\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=overfitting_data, x='Tree Depth', y='Accuracy', hue='Dataset', \n",
        "             marker='o', markersize=6)\n",
        "plt.title('Overfitting in Decision Trees')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "cell-overfitting-demo"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This overfitting problem is the reason that decision trees on their own\n",
        "tend to struggle with complex tasks, but it’s also a problem with lots\n",
        "of models, and it’s something you always have to look out for when\n",
        "building a machine learning model.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Different algorithms approach learning differently: logistic regression\n",
        "finds linear boundaries, decision trees ask sequential questions, random\n",
        "forests combine many trees through voting, and gradient boosting trains\n",
        "trees that learn from previous mistakes.\n",
        "\n",
        "No algorithm works best in all situations. Simpler models like logistic\n",
        "regression and shallow decision trees are more interpretable but may\n",
        "miss complex patterns. Ensemble methods like random forests and gradient\n",
        "boosting often achieve better performance but sacrifice\n",
        "interpretability. The right choice depends on your data, your\n",
        "constraints, and whether you need to explain predictions to\n",
        "stakeholders."
      ],
      "id": "d9cddea4-7f9c-410e-942a-214f75da620a"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "C:.johnson-club.venv"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  }
}